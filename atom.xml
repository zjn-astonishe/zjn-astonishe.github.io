<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZJN_BLOG</title>
  
  
  <link href="http://zjn-astonishe.github.io/atom.xml" rel="self"/>
  
  <link href="http://zjn-astonishe.github.io/"/>
  <updated>2025-07-03T02:02:04.764Z</updated>
  <id>http://zjn-astonishe.github.io/</id>
  
  <author>
    <name>ZJN</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM Agent Memory</title>
    <link href="http://zjn-astonishe.github.io/2025/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-30-LLM%20Agent%20Memory/"/>
    <id>http://zjn-astonishe.github.io/2025/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-30-LLM%20Agent%20Memory/</id>
    <published>2025-06-30T08:03:23.000Z</published>
    <updated>2025-07-03T02:02:04.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LLM-Agent-Memory"><a href="#LLM-Agent-Memory" class="headerlink" title="LLM Agent Memory"></a>LLM Agent Memory</h1><h2 id="Agent-Workflow-Memory"><a href="#Agent-Workflow-Memory" class="headerlink" title="Agent Workflow Memory"></a>Agent Workflow Memory</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Agent提取和学习在类似任务和环境中共享的可重用任务工作流，模仿人类从过去的成功和失败中学习经验指导未来的活动，随着时间的推移对任务上下文或环境发生变化进行适应。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>AWM从一组基本的内置动作开始，并以流式方式解决新任务，不断地从手头的任务中归纳出工作流，例如，从最初的几个例子中学习“通过名称查找地点”这个动作。然后，AWM继续基于新的经验和先前获得的工作流来构建更复杂的工作流。例如，“通过名称查找地点”工作流一旦被归纳出来，就可以有效地充当子目标，以构建更复杂的“获取地点的邮政编码”工作流。这种持续学习机制会产生滚雪球效应，从而归纳和应用日益复杂的工作流。</p><p>AWM可以在离线和在线两种场景中运行，分别对应有标注示例可用和无标注示例可用的情况。</p><ul><li>离线: 当任务拥有高质量的标注示例的时候，以离线方式运行，从标注示例中提取可复用的workflows，并整合到memory中，以辅助测试时的推理。</li><li>在线: 如果不存在任何标注示例，则在无监督的环境中运行，评估模块迭代地从自我生成的过去预测中归纳出workflows，并判断预测是否正确。<ul><li>就是要加个评估模块。</li></ul></li></ul><h3 id="AWM建模"><a href="#AWM建模" class="headerlink" title="AWM建模"></a>AWM建模</h3><h4 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h4><p>考虑Agent是由大语言模型骨架$L$和记忆$M$组成。</p><ul><li>基础记忆是包含”CLICK”和”TYPE”等内置操作的文档。</li><li>记忆作为系统prompt或主prompt上下文中的辅助信息。</li></ul><p>Agent需解决由自然语言指令$q$指定的任务。其在由转移函数$T$定义的环境中执行操作。</p><ul><li>对于每个时间步$t_i$，环境状态$s_i$给出观察结果$o_i$，然后将其传递到模型$L$中。</li><li>通过$L(q, M, o_i)\rightarrow a_i$生成动作$a_i$后，在环境中执行，于是得到环境转移$T(s_i, a_i)\rightarrow s_{i+1}$。</li><li>以上观察-行动的循环会一直迭代，直到模型预测停止动作$a_i=STOP$，或者达到任务终止条件(例如: 预定的最大步数)。</li></ul><p>每个完成的任务构成一次经验$e$，包含一条自然语言指令$q$和一个尝试解决任务的步骤轨迹$(P, O)$。</p><ul><li>每个步骤$\{p_i|p_i\in P\}$包含从当前状态获得的Agent观察结果$\{o_i|o_i\in O\}$。</li></ul><p>目标是从过去或收集的经验集合$\mathcal{E}=\{e_i\}_{i=1}^m$中，通过归纳模块$I$归纳出有用的工作流程$\mathcal{W}=\{w\}$，即有$I(\mathcal{E})\rightarrow \mathcal{W}$。</p><p>然后会将归纳出的工作流程添加到Agent的记忆$M$中，作为后续任务解决的指导。即$M\bigcup \mathcal{W} \rightarrow M_w$</p><h4 id="Workflow-Representation"><a href="#Workflow-Representation" class="headerlink" title="Workflow Representation"></a>Workflow Representation</h4><p><img src="" alt="img"></p><p>Workflow由两个组件构成:</p><ul><li>Workflow的文本描述$d$。本质上是对Workflow的功能摘要，通过从经验指令$p$中启发式地提取或使用模型$L$总结。</li><li>Workflow的轨迹$\mathcal{T}$。即完成Workflow所需执行的一系列步骤$(p_1, p_2, …)$。每个步骤$p_i$包含3个部分:<ul><li>当前环境状态的自然语言描述。</li><li>Agent阐述的推理过程: 基于观察结果(环境状态)决定下一步要执行哪个动作的思考过程。</li><li>可执行动作: 在环境中实际执行的操作，表示为可执行的程序指令。例如: 点击: “click(‘element_id’)”, 输入文本: “type(‘element_id’, ‘text’)”, 终止: “stop()”。</li></ul></li></ul><h4 id="Inducing-and-using-Workflow"><a href="#Inducing-and-using-Workflow" class="headerlink" title="Inducing and using Workflow"></a>Inducing and using Workflow</h4><p>AWM的核心是一个归纳模块$I$，从一个或多个过去的Agent经验$\mathcal{E}=\{e_i\}_{i=1}^m$中归纳出一组工作流$\mathcal{W}$。</p><ul><li>每个经验$e=(q, P^e)$包含一个自然语言指令$q$和一个为了解决指令$q$由一系列步骤$P^e=\{p_1^e, …, p_n^e\}$组成的动作轨迹$\mathcal{T}$。</li></ul><p>工作流的归纳模块通过接收经验$\mathcal{E}$并产生一组工作流$\mathcal{W}$来运作:$I(\mathcal{E})\rightarrow\mathcal{W}=\{w\}=\{(d_j, P_j^d)\}$</p><ul><li>基于LLM的工作流归纳:<ul><li>从一个或多个输入经验中提取公共子程序作为prompt输入LLM从而生成归纳$I$。</li><li>与指定具体、重复性较低的任务指令(例如，“在Amazon上购买猫粮并送到我的地址”)不同，特意提示模型以更精细的粒度归纳工作流。<ul><li>子任务“在Amazon上搜索产品”经常作为多个类似指令的一部分重新出现。</li><li>同时，没有给出特定于示例的值(例如，“猫粮”)，而是通过抽象出特定于示例的上下文来增强工作流的通用性，即通过在工作流归纳提示中指定，将“猫粮”替换为更通用的名称“{product-name}”。这些工作流被分割(基于模型输出中的双行换行符，即换行后空一行)，并单独存储在工作流内存中。</li></ul></li><li>在工作流$\mathcal{W}$被归纳出来之后，会被整合到Agent的辅助记忆中，表示为$M+\mathcal{W}\rightarrow M_w$，其中$M$代表原始智能体的记忆，而$M_w$代表通过归纳工作流增强后的Agent记忆。<ul><li>当下一次要解决给定的指令$q$时，Agent通过$L(q, M_w, o)=L(q, M+\mathcal{W}, o)\rightarrow a$产生一系列动作。</li></ul></li><li>离线场景<ul><li>当有额外的标准经验可用时(例如人工标注的数据或模型合成的数据)，采用离线模式运行，执行工作流归纳和利用两个独立的过程。<ul><li>首先将所有的训练示例连接成一个prompt，并发送到LLM，以在训练时创建一组工作流: $I(\mathcal{E}_{train})\rightarrow \mathcal{W}_{offline}$。</li><li>其次，AWM在推理时将所有归纳的工作流整合到Agent的记忆中，以解决测试指令。表示为$L(q, M+W_{offline}, o_{test_i})\rightarrow a_{test_i}$。</li><li>由于工作流在测试时推理之前被已经被完全归纳，因此Agent使用相同的工作流记忆$\mathcal{W}_{offline}$来解决每个测试。</li></ul></li></ul></li><li>在线场景<ul><li>没有额外的标准经验时，采用在线模式无监督运行。在这种环境中，只需处理测试的数据。在每次测试数据完成推理后，执行归纳整合，然后将经验利用到下一个测试数据的推理中。</li><li>具体来说，Agent从默认记忆$M$开始，给定传递给Agent的第$t$个测试指令$q_t$，Agent尝试通过生成动作轨迹$(p_t1, p_t2, …)$解决任务，于是构成了经验$e_t=(q_t, \{p_t\})$。并采用一个基于LLM的评估模型输出一个二元标签$L_{eval}(e_t)\in\{0, 1\}$。该标签用于判断$e_t$是否成功解决了$q_t$。如果成功解决，则为1，将其转化为工作流$I(e_t)\rightarrow\{w_t\}$，并将$\{w_t\}$添加到Agent的记忆中$M_t+\{w_t\}\rightarrow M_{t+1}$，用于辅助处理第$t+1$条指令。</li><li>整个流程是通过迭代地预测动作并从流式测试指令中归纳工作流来继续这个记忆更新过程，直到所有测试都处理完毕。评估所有测试的预测动作轨迹$\{p_t\}$的成功率，并报告平均分数。</li></ul></li></ul></li></ul><h2 id="MEM1-Learning-to-Synergize-Memory-and-Reasoning-for-Efficient-Long-Horizon-Agents"><a href="#MEM1-Learning-to-Synergize-Memory-and-Reasoning-for-Efficient-Long-Horizon-Agents" class="headerlink" title="MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents"></a>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LLM-Agent-Memory&quot;&gt;&lt;a href=&quot;#LLM-Agent-Memory&quot; class=&quot;headerlink&quot; title=&quot;LLM Agent Memory&quot;&gt;&lt;/a&gt;LLM Agent Memory&lt;/h1&gt;&lt;h2 id=&quot;Agent-Wor</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>LLM Agent Serving</title>
    <link href="http://zjn-astonishe.github.io/2025/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-29-LLM%20Agent%20Serving/"/>
    <id>http://zjn-astonishe.github.io/2025/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-29-LLM%20Agent%20Serving/</id>
    <published>2025-06-29T03:22:18.000Z</published>
    <updated>2025-07-02T08:37:11.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LLM-Agent-Serving"><a href="#LLM-Agent-Serving" class="headerlink" title="LLM Agent Serving"></a>LLM Agent Serving</h1><h2 id="Throughput-Optimal-Scheduling-Algorithms-for-LLM-Inference-and-AI-Agents"><a href="#Throughput-Optimal-Scheduling-Algorithms-for-LLM-Inference-and-AI-Agents" class="headerlink" title="Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents"></a>Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于 LLM 引擎以自回归方式生成 token，因此处理单个请求需要多次运行模型，每次迭代生成一个输出 token。为了优化 GPU 利用率，重要的是在解码阶段批量处理多个请求。本文研究了各种调度算法在形成批次时的性能保证。<br>通过排队理论，分析“对于一个LLM推理系统，其最大吞吐率的根本限制是什么？哪些类型的调度算法能够达到这个限制？”</p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>FasterTransformer在请求级别执行批处理，从而提高解码阶段的吞吐量。优先考虑解码阶段。当解码阶段没有足够的请求进行批处理时，这些迭代的吞吐量就会受到影响。</p><p>Orca和vLLM在 token 级别实现批处理，并优先处理新到达请求的预填充阶段，从而使这些请求能够更快地进入解码阶段。与 FasterTransformer 相比，Orca 和 vLLM 是为了减少延迟，尽管它们在是否支持混合批处理（即将 prefill 和 decode 两个阶段合并在一起进行批量处理）方面有所不同。</p><p>Sarathi-Serve引入了 chunk-prefill 来限制单个批次中的 token 数量和 预填充 token 的长度，从而防止具有较长预填充的请求阻塞解码。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h4 id="LLM的生成式推理"><a href="#LLM的生成式推理" class="headerlink" title="LLM的生成式推理"></a>LLM的生成式推理</h4><p>在处理输入请求时，LLM 推理包括两个阶段：预填充(prefill)和解码(decoding)。</p><ul><li>预填充阶段处理整个请求，以计算 KV 缓存（Key-Value cache，用于存储每一层的 attention keys 和 values 以避免冗余计算），并在单个步骤中生成初始响应 token。</li><li>解码阶段然后利用先前的上下文或 KV 缓存，一次生成一个后续 token。</li></ul><p>这些阶段具有不同的资源利用模式：预填充是计算密集型的（compute-bound），而解码是内存 I/O 密集型的（memory I/O-bound）。传统的 LLM 推理引擎将这两个阶段都放在同一个 GPU 组上以最大化资源利用率，尽管它们的计算特性不同。但这种方式会造成两个阶段互相干扰，且资源分配和并行策略紧密耦合。现在更提倡对两个阶段分离解耦放在不同的GPU上，详见(<a href="https://hub.baai.ac.cn/view/37129">LLM Serving有效吞吐量的最大化实现</a>)。</p><h4 id="Inference-and-serving-goal"><a href="#Inference-and-serving-goal" class="headerlink" title="Inference and serving goal"></a>Inference and serving goal</h4><p>有两个关键指标用于评估LLM服务的性能: 吞吐量和推理延迟。</p><ul><li>吞吐量(Throughput): 衡量在给定时间内生成的 token 数量。</li><li>推理延迟(Inference latency): 表示完成一个请求所需的时间。主要的延迟指标: <ul><li>TTFT(Time to first token): LLM输出第一个生成词元所需的时间，主要是预填充阶段。</li><li>TBT(time between token or TPOT, Time per output token): 衡量两个连续生成的词元之间的平均时延，主要是解码阶段。</li></ul></li><li>服务级别目标(Service level objective, SLO): 衡量延迟性能。</li></ul><p>具体衡量标准: </p><ul><li>对于在线服务，目标是在延迟 SLO 约束下优化吞吐量。</li><li>对于离线推理或批量推理，目标是优化批量吞吐量。</li></ul><h4 id="Inference-optimization"><a href="#Inference-optimization" class="headerlink" title="Inference optimization"></a>Inference optimization</h4><p>为了应对在满足性能要求的同时，最大限度地减少资源浪费的挑战，过去的研究利用 GPU 混合并行化策略，结合数据并行、张量并行和流水线并行来优化跨 GPU 的计算。</p><ul><li>张量模型并行(TP): 通过按行和按列划分 transformer 权重矩阵，将计算分布在 GPU 上，并通过两次 AllReduce 操作聚合层输出(会将所有参与计算的 GPU 上的数据进行某种操作(例如求和)，并将最终结果分发回所有 GPU。)。</li><li>流水线并行(PP): 将模型分割成多个阶段，分配给特定的 GPU 或 GPU 组，并在各阶段之间传递层间激活(本阶段的输出作为下一阶段的输入)。</li><li>先进的内存管理技术，如 paged attention，有助于缓解内存压力，提高资源利用率。</li><li>调度器优化，包括动态批处理和分块预填充处理，以及分离 prefill 和 decoding 阶段，确保高吞吐量，同时满足延迟约束，使这些系统能够有效地处理各种 LLM 推理工作负载。</li></ul><h4 id="Batching"><a href="#Batching" class="headerlink" title="Batching"></a>Batching</h4><p>计算中 prefill 阶段和 decoding 阶段的差异会导致应用批处理策略时产生不同的性能结果。 一种称为 continuous batching 的优化技术将新的请求 prefill 操作与正在进行的请求 decoding 操作合并在同一批处理中，以提高 GPU 利用率，从而在 token 级别做出批处理决策。 然而，这种方法会在 prefill 阶段和 decoding 阶段之间产生显着的干扰。 即使将单个 prefill 作业引入到 decoding token 的批处理中，也会大大降低这两个操作的性能，并且随着 prefill 输入长度的增加，性能损失会变得更加严重。</p><h3 id="A-stochastic-processing-model-for-LLM-inference"><a href="#A-stochastic-processing-model-for-LLM-inference" class="headerlink" title="A stochastic processing model for LLM inference"></a>A stochastic processing model for LLM inference</h3><p>根据经验发现，批处理的时延主要取决于token的总数，而不是批处理的具体内容，因此不再需要考虑批处理内容的复杂组合。且这种关系是分段线性的，这意味着在一定范围内，处理时间随令牌数量线性增长，但可能在某个阈值（如达到某个硬件限制或优化点）后，增长率会发生变化，形成多个线性段。</p><h4 id="Model-Setup"><a href="#Model-Setup" class="headerlink" title="Model_Setup"></a>Model_Setup</h4><p>以当前先进的LLM服务系统Sarathi-serve构建一个随机处理模型。</p><ul><li>建模请求到来(requests arrival)<ul><li>时间是离散的，辅以下标$n\in \mathbb{N}\equiv \{1, 2, …, \}$。</li><li>对于每个时隙$n$，允许在同一时隙到达多个请求，于是使用$a_n$表示在该时隙到达系统的请求的数量。从物理上讲，时隙可以对应于时钟频率单位或其他时间单位。</li><li>假设所有的请求都是有序的(也就是有标号的?)。对于$i=1, 2, …, $，假设请求$i$具有$v_p(i)\in \mathbb{N}$个有序的预填充tokens，$v_d(i)\in\mathbb{N}$个有序的解码tokens和特征$z(i)\in \mathcal{K}$(其中$\mathcal{K}$是一个有限集)。特征$z(i)$可以捕获请求的类型/内容。于是使用$(v_p(i), v_d(i), z(i))$描述系统，不过调度算法可能无法观察到它们(特别是$v_d(i)$)。</li><li>假设两个独立的iid序列:<script type="math/tex">\{a_n: n\in \mathbb{N}\} \quad \{(v_p(i), v_d(i), z(i)), i \in\mathbb{N}\}</script><ul><li>其中，单个请求内的$(v_p(i), v_d(i), z(i))$允许相关，且每个量都具有一阶矩。<script type="math/tex">\lambda=\mathbb{E}(a_1), m_p = \mathbb{E}(v_p(1)), m_d=\mathbb{E}(v_d(1))</script></li></ul></li></ul></li><li>迭代级别的批量服务(Iteration-level batch serving)<ul><li>假设系统有一个处理预填充token和解码token的LLM引擎，该引擎的调度器会从多个请求中选取一个批量的token同时处理，以提升计算效率。特别地，来自每个请求的一定数量的未处理tokens可以根据以下可行性约束划分为一个批量。<ul><li>同一个请求的所有的预填充token必须在第一个解码token被加载前处理。</li><li>在请求的预填充和解码阶段，一个token只有在处理完前一个token才能被加载到一个批量中，即一个批量中不能出现同一请求的两个token。</li><li>除非在预填充阶段，允许连续多个token在同一批量。</li><li>同一个批次中不能加载同一个请求的两个解码token(因为解码是自回归一个个出现的？)，也不允许有来自同一个请求的一个预填充token和一个解码token(因为要避免干扰？)。</li></ul></li><li>将一个批次中加载的token的总数$b’$称为$token_load$，并且需满足$b’\le b$，$b$是预定义的$token_budget$。一次迭代为完成对一个批次的处理。<ul><li>以上定义可以参见”Sarathi-serve”的定义。</li><li>$token_budget$是为了确保迭代时间不会因长输入prompt而产生巨大差异，从而对解码阶段的请求的TBT产生负面影响。通常设置为256或512。</li><li>一个批次中的请求的数量才是$batch_size$。</li></ul></li><li>调度算法可以选择$b’$并在每次迭代时确定批次的token组成。<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Visualization%20of%20key%20scheduling%20terminologies%20in%20LLM%20engine.png" alt="img"><ul><li>图中展示的是将预填充和解码阶段的token混合在一个批次处理的选择过程(即使来自不同的请求，也是不提倡混合的，这样会产生干扰)。图中还展示了定义的规则: 预填充阶段的token是可以有多个在同一个批次的，但是解码阶段的token不可以。</li></ul></li></ul></li><li>批次处理时间(Batch processing time)<ul><li>意图建立一个解析形式或分析性的批次处理时间预测建模。</li><li>一个批次的运行时间实际可以很好地近似为一个分段的线性函数，该函数仅取决于token的负载$b’$: $t_{b’} = c + a \cdot \max(0, b’-b_0)$。以上是一个经验公式，通过两类实验获得：<ul><li>一是探究阈值$token_budget$对批次处理时间的关系，其中的重要发现是当阈值固定时，其实token内容的构成对批次处理时间的影响非常小。</li><li>二是探究随着所加载token总数的变化，批次处理时间的增长轨迹可以很好地近似为一个分段线性函数。</li><li>不过该公式与实际处理时间有高度一致性，因为在LLM推理中线性层(前馈神经网络)的计算占有主导地位。再具体些的话，则需要考虑注意力层计算和KV缓存内存开销。</li></ul></li><li>决策是在某个时隙的开始时进行。一个批次的处理会跨越多个时隙，但具有原子性，是不间断的。批次完成是在某个时隙的结束时完成。对于一个请求来说，最后一个解码在一个时隙结束时处理完成后，该请求就会立即离开系统。</li></ul></li></ul><h4 id="马尔可夫链-A-Markov-chain"><a href="#马尔可夫链-A-Markov-chain" class="headerlink" title="马尔可夫链(A Markov chain)"></a>马尔可夫链(A Markov chain)</h4><p>引入离散时间马尔可夫链(DTMCs, discrete-time Markov chains)描述系统的动态。</p><ul><li>在每个时隙$n$，令$Q_n$表示尚未离开的请求集合。假设$\{(P_i(n), D_i(n), Z_i(n)):i\in\mathscr{Q}_n\}$是有序的(按请求到达时间排序)。<ul><li>$P_i(n)$表示在预填充阶段未处理的token的数量，不包括当前在批处理中正在处理的token。</li><li>$D_i(n)$表示在解码阶段未处理的token的数量，不包括当前在批处理中正在处理的token。</li></ul></li><li>令$R(n)$表示当前批处理的剩余时间。当$R(n)=0$时，则可以形成新的批次。否则不需要进行决策。时隙$n$开始时的系统状态定义为:<script type="math/tex">X(n)=(R(n), \{(P_i(n), D_i(n), Z_i(n)):i\in\mathscr{Q}_n\})</script><ul><li>将$\mathscr{X}$定义为在某个时隙所有可能的状态$X(n)$的集合，其中$n\in N$。</li></ul></li><li>调度算法(Scheduling algorithms):<ul><li>令$x=(r, \{(p_i, d_i, z_i):i\in Q\})\in X$表示任意系统状态。</li><li>给定一个状态$x$且$r=0$，该状态表明上个批次已经处理完毕，可以开始新批次处理的状态。调度算法选择一个批处理配置:$\pi(x) = (\delta p_i, \delta d_i)_{i\in Q}$。<ul><li>$\delta p_i$表示来自请求$i$的，要包含在该次批处理中的预处理阶段token的数量。</li><li>$\delta d_i$表示来自请求$i$的，要包含在该次批处理中的解码阶段token的数量。</li></ul></li><li>调度算法$\pi(x)$需要满足以下可行性约束条件:<script type="math/tex; mode=display">p_i \gt 0\quad implies\quad \delta_i^d = 0\\\delta_i^p \le p_i\\\delta_i^d \le 1\\\sum_{i\in\mathscr{Q}}(\delta_i^d + \delta_i^p) = b'\le b</script><ul><li>第一个约束要求在解码之前完成对预填充阶段token的处理。</li><li>第二个约束限制了在某次批处理中的预填充阶段token的数量不大于请求中剩余的预填充阶段token数量。</li><li>第三个约束限制了批处理中的解码阶段只能进行顺序解码(不能一次处理多个token)。</li><li>第四个约束限制了一次批处理中的token总数不大于$token_budget$。</li></ul></li><li>当$r \gt 0$时，强制让$b’=0$。因为在此过程中批处理正在进行中。当然$r=0$时，也可以这样设置。</li></ul></li><li>系统调度(System dynamics)<ul><li>令系统在时隙$n$的状态为: $X(n)=(R(n), \{P_i(n), D_i(n), Z_i(n), i\in Q_n\})$。使用$X’(n)$表示时隙$n$进行决策后的状态，该状态是在考虑时隙$n$中新请求到达之前的状态。特别地，令$\pi(X(n))=(\delta p_i, \delta d_i)_{i\in Q}$：<script type="math/tex">P_i'(n) - P_i(n) - \delta_i^p, i\in\mathscr{Q}_n,\\ D_i'(n) = D_i(n) - \delta_i^d, i\in\mathscr{Q}_n,\\ \mathscr{Q}_n'=\mathscr{Q}_n \setminus\{i\in\mathscr{Q}_n:D'(n) = 0\}</script><ul><li>$\setminus$为集合差运算，表示从左边的集合中移除属于右边集合的元素。</li><li>第一个公式表示经过该时隙处理后剩余的预填充token的数量。</li><li>第二个公式表示经过该时隙处理后剩余的解码token的数量。</li><li>第三个公式表示经过该时隙处理后剩余的请求的数量。</li></ul></li><li>考虑在时隙$n$中会有新请求到达，则时隙$n+1$的状态可以表示为: $X(n+1)=(R(n+1), \{P_i(n+1), D_i(n+1), Z_i(n+1), i\in\mathscr{Q}_{n+1}\})$。时隙$n+1$的请求集合为$\mathscr{Q}_{n+1} = \mathscr{Q}_n’ \bigcup \{\mathscr{A}_n\}$。<script type="math/tex">\forall i\in \mathscr{Q}_n': P_i(n+1)=P_i'(n), D_i(n+1)=D_i'(n), Z_i(n+1)=Z_i(n)\\ \forall i\in \mathscr{A}_n: P_i(n+1)=v_p(i), D_i(n+1)=v_d(i), Z_i(n+1)=z'(i)</script><ul><li>前者是未完成的旧批次，后者是新的批次。</li></ul></li><li>对于剩余的处理事件$R(n+1)$:<script type="math/tex">R(n+1)=\begin{cases} R(n) - 1 & R(n)\gt 0\\ t_{b'} - 1 & R(n) = 0 \end{cases}</script><ul><li>其中$t_{b’}$是处理新批次$b’:=\sum_{i\in\mathscr{Q}(n)}(\delta_i^p + \delta_i^d)$所需的时隙数。前者表示批次仍在处理，后者表示批次已经处理完成，准备处理下一个新的批次。</li><li>假设iid假设成立，在任何调度算法下，$\{X_n:n\in N\}$都是一个DTMC(离散马尔可夫链)。</li></ul></li></ul></li></ul><h4 id="吞吐量优化算法-Throughput-optimal-algorithms"><a href="#吞吐量优化算法-Throughput-optimal-algorithms" class="headerlink" title="吞吐量优化算法(Throughput-optimal algorithms)"></a>吞吐量优化算法(Throughput-optimal algorithms)</h4><p>要解决的核心问题是什么是系统能够达到的最大吞吐率，哪些调度算法能够达到最大吞吐率。</p><p>一个调度算法被成为“吞吐量最优”，意味着在特定的请求到达率($\lambda$)下，能够保持稳定队列的系统。即请求的到达率没有超过系统的最大处理能力，那么系统中的请求数量或令牌数量不会无限增长，系统能够处理所有到达的请求并保持稳定。</p><ul><li>“稳定”: 在数学上是指关联的离散时间马尔可夫链(DTMC)是不可约且正常返(irreducible and positive recurrent)的。<ul><li>不可约: 系统可以从任何一种状态(队列很长、正在处理某种类型的批次)转移到任何其他状态(队列变短、正在处理另一种不同类型的批次)，系统行为不会被限制在某个子集中。</li><li>正常返(positive recurrent): 系统状态不会无限地“漂移”到状态空间之外(队列无限增长)。系统会周期性地回到之前的状态，并且回到任何状态的平均时间是有限的。系统能够在长期运行中保持其状态(如队列长度)在一定界限内，不会无限膨胀。</li></ul></li></ul><p>一类为”work-conserving”的调度算法能够几乎实现系统最大化吞吐率。特别地，当$\pi(x)$能够形成一个满足$b’=b$的批次时，一个调度算法被称为是”work-conserving”的。即对于$\sum_{i\in\mathscr{Q}}(p_i+1(p_i=0))\ge b$，满足: $\pi(x)=\sum_{i\in\mathscr{Q}}(\delta_i^d + \delta_i^p) = b$(不会浪费任何带宽，保证每个批次都到达阈值，通常的实现方法是将预填充和解码的token混合到一个批次处理)。</p><p>$(K_p, K_d)$-FCFS算法:</p><ul><li>令$K_p, K_d$为不小于1的整数。在阶段$f\in\{p, d\}$加载token时，仅加载最多$K_f$个最旧请求的token(token的选择可以遵循任何规则，如最小剩余token优先，因为本身请求就是最旧的了？)</li><li>假设每个DTMC都是不可约的(通过假设$P\{a_n=0\}\gt 0$，即一段时间内没有传入新的请求，如果这段时间足够长，那么最终work-conserving策略会清空队列)。</li><li>假设用户的请求到达系统是独立同分布的，每个请求的预填充 token 数量、解码(decode) token 数量以及特征也是独立同分布的。这些序列彼此独立。满足iid假设和第一矩假设(<a href="#Model_Setup">Model Setup</a>)，更进一步假设$\mathbb{E}[D_1^2]\lt\infin$具有二阶矩。</li><li>如果系统满足以下负载条件，则说明DTMC$\{X_n, x\in N\}$在任意work-conserving的算法中是正常返(positive recurrent)的。其中$\frac{b}{t_b}$表示处理token的最大速率。<script type="math/tex">\lambda(m_p+m_d)\lt\frac{b}{t_b}</script></li><li>如果系统满足以下负载条件，<script type="math/tex">\lambda(m_p+m_d)\gt\frac{b}{t_b}</script><ul><li>则说明对于时隙n时所有未处理的token$|X_n|=\sum_{i\in\mathscr{Q}_n}(P_i(n)+D_i(n))$，永远都有未处理完的token，即不是正常返的，无法终止。<script type="math/tex">\mathbb{P}\{\lim_{n\rightarrow\infin}|X_n|=\infin\}=1</script></li><li>当系统过载时，显然没有任何调度算法可以稳定系统。在此过载情况下，系统内存可能会耗尽，从而导致请求被阻止或丢弃。</li></ul></li><li>在实际部署中，如果一些系统使用的调度算法不是”work-conserving”的，即使满足负载条件$\lambda(m_p+m_d)\lt \frac{b}{t_b}$，该系统也可能是不稳定的。当LLM实例连接到网络以处理AI智能体的工作负载时，即使算法是”work-conserving”的，也可能无法实现最大的吞吐量。</li><li>实际证明参考原论文附录B。</li></ul><h3 id="Stability-issues-of-incumbent-scheduling-algorithms"><a href="#Stability-issues-of-incumbent-scheduling-algorithms" class="headerlink" title="Stability issues of incumbent scheduling algorithms"></a>Stability issues of incumbent scheduling algorithms</h3><p>介绍几种现有的调度算法<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Example%20workload%20and%20where%20work-conserving%20criteria%20are%20broken.png" alt="img"></p><ul><li>Decode-prioritized schedule (without mixed batching, FasterTransformer): 根据FCFS或SJF，优先处理已经完成预填充的请求的解码阶段。对这些请求的解码token批量处理，且不允许预填充和解码的token混合处理。不稳定性在于会导致预填充队列堆积。</li><li>Prefill-prioritized schedule (without mixed batching, vLLM-vanilla): 根据FCFS或SJF，优先处理处于预填充阶段的请求，即对于已进入解码阶段的请求先不处理，直到所有请求的预填充阶段都处理完成才统一对解码阶段进行处理。也不允许预填充和解码的token混合处理。不稳定性在于会导致解码队列堆积。</li><li>Prefill-prioritized schedule (with mixed batching, Orca): 与前者的区别是，允许对预填充和解码的token混合处理。即对于已完成预填充阶段的请求的解码token，尽可能优先填充处理。</li><li>Decode-prioritized chunk schedule (Sarathi-Serve): 当构造待处理批量的时候，尽可能多地填充解码token(不过每个请求最多只能给出1个token，因为自回归顺序解码)。如果批次中的token数量不大于阈值token budget，则填充预填充阶段的token。因为前面已经从经验得到了批次处理时延只和token总数相关的结论，所以该算法中批次的组成并不重要。</li></ul><h3 id="Generalization-to-the-AI-agent-workload"><a href="#Generalization-to-the-AI-agent-workload" class="headerlink" title="Generalization to the AI-agent workload"></a>Generalization to the AI-agent workload</h3><p>Agent工作负载的一个请求可能包含一系列的LLM调用，设计不同LLM引擎之间的交互。此类请求的执行流程可以根据LLM的输出动态变化。针对它进行高效的调度和资源优化对于最大化吞吐量和最小化延迟至关重要。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/AI%20Agent%20Infrastructure.png" alt="img"></p><h4 id="异构并行的LLM引擎-Homogeneous-parallel-LLM-engines"><a href="#异构并行的LLM引擎-Homogeneous-parallel-LLM-engines" class="headerlink" title="异构并行的LLM引擎(Homogeneous, parallel LLM engines)"></a>异构并行的LLM引擎(Homogeneous, parallel LLM engines)</h4><p>假设有$K$个相同的LLM引擎，可以被配置为并行运行。请求可以根据负载均衡算法被路由到任何一个LLM引擎。于是负载条件(load condition)可以表示为: $\lambda(m_p + m_d)\lt \frac{Kb}{t_b}$。</p><ul><li>一种负载均衡的算法是将一个请求(以均匀概率)随机分配给$K$个引擎中的一个。</li><li>另一种算法是分配给目前具有最少数量请求的引擎。</li><li>当然，如果能够所有LLM引擎中未处理的token数量信息是已知的，则可以分配给使用具有最少token数量的引擎。<br>以上算法都是稳定的。证明过程原文中没看到，后续再观察。</li></ul><h4 id="一个处理两种请求类型的LLM引擎-An-LLM-engine-serving-two-types-of-requests"><a href="#一个处理两种请求类型的LLM引擎-An-LLM-engine-serving-two-types-of-requests" class="headerlink" title="一个处理两种请求类型的LLM引擎(An LLM engine serving two types of requests)"></a>一个处理两种请求类型的LLM引擎(An LLM engine serving two types of requests)</h4><p>假设有$j\in\{1, 2\}$两种类型的请求以$\lambda^j$的速度到达，平均token大小为$(m^p_j, m^d_j)$，假设算法满足$\sum_{j\in\{1, 2\}}\lambda^j(m_p^j + m_d^j)\lt\frac{b}{t_p}$，则该算法能使系统稳定。</p><h4 id="LLM引擎的两种网络-Two-networks-of-LLM-engines"><a href="#LLM引擎的两种网络-Two-networks-of-LLM-engines" class="headerlink" title="LLM引擎的两种网络(Two networks of LLM engines)"></a>LLM引擎的两种网络(Two networks of LLM engines)</h4><ul><li>fork-join network<ul><li>假设有四个Agent，记为$\{a_j\}$，$j\in\{0, 1, 2, 3\}$。其中$a_0$的请求到达速率为$\lambda$，当请求被$a_0$处理完后，会分为两个子请求，子请求1由$a_1$处理，子请求2由$a_2$处理。在同一请求的两个子请求都完成后会进行合并，并到达$a_3$进行处理。</li><li>假设$a_j$的token大小为$(m_j^p, m_j^d)$，满足$\lambda(m_j^p+m_j^d)\lt \frac{b^j}{t^j_b}, j\in\{0, 1, 2, 3\}$，在$a_j$中都是$(K_p^j, K_d^j)$-FCFS work-conserving调度算法。则描述系统的DTMC是正常返的。</li><li>如果把$a_3$去掉也不影响。</li></ul></li><li>Rybko-Stolyar(RS) type network<ul><li>假设有一个两个Agent($a_1, a_2$，也可称为LLM引擎)构成的网络。</li><li>假设有两种类型的请求:<ul><li>A类请求先由$a_1$处理一个短任务，然后由$a_2$处理一个长任务。</li><li>B类请求先由$a_2$处理一个短任务，然后由$a_1$处理一个长任务。</li></ul></li><li>不同类型的任务在不同Agent上有不同的平均token大小，表示不同的计算需求。</li><li>经过实验发现，虽然系统满足系统负载要求且使用了”work-conversing”的调度算法，但依然可能使得系统是不稳定的。(但经过翻转优先级设置，又变稳定了)</li><li>说明在 LLM Agent 网络这种更复杂的系统中，并非所有工作守恒的调度策略都能保证系统的稳定性(即吞吐量最优)。调度策略中的优先级设定等细节对于网络的整体稳定性至关重要。</li></ul></li></ul><h3 id="Latency-Optimization"><a href="#Latency-Optimization" class="headerlink" title="Latency Optimization"></a>Latency Optimization</h3><p>本文把其列为未来的工作。不过通过实验发现，Sarathi-Serve中的时延(无论是端到端时延还是预填充时延)都主要受到token budget的影响。</p><ul><li>适中的token budget(512)可以降低端到端延迟的中位数。</li><li>较大的token budget(1024)可以改善预填充时延。</li><li>当token budget较小(128)的时候，由于迭代过程中重复的cross-attention，开销会显著增加(即时延会增加)。</li></ul><p>设计调度算法时，不仅要考虑 token_budget，还要考虑 prefill 长度的分布以及目标工作负载的服务级别目标(SLO，即系统需要满足特定的延迟要求)。</p><h3 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h3><ul><li>各种负载情况下，基于TBT、TTFT、TPOT等性能指标下的最优调度策略。</li><li>更长上下文或测试时工作负载的KV cache内存管理。</li><li>多租户环境下，<ul><li>不同类型请求共同分配在同一组Agent，调度器需要决定如何分配模型的时间和计算资源给不同的请求，尤其是区分高优先级和低优先级请求。<ul><li>尽力而为型(Best-effort)请求: 对延迟不太敏感，可以尽力处理，优先级相对较低。</li><li>敏感型(Latency-critical)请求: 如实时交互，对延迟要求非常高，需要优先处理以确保及时响应。</li><li>将这两种类型的请求放在一起处理(collocated together)增加了调度的复杂性，需要在保证低延迟请求性能的同时，尽量提高整体资源利用率。</li></ul></li><li>将多租户的不同模型共同分配在同一组物理服务器上的调度策略。涉及到更底层的资源管理和分配，调度器需要考虑如何在这些模型之间划分计算、内存等资源，以满足每个模型处理的请求类型的性能需求。</li></ul></li><li>联合优化技术，解决模型自动缩放、多模型资源分配、KV cache策略和负载均衡策略问题。</li><li>还是得挖掘排队理论和实验实践结合。</li></ul><h2 id="Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs"><a href="#Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs" class="headerlink" title="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"></a>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>提交给LLM服务引擎的程序会经历漫长的累积等待时间，主要是由于单个LLM请求和程序的队首阻塞造成的。</li><li>现存的LLM服务引擎(Serving Engines)存在不足:<ul><li>vLLM专注于通过提升KV cache的效率，加速CUDA kernels和提出更好的LLM请求调度算法，优化单个独立的LLM调用或者静态的LLM应用。但未能考虑到程序级别(program-level)的上下文，例如同一程序中的LLM调用之间的依赖关系，或者像总执行时间这样的统计信息。因此通常会因为复杂程序而导致次优的端到端性能，特别是程序的端到端延迟。</li><li>形式上，单线程程序的端到端延迟包括:<ul><li>等待时间(主要优化目标): 程序在引擎上的LLM调用的总排队时间。随着负载增加而增加。优先考虑减少该时间，不仅可以改善程序的延迟，还可以通过缓解队首阻塞(Head-of-line)提高LLM引擎的吞吐量。不仅要考虑每个LLM调用的排队时间，还要考虑程序的等待时间。</li><li>执行时间(主要优化目标): LLM调用的累积前馈时间。程序的执行时间主要取决于LLM引擎管理prefill和decoding阶段的效率。<ul><li>具有长累积prefill的Agentic Workloads中，Autexllix会专注于优化prefill阶段，通过prefill caching消除大部分的prefill计算(存储并重用相关的KV cache条目，跨LLM请求的系统prompt)</li><li>数据局部性: 单个程序中，所有输入长度的缓存命中率都保持在90%以上，表明同一程序的LLM调用共享相同的上下文。考虑不同的程序时，缓存命中率则随着输入长度的增加呈指数衰减，表明不同程序仅仅共享系统prompt。</li></ul></li><li>拦截时间: 等待外部中断花费的时间(工具调用或人为输入)</li></ul></li></ul></li><li>提升吞吐量。</li></ul><h3 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/AI%20Agent%20Infrastructure.png" alt="img"></p><h4 id="LLM-Inference"><a href="#LLM-Inference" class="headerlink" title="LLM Inference"></a>LLM Inference</h4><p>LLM推理过程针对每个请求分为两个阶段进行:</p><ul><li>预填充(Prefill)阶段: 将输入的prompt转换为中间的token状态。</li><li>解码阶段: 基于先前的token序列，自回归地逐个生成新的token。</li></ul><p>为了减少计算量，LLM服务系统利用KV Cache，用于存储中间的token状态，以加速生成新的token。</p><h4 id="LLM-Serving"><a href="#LLM-Serving" class="headerlink" title="LLM Serving"></a>LLM Serving</h4><p>LLM Serving系统需要将LLM调用跨越引擎进行路由和在每个引擎中完成LLM调用的执行。</p><ul><li>采用负载均衡技术，通过实时迁移、分离prefills和decodes、构建前缀树、在引擎之间迁移KV-cache等方法，满足请求的SLO(Service Level Objectives，类似服务指标？)并改善尾部延迟(响应最慢的请求)。</li></ul><p>LLM Serving系统的引擎内部的设计可以参考传统操作系统的成果——内存管理、内核优化和调度方案。</p><ul><li>集成虚拟内存和分页技术，减少KV-cache碎片。</li><li>引入共享内存以缓存跨LLM请求的前缀，管理GPU、CPU和磁盘之间的缓存层次结构，以确保数据能高效地被访问。</li><li>改进GPU内核的实现，加速self-attention计算，流水线化不同的算子，并实现更好的张量或流水线并行。</li><li>设计更好的调度策略，决定哪些LLM请求何时在GPU上执行。将prefills和decodes打包在一起处理以及LLM请求的抢占机制(允许高优先级的短进程中断正在执行的长请求)以缩短响应时间。</li></ul><p>不过以上工作大都集中于优化独立的LLM请求，相当于优化通用程序中的函数调用。</p><p>Autellix专注于程序级别的优化(进程级别)，特别是调度，以弥补在处理动态、多步长LLM Agent程序时的不足。</p><ul><li>类似传统操作系统管理跨CPU内核的整个进程。</li></ul><h4 id="Agentic-Workflow"><a href="#Agentic-Workflow" class="headerlink" title="Agentic Workflow"></a>Agentic Workflow</h4><p>智能体程序是一种动态执行的工作流程(Workflow)，由有向无环图(Directed acyclic graph, DAG)表示，包含来自一个或多个智能体的LLM调用和外部中断(包括工具调用、通用代码执行或人工输入)。假设程序的LLM调用模式仅在运行时出现，因此很难完全提前了解或预测整个图。its scheduler. Autellix’s non-clairvoyant scheduler requires<br>only the cumulative service times of LLM calls within the<br>same program</p><p>单线程的程序的动态特性体现在两个维度上:</p><ul><li>程序的长度——由用户输入的prompt决定。</li><li>LLM调用和中断的顺序——由程序的控制流决定。</li></ul><p>多线程的程序通常会形成有向无环图(DAG)。经典的多线程程序Map-Reduce和基于蒙特卡洛树搜索(常用的搜索和规划方法)的线程数量会随着时间的推移变化，多个线程之间不断地fork和merge，使得图的结构不断变化。而且，多线程中的每个线程可能包含不同的LLM调用和外部中断序列。</p><h4 id="Agentic-Programs"><a href="#Agentic-Programs" class="headerlink" title="Agentic Programs"></a>Agentic Programs</h4><p>主要是在应用层，开发者在此构建复杂的Agentic Programs以编排agent、工具和人类之间的交互。</p><ul><li>Agent被定义为一个包含了指定Agent角色的系统prompt和LLM模型类的元组。</li></ul><p>直接通过LLM调用(call)和外部中断实现交互。外部中断主要是与系统外的工具环境进行交互。现有的Agentic编排框架(LangChain和Autogen)为开发者提供了管理程序的控制流原语，以指定何时执行Agent、调用哪些工具、是否需要人工输入。原语遵循通用的编程语义，包括条件语句、循环、错误处理和终止条件。</p><p>程序还会维护一个全局历史记录，记录 agents、工具和人类的输出。基于 LLM 的聊天机器人会累积 LLM agents 的输出和人类输入之间的消息。Autellix便希望利用全局历史记录，动态构建程序的执行图(DAG)的内部状态，并存储到进程表中。</p><h4 id="Agentic-Applications"><a href="#Agentic-Applications" class="headerlink" title="Agentic Applications"></a>Agentic Applications</h4><p>扩展推理时间计算(LLM调用的次数)以及相应的总解码tokens提高在复杂任务的性能，会使用以下方法: </p><ul><li>逐步推理分解任务。</li><li>显式思维注入指导推理、规划和搜索以探索可能的解决方案。</li><li>自我批评评估行为。</li><li>自我反思从失败中学习。</li><li>多智能体协作。</li></ul><p>单线程的应用主要是结合思维链(CoT)，多线程的应用则主要使用蒙特卡洛树搜索(MCTS)，集成了并行规划、自我批评、自我反思和多智能体协作。分布式的应用程序还可以结合best-of-N采样、集束搜索(beam search)、前瞻(lookahead)技术和遗传算法探索和发现最优解。</p><p>Agentic programs表现出的特性: </p><ul><li>动态性: 同一程序的不同用户prompt可能会产生完全不同的执行模式。</li><li>非确定性: 未来是未知的，程序何时决定终止也是无法准确预测的。</li><li>分布式: 并行调用。</li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Autellix 是一个多引擎 LLM 推理服务系统，包含前端、调度器和负载均衡器。提出的Autellix是一个旨在运行程序而非单个LLM调用的LLM推理系统。经操作系统进程调度器的启发，核心思想是根据LLM调用所属程序的先前已完成调用的总执行时间确定LLM调用的优先级。</p><ul><li>来自较长程序的LLM调用不太可能很快完成，需降低优先级，从而允许较短的程序首先完成，从而有效减少队头阻塞(Head-of-Line blocking)。</li><li>如果一个程序之前已经运行了很长时间(累积执行时间较长)，那么后续的LLM调用优先级会被降低。</li></ul><p>引入一个能够利用全局程序级统计信息(程序在引擎的累积执行时间)，以最大限度地减少等待时间并提高引擎的吞吐量。<br>两种非预知的调度算法:</p><ul><li>假设没有程序的先验工作负载知识，用于单线程程序的PLAS(Program-Level Attained Service): 根据其源程序当前的累积服务时间或执行时间确定LLM调度的优先级。</li><li>用于表示为通用动态有向无环图的多线程程序的ATLAS(Adaptive Thread-Level Attained Service): 基于同一程序中所有线程的最大累积服务时间确定LLM调用的优先级。根据程序的关键路径对调用进行排序。除了减少等待时间，还通过优先处理关键的LLM调用减少程序的完工时间，否则这些调用会阻止程序的进度。</li></ul><p>能够跨多个引擎对程序的LLM调用进行路由。因为观察到一个程序内的LLM调用通常共享共同的前缀和累积的对话状态，而跨程序(跨进程)的调用只共享了系统的prompt。为了避免重复计算程序的KV cache，通过将较长的LLM调用路由到其程序执行的引擎尊重原程序的数据局部性，将较短的调用负载均衡到其他引擎(只需要由系统prompt构成这些调用的大部分输入)。</p><p>Autellix是作为LLM服务引擎的顶部一层实现的，向用户开放有状态的API接口，易于部署。</p><h3 id="Autellix-Design"><a href="#Autellix-Design" class="headerlink" title="Autellix Design"></a>Autellix Design</h3><p>Autellix整体架构由两个关键组件组成: </p><ul><li>程序感知调度器(Program-aware scheduler): 用于减少调用级别(call-level)和程序级别(program-level)的阻塞。</li><li>数据局部感知负载均衡器(Data-locality-aware-load-balancer)</li></ul><h4 id="基础目标"><a href="#基础目标" class="headerlink" title="基础目标"></a>基础目标</h4><ul><li>提升用户程序的整体系统端到端延迟。</li><li>为供应商最大化GPU利用率，提升吞吐量。</li><li>缓解程序资源匮乏，从而提升公平性。</li></ul><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>Autellix是不具备预见性的，无法预先知道要运行程序的到达时间、已执行工作流的结构或总体工作负载的分布。<br>因此当一个程序到达时，Autellix需要在程序运行工程中动态构建其有向无环图作为内部表示(Internal representation, IR)。因此能处理任何调用底层LLM引擎的通用程序，无需开发者提供关于程序结构的额外信息。</p><h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Autellix%20architecture.png" alt="img"></p><p>Autellix的创新还在于它的LLM调用是有状态的，程序从用户的本地机器执行时，就会与Autellix建立会话(Session)。随着时间的推移，程序会发出带有相关会话ID的LLM调用。当会话开始时，Autellix会向维护着的全局进程表添加程序的相应条目，跟踪程序的元数据(总服务时间、线程级元数据、程序LLM调用的等待时间)。引擎级调度器和有状态负载均衡器(两个组件)都利用该表调度下一个解码批处理的LLM调用，并根据其程序的数据局部性将LLM调用路由到合适的引擎。</p><h4 id="Program-Aware-Scheduler"><a href="#Program-Aware-Scheduler" class="headerlink" title="Program-Aware Scheduler"></a>Program-Aware Scheduler</h4><ul><li>目标: 最小化端到端延迟，最小化响应时延，减轻程序级别和调用级别的队首阻塞(head-of-line blocking)</li><li>约束: 没有先验知识。</li></ul><p>基于程序级别的统计数据(例如总的累计运行时间，记录在全局进程表)为每个LLM调用分配优先级，并且允许LLM调用根据优先级动态抢占。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Program-Aware%20Scheduler.png" alt="img"></p><ul><li>Program-level Prioritization<ul><li>Process Table(进程表)<ul><li>维护一个记录了所有运行程序的全局进程表，每当程序的一个LLM调用完成，该表就会相应地更新。<ul><li>服务时间(Serving time): <ul><li>对于单线程程序，是所有LLM引擎的模型执行器上已完成的调用的累积执行时间。</li><li>对于多线程程序，是运行时间最长的线程的关键路径的长度。</li></ul></li><li>等待时间(Waiting time): 调用在LLM引擎调度队列的等待时间，主要用于反饥饿机制(anti-starvation)。</li><li>引擎ID(Engine IDs): 程序正在运行的位置，主要用于负载均衡。</li><li>线程元数据(Thread Metadata): 每个线程对应一个活跃的LLM调用。元数据包括了活跃调用的到达、等待和服务时间。</li><li>最近的调用到来的时间(Most recent call arrival): 程序上一次有新的LLM调用到达的时间，用于追踪过时的程序。</li><li>最近的调用完成的时间(Most recent call completion): 上一次有LLM调用完成的时间，用于检测长时间的外部中断。</li></ul></li></ul></li><li>Single-Threaded Programs<ul><li>使用的调度策略是Least-Attained-Service(LAS) algorithm，并引入Program-Level Attained Service(PLAS)。对于一个单线程的程序来说，运行时间就是所有先前已完成的LLM调用的总运行时间。</li><li>如果提交了第j个LLM调用$c_j$，其程序ID为$c_j.id$，PLAS会根据所有先前具有相同ID的LLM调用的运行时间$t_k$之和，为$c_j$分配一个优先级<script type="math/tex">p(c_j)=\sum_{k < j, c_k.id=c_j.id}t_k</script></li><li>优先值$p(c_j)$越大表明优先级越低，这是因为Autellix倾向于帮助较短的程序更快完成，减少响应时间。</li></ul></li><li>Multi-Threaded Programs<ul><li>由于多线程的程序被建模为由LLM调用构成的动态有向无环图。因此程序的执行时间由对应的DAG上的关键路径决定。<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Critical%20path%20for%20multi-threaded%20programs.png" alt="img"></li><li>引入Adaptive Thread-Level Attained Service(ATLAS)，基于调用的服务的时间，根据程序的关键路径优先处理调用。旨在根据同一程序中其父节点$\mathcal{P}(c_j)$的优先级($p(c_k)$)和已完成的服务时间$t_k$，为每个新到达的LLM调用$c_j$分配一个优先级<script type="math/tex">p(c_j)=\begin{cases}0, & if\quad c_j\quad is\quad root\\ max_{c_k\in\mathcal{P(c_j)}}\{p(c_k) + t_k\} & otherwise\end{cases}</script></li><li>要结合处理最长关键路径线程和优先处理较短程序的目标，则需在全局进程表中加入一个标量: the longest observed critical path。程序中的每个活跃的LLM调用都会继承该值作为其初始的优先级，并且在call完成时，仅当其自身的critical path更长时才更新这个标量。由于给定程序的所有call都是从同一条目进行优先级派生的，因此调度器会自然地将程序的并行调用分组到一起，防止落后的线程延迟程序的完成时间。</li></ul></li></ul></li><li>Preemptive Scheduling(抢占式调度)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/LLM%20call%20lifecycle%20based%20on%20discretized%20prioritization.png" alt="img"><ul><li>Multi-level Program-based Scheduling<ul><li>基于连续优先级进行的调度和抢占程序可能会退化为最坏情况的循环调度。性能比FCFS(先到先处理)更差，导致不必要的上下文切换(CPU和GPU之间频繁的KV缓存交换)。于是Autellix将优先级离散化为一组有限的队列(参考操作系统的多级反馈队列)，假设有K个优先级递减的队列$(Q_1, Q_2, …, Q_K)$，每个队列$Q_i$覆盖一个优先级范围$[Q_i^{lo}, Q_i^{hi})$，其中$Q_1^{lo}=0, Q_K^{hi}=\infin, Q_{i+1}^{lo}=Q_i^{hi}$。</li></ul></li><li>Anti-Starvation<ul><li>为了解决低优先级的较长程序一直无法被处理而产生饥饿的问题。Autellix利用全局进程表衡量程序级别的饥饿情况，对于程序$p$的某个LLM调用$c$，如果总等待时间($W_{total}=W_p+W_c$)与总服务时间$T_{total}=T_p+T_c$之比超过阈值$\beta$，则将调用$c$提升到$Q_1$队列，并需要将$c$的等待时间和执行时间重置为0。可通过修改$\beta$的值权衡平均响应和公平性。<script type="math/tex">\frac{W_{total}}{T_{total}}\ge \beta</script></li></ul></li><li>Memory Management(内存管理)<ul><li>抢占式的调度会导致频繁的GPU-CPU切换(主要是KV-cache块会被反复交换以服务不同的请求)。</li><li>Autellix通过多步调度减少总的交换次数，每N个解码步骤运行一次调度器，而不是每一步都运行。调度器还会过度配置一些已经在GPU上但正在排队的请求，以便在某些请求在N步之前完成时能够立即添加新请求。</li><li>还采用一个更高效的交换内核，不是为每个块调用单独的异步传输，而是将所有的KV块收集到一个连续的缓冲区中，通过一次操作完成传输，从而减少碎片、降低每个块的开销和降低端到端的延迟提高PCIe的带宽利用率。</li></ul></li></ul></li></ul><h4 id="Load-Balancer"><a href="#Load-Balancer" class="headerlink" title="Load Balancer"></a>Load Balancer</h4><p>随着负载的增加，部署多个引擎副本是必要的。但是不考虑数据局部性的话会导致次优性能。</p><ul><li>对于短请求，通过实验发现由于是共享system prompt，所以缓存命中率其实差异不大，无需过分要求数据局部性。于是将其路由到当前负载最低的引擎，有助于负载均衡。</li><li>对于长请求，则需先检查该程序是否已经在一个引擎运行过，如果已有指定引擎，则将长请求只分配给指定引擎(哪怕会排队)，从而最大化KV-cache的重用率。否则，将其路由到当前负载最低的引擎，并要将该引擎记录到全局进程表。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Load%20Balancer.png" alt="img"></p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="Fronted"><a href="#Fronted" class="headerlink" title="Fronted"></a>Fronted</h4><p>提供一种底层有状态的接口，即对于开发者来说，看起来是无状态的。用户只需将Autellix的库导入到他们的Python应用程序中，并在程序初始化时，Autellix会自动向后端发出一个start_session请求。此操作会返回一个唯一的会话标识符，并在进程表中创建一个相应的条目。后续的LLM调用会被透明地标注上相应的会话ID、程序ID和线程ID，然后再被分派到后端。当程序完成或遇到错误时，Autellix会调用end_session，从进程表中移除相关的条目。但并未对用户权限进行限制，即用户能够修改底层状态。</p><h4 id="LLM-Engine"><a href="#LLM-Engine" class="headerlink" title="LLM Engine"></a>LLM Engine</h4><p>基于vLLM v0.6.1构建，仅集成了新策略(PLAS, ATLAS和MLFQ)和用于提高效率的内存交换内核(Load Balancer部分的算法)对调度程序进行修改。</p><p>在vLLM中，每个KV块都通过cudaMemcpyAsync单独传输，从而产生了许多小的碎片化传输，降低PCIe带宽的利用率，导致重复DMA设置等高开销。为解决该问题，分配一个主机缓冲区，将所有的KV块合并成一个连续的块，从而实现一次性批量传输。</p><h4 id="Multi-engine"><a href="#Multi-engine" class="headerlink" title="Multi-engine"></a>Multi-engine</h4><p>在现有的AsyncLLMEngine上构建AsyncMultiLLMEngine层。每个LLM引擎的副本都在一个独立的Python进程中运行，以实现隔离和并行性。这些副本通过元引擎作为中心协调者进行管理。元引擎和各个引擎副本之间使用标准的IPC原语(mp.Queue, mp.Pipe)通信。处理流程如下:</p><ul><li>当元引擎接收到一个来自前端的请求时，将请求分配给一个合适的引擎副本。同时立即向前端返回一个”未来对象(future-like object)”，表示结果将在未来可用，这样前端就不会被阻塞。</li><li>被选中的引擎进程异步执行接收到的任务，在任务完成后通过IPC通道将结果返回给元引擎。</li><li>元引擎收到结果后，解析之前返回给前端的未来对象，并将最终输出结果填写到未来对象并再次打包发送给前端。</li></ul><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><h4 id="Graph-Optimization"><a href="#Graph-Optimization" class="headerlink" title="Graph Optimization"></a>Graph Optimization</h4><p>Autellix是假设完全没有先验知识的，虽然效果不错，但是通过实验也可以看到和拥有完全先验知识的SRPT算法有不小的差距。当然，完全先验知识是不现实的，但可以考虑像编译器的分支预测和推测执行一样的思路，预测LLM调用。</p><h4 id="Post-Training"><a href="#Post-Training" class="headerlink" title="Post-Training"></a>Post-Training</h4><p>后训练，如Deepseek-R1和OpenAI的o1/o3模型，端到端强化学习进行后训练，优化思维过程。在此过程中，如果使用分布式强化学习系统加速，交替进行分布式在线采样和训练，则非常适用Autellix框架(能够减少批次采样的总完成时间)。</p><h2 id="Tempo-Application-aware-LLM-Serving-with-Mixed-SLO-Requirements"><a href="#Tempo-Application-aware-LLM-Serving-with-Mixed-SLO-Requirements" class="headerlink" title="Tempo: Application-aware LLM Serving with Mixed SLO Requirements"></a>Tempo: Application-aware LLM Serving with Mixed SLO Requirements</h2><h2 id="Cost-Efficient-Serving-of-LLM-Agents-via-Test-Time-Plan-Caching"><a href="#Cost-Efficient-Serving-of-LLM-Agents-via-Test-Time-Plan-Caching" class="headerlink" title="Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"></a>Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LLM-Agent-Serving&quot;&gt;&lt;a href=&quot;#LLM-Agent-Serving&quot; class=&quot;headerlink&quot; title=&quot;LLM Agent Serving&quot;&gt;&lt;/a&gt;LLM Agent Serving&lt;/h1&gt;&lt;h2 id=&quot;Throu</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Agentic Workflow</title>
    <link href="http://zjn-astonishe.github.io/2025/06/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-09-Agentic%20Workflow/"/>
    <id>http://zjn-astonishe.github.io/2025/06/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-09-Agentic%20Workflow/</id>
    <published>2025-06-09T06:24:58.000Z</published>
    <updated>2025-06-12T02:16:58.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Agentic-Workflow"><a href="#Agentic-Workflow" class="headerlink" title="Agentic Workflow"></a>Agentic Workflow</h1><h2 id="Toward-Super-Agent-System-with-Hybrid-AI-Routers"><a href="#Toward-Super-Agent-System-with-Hybrid-AI-Routers" class="headerlink" title="Toward Super Agent System with Hybrid AI Routers"></a>Toward Super Agent System with Hybrid AI Routers</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>早期的模型部署技术在应对高并发请求时表现不佳，效率问题凸显。这需要解决包括模型选择、任务分配、系统容错等在内的一系列复杂的技术挑战。</p><ul><li>当系统接到某项任务时，需要智能地判断是交给某个 Agent 单独完成，还是交给多个 Agent 共同协同工作。</li><li>提升效率的关键：选“对的”模型，比选“大”模型更重要</li><li>用户的一条简单提示背后，不再是单一模型生成应答，而是一整套智能体系统在幕后完成复杂的任务分解、协作和执行流程。</li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>本质是一个”Prompt-to-Model”的分类系统。<br>通过谷歌的 BERT 等语义理解模型和监督学习方法，通过分析任务需求，动态选择最适合的专家模型对相关请求进行处理。<br><img src="" alt="Overview of the Super Agent System"><br><img src="" alt="Router 系统对路由器数据准备、路由器模型训练和部署流程的概述"></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>考虑到真实高并发场景（如每秒数万次请求）下单点故障的情况，研究人员设计了一种冗余机制：当某次请求失败时，系统可自动切换到备用结点重新发起请求，以保障系统的持续正常运行。</p><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>TensorOpera 与高通合作，成功将 Router 部署在高通显卡上，显著提升了能效比和性价比。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="意图路由和自动规划-Intent-Router-Planner"><a href="#意图路由和自动规划-Intent-Router-Planner" class="headerlink" title="意图路由和自动规划(Intent Router + Planner)"></a>意图路由和自动规划(Intent Router + Planner)</h4><p>用户只需要输入自然语言请求，系统会自动识别其意图并路由到合适的任务Agent。<br>系统还能自动生成多 Agent 协作的执行计划。</p><ul><li>以用户需要写一篇关于不稳定关税交易策略的文章为例，该系统会自动规划并协调三个 Agent 构成完整的流程，它们分别负责：查找实时关税信息、设计金融策略，以及实现 C++ 代码。</li><li>现阶段研究人员正在尝试更大规模的 Agent 协作，尝试让 1000 个 Agent 进行协作和交流，共同完成盖房子等更复杂的任务。</li></ul><p><img src="" alt="通过函数调用对用户意图进行分类"><br><img src="" alt="自动智能体工作流计划"></p><h4 id="任务专用智能体-Task-Agents"><a href="#任务专用智能体-Task-Agents" class="headerlink" title="任务专用智能体(Task Agents)"></a>任务专用智能体(Task Agents)</h4><p>用专业的Agent专注完成特定的任务。<br>每个 Task Agent 都是一个“任务专家”，集成了记忆（Memory）、工具使用能力（Tool Use）与检索增强生成能力（RAG，Retrieval-Augmented Generation）。</p><ul><li>可调用数据库、执行 API 操作，甚至与物理世界进行交互，从而实现从数字到物理的复杂任务自动化。</li></ul><p><img src="" alt="Task Agent"></p><h4 id="智能模型选择系统-Model-Router"><a href="#智能模型选择系统-Model-Router" class="headerlink" title="智能模型选择系统(Model Router)"></a>智能模型选择系统(Model Router)</h4><p>不同厂商的模型在架构、参数规模和优化目标上存在差异。需要路由选择当前任务的“最优解”模型。因此该模块为核心模块。</p><ul><li>综合考虑指标为: 准确率、响应速度、成本。</li></ul><p><img src="" alt="具有成本优化配置的模型路由器"></p><h4 id="端云混合部署"><a href="#端云混合部署" class="headerlink" title="端云混合部署"></a>端云混合部署</h4><p>并非传统的单体架构，而是采用模块化、插件化的设计理念，为不同场景需求提供灵活可配置的解决方案。</p><ul><li>开发者只需定义任务意图与流程规划，系统可自动分配 Agent 执行。</li><li>可根据实际场景灵活部署在本地、边缘或云端。</li><li>具备极强的“系统演化能力”，可随着模型更新动态替换，进而具备持续进化的能力。</li></ul><h2 id="AFlow-Automating-Agentic-Workflow-Generation"><a href="#AFlow-Automating-Agentic-Workflow-Generation" class="headerlink" title="AFlow: Automating Agentic Workflow Generation"></a>AFlow: Automating Agentic Workflow Generation</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>大型语言模型（LLMs）在解决多样化的复杂任务方面表现出色，但设计和优化其工作流需要<strong>大量人力</strong>，限制了可扩展性和跨任务技能迁移。</p><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>AFLOW框架基于蒙特卡洛树搜索（MCTS），旨在自动生成和优化代理工作流。</p><ul><li>将工作流优化问题进行了形式化定义，工作流建模为由代码连接的结点和边，表示操作间的逻辑、依赖关系和流程，形成一个庞大的搜索空间。/</li><li>使用预定义的操作符作为构建块，并结合多种创新技术（如软混合概率选择、LLM驱动的结点扩展、执行评估和经验反向传播）来高效探索这一空间，自动创建优化的工作流，以最大化任务性能并减少人工干预。</li></ul><h3 id="智能工作流-Agentic-Workflow"><a href="#智能工作流-Agentic-Workflow" class="headerlink" title="智能工作流(Agentic Workflow)"></a>智能工作流(Agentic Workflow)</h3><p><img src="" alt="工作流组成"></p><h4 id="结点-Node"><a href="#结点-Node" class="headerlink" title="结点(Node)"></a>结点(Node)</h4><p>工作流$W$定义为一系列通过边连接的调用大语言模型的结点。每个结点$N_i$表示由LLM执行的具体操作。</p><script type="math/tex; mode=display">N=\{N_1, N_2, ..., N_i, ...\}</script><ul><li>模型$M$: 在结点$N_i$调用的具体语言模型。</li><li>提示$P$: 提供给模型的输入或任务描述。</li><li>温度$\tau$: 控制结点$N_i$中LLM输出随机性的参数。</li><li>输出格式$F$: 模型输出的结构化格式。</li></ul><h4 id="边-Edge"><a href="#边-Edge" class="headerlink" title="边(Edge)"></a>边(Edge)</h4><p>边$E$是定义结点关系的抽象结构，控制执行顺序(表示结点之间的执行顺序和逻辑关系)。可用结构: </p><ul><li>图结构: 一种灵活的结构，表示结点间的层次、顺序或并行关系，支持复杂的分支工作流。</li><li>代码: 一种全面的表示方法，能够表达线性序列、条件逻辑、循环，并结合图或网络结构，为LLM工作流执行提供精确控制。<ul><li>作为AFlow的主要边结构，最大化表达能力。</li></ul></li><li>神经网络: 一种可以表示结点间复杂非线性关系的结构，基于输入和反馈实现自适应和可学习的工作流。</li></ul><h3 id="自动工作流优化-Automated-Workflow-Optimization"><a href="#自动工作流优化-Automated-Workflow-Optimization" class="headerlink" title="自动工作流优化(Automated Workflow Optimization)"></a>自动工作流优化(Automated Workflow Optimization)</h3><p>给定任务$T$和评估函数$G$，自动工作流优化的目标是发现一个工作流$W$，使得$G(W, T)$最大化。可以被形式化为一个搜索过程，其中算法$A$探索搜索空间$S$以确定最优的工作流配置。</p><h4 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h4><script type="math/tex; mode=display">S=\{(N, E)\mid E\in\epsilon\}\\\mathcal{N}=\{N(M, \tau, P, F)\mid M\in \mathcal{M}, \tau\in[0, 1], P\in\mathcal{P}, F\in\mathcal{F}\}</script><ul><li>$\mathcal{M}$: 语言模型集合</li><li>$\mathcal{P}$: 提示集合</li><li>$\mathcal{F}$: 输出格式集合</li><li>$\epsilon$: 边结构集合</li></ul><h4 id="工作流优化问题"><a href="#工作流优化问题" class="headerlink" title="工作流优化问题"></a>工作流优化问题</h4><script type="math/tex; mode=display">W=A(S, G, T)\\W^*=\arg\max_{W\in S}G(W, T)</script><h3 id="AFlow"><a href="#AFlow" class="headerlink" title="AFlow"></a>AFlow</h3><p>为了增强搜索效率，AFlow简化了搜索空间，固定了模型$M$、温度$\tau$和格式$F$等关键参数，主要只关注代码表示的边$E$和提示$P$。并且引入了操作符$\mathcal{O}$。于是将优化问题形式化为:</p><script type="math/tex; mode=display">S_{AFlow} = \{(P_1, ..., P_n, E, O_1, ..., O_n)|P_i\in \mathcal{P}, E\in\epsilon, O_i\in \mathcal{O}\}\\ W^*=AFLOW(S_{AFlow}, G, T)</script><ul><li>$S_{AFlow}$表示由提示词、边、操作符组成的集合。</li><li>$W^*$表示通过AFLOW框架在给定$S_{AFlow}$、评估函数$G$、任务$T$下最大化的最优工作流配置。</li></ul><h4 id="操作符-O-模板"><a href="#操作符-O-模板" class="headerlink" title="操作符$O$模板"></a>操作符$O$模板</h4><p>操作符$O$是一系列常见的操作，以提升不同任务的搜索效率。</p><ul><li>Generate: 生成</li><li>Format: 格式化</li><li>Review and Revise: 审查与修订</li><li>Ensemble: 集成</li><li>Test: 测试</li><li>Programmer: 程序编写</li><li>自定义…</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> metagpt.actions <span class="keyword">import</span> Action</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleWriteCode</span>(<span class="title class_ inherited__">Action</span>):</span><br><span class="line">    PROMPT_TEMPLATE: <span class="built_in">str</span> = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Write a python function that can &#123;instruction&#125; and provide two runnnable test cases.</span></span><br><span class="line"><span class="string">    Return ```python your_code_here ``` with NO other texts,</span></span><br><span class="line"><span class="string">    your code:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    name: <span class="built_in">str</span> = <span class="string">&quot;SimpleWriteCode&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, instruction: <span class="built_in">str</span></span>):</span><br><span class="line">        prompt = self.PROMPT_TEMPLATE.<span class="built_in">format</span>(instruction=instruction)</span><br><span class="line"></span><br><span class="line">        rsp = <span class="keyword">await</span> self._aask(prompt)</span><br><span class="line"></span><br><span class="line">        code_text = SimpleWriteCode.parse_code(rsp)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> code_text</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_code</span>(<span class="params">rsp</span>):</span><br><span class="line">        pattern = <span class="string">r&quot;```python(.*)```&quot;</span></span><br><span class="line">        <span class="keyword">match</span> = re.search(pattern, rsp, re.DOTALL)</span><br><span class="line">        code_text = <span class="keyword">match</span>.group(<span class="number">1</span>) <span class="keyword">if</span> <span class="keyword">match</span> <span class="keyword">else</span> rsp</span><br><span class="line">        <span class="keyword">return</span> code_text</span><br></pre></td></tr></table></figure><h4 id="提示词-P-模板"><a href="#提示词-P-模板" class="headerlink" title="提示词$P$模板"></a>提示词$P$模板</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">REFINE_ANSWER_PROMPT = <span class="string">&quot;&quot;&quot;给出数学问题以及代码执行的输出，请提供一个格式良好且详细的解答。遵循以下指南：</span></span><br><span class="line"><span class="string">1. 以清晰的问题陈述开始。</span></span><br><span class="line"><span class="string">2. 解释所采用的方法及使用的任何公式或概念。</span></span><br><span class="line"><span class="string">3. 使用LaTeX标记展示数学表达式的逐步计算过程。</span></span><br><span class="line"><span class="string">4. 解读代码输出并将其融入到你的解释中。</span></span><br><span class="line"><span class="string">5. 提供最终答案，并用\boxed&#123;&#125; LaTeX标记包裹。</span></span><br><span class="line"><span class="string">6. 确保所有数学符号都使用LaTeX格式。你的回答应当全面、数学上严谨且易于理解。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">GENERATE_SOLUTION_PROMPT = <span class="string">&quot;&quot;&quot;请逐步解决给定的数学问题。遵循以下指南：</span></span><br><span class="line"><span class="string">1. 清晰地陈述问题。</span></span><br><span class="line"><span class="string">2. 概述方法及相关的公式或概念。</span></span><br><span class="line"><span class="string">3. 提供详细的计算过程，使用LaTeX标记表示数学表达式。</span></span><br><span class="line"><span class="string">4. 解释每一步推理的理由。</span></span><br><span class="line"><span class="string">5. 最终答案需用\boxed&#123;&#125; LaTeX标记包裹。</span></span><br><span class="line"><span class="string">6. 确保所有数学符号都使用LaTeX格式。你的解答应该详尽、数学正确且易于理解。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">DETAILED_SOLUTION_PROMPT = <span class="string">&quot;&quot;&quot;为给定的数学问题提供一个全面的、步骤详细的解答。你的回答应包括：</span></span><br><span class="line"><span class="string">1. 对问题的清晰重述。</span></span><br><span class="line"><span class="string">2. 解释涉及的数学概念和定理。</span></span><br><span class="line"><span class="string">3. 通向解答的详细逻辑步骤。</span></span><br><span class="line"><span class="string">4. 每个步骤的清楚解释，包括背后的理由。</span></span><br><span class="line"><span class="string">5. 所有的数学表达式和方程都应使用LaTeX格式。</span></span><br><span class="line"><span class="string">6. 如果适用，提供视觉辅助或图表（在文本中描述）。</span></span><br><span class="line"><span class="string">7. 最终答案明确标注并用\boxed&#123;&#125; LaTeX标记包裹。</span></span><br><span class="line"><span class="string">8. 如果相关，简要说明结果的重要性。</span></span><br><span class="line"><span class="string">确保你的解答严格、易于跟随并且对学习该概念的人具有教育意义。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="代码边-E-模板"><a href="#代码边-E-模板" class="headerlink" title="代码边$E$模板"></a>代码边$E$模板</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">code_python = generateCode()</span><br><span class="line">max_iter = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_iter):</span><br><span class="line">    flag = runcode(code_python)</span><br><span class="line">    <span class="keyword">if</span> flag:</span><br><span class="line">      <span class="keyword">return</span> true</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        code_python = regenerateCode()</span><br></pre></td></tr></table></figure><h4 id="AFlow框架总体结构"><a href="#AFlow框架总体结构" class="headerlink" title="AFlow框架总体结构"></a>AFlow框架总体结构</h4><p><img src="" alt="Overall AFLOW framework"></p><p>AFlow框架总体结构是</p><ul><li>设置一个由仅具有灵活提示参数的结点、给定的操作符集合以及代码表示的边组成的搜索空间。</li><li>在此搜索空间中执行基于蒙特卡洛树搜索(MCTS)的变体搜索。执行以下操作直到达到最大迭代次数或满足收敛条件为止。<ul><li>软混合概率选择: Soft-Mixed Probability Selection</li><li>基于LLM的扩展: LLM-Based Expansion</li><li>执行评估: Executing Evaluation</li><li>经验反向传播: Experience Backpropagation</li></ul></li></ul><h4 id="算法-文字描述"><a href="#算法-文字描述" class="headerlink" title="算法(文字描述)"></a>算法(文字描述)</h4><p>包括初始化、选择、扩展、评估、回传和终止条件等。</p><ul><li>初始化<ul><li>输入: 初始工作流$W_0$，评估器$G$，数据集$D$，迭代轮数$N$，操作符集$\mathcal{O}$，前k个结点$k$，早期停止轮数$n$。</li><li>输出: 最优工作流$W^*$</li><li>步骤: <ul><li>初始化结果集$result\leftarrow\empty$和经验集$experience\leftarrow\empty$。</li><li>将数据集$D$随机划分为验证集$D_V$(20%)和测试集$D_T$(80%)。</li><li>在验证集$D_V$上执行初始工作流$W_0$，得到分数$scores$。</li><li>根据分数$scores$选择高方差的实例，形成最终的验证集$D_V$。</li></ul></li></ul></li><li>迭代优化(从第1轮到第N轮)<ul><li>如果当前是第1轮，选择初始工作流$W_0$作为父结点$parent$。</li><li>如果不是，则根据软混合概率选择策略选择父结点$parent$。候选结点包括得分最高的$k$个结点和初始结点(公式见下一节<a href="#算法(伪代码">算法(伪代码)</a>))。</li><li>加载父结点的上下文$context$。</li><li>扩展: 使用LLM优化器$Optimizor$，利用过去的修改和相应的改进或失败生成新的工作流$W_{round}$和修改$modification$(新的提示和结点间连接)。</li><li>评估: 在验证集$D_V$上执行5次新的工作流$W_{round}$，计算平均分数$avgScore$和标准差。</li><li>回传: 创建经验$experience$并将其添加到经验集$experiences$。</li><li>如果$avgScore$高于最佳分数$bestScore$，则更新$W^*$和$bestScore$。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">results = []        <span class="comment"># 结果集，存储每轮迭代结果，包括轮数、分数和成本</span></span><br><span class="line">experiences = []    <span class="comment"># 经验集，存储每轮迭代的经验，包括父结点、修改和平均分数</span></span><br><span class="line">N = <span class="number">20</span></span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">n = <span class="number">5</span></span><br><span class="line">D_V, D_T = RandomSplit(D, <span class="number">0.2</span>, <span class="number">0.8</span>) <span class="comment"># 数据集分割</span></span><br><span class="line">scores = Execute(W_0, G, D_V) <span class="comment"># 初始工作流执行</span></span><br><span class="line">D_V = SelectHighVarianceInstances(D_V, scores, threshold) <span class="comment"># 高方差实例选择，形成最终的验证集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代优化</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">round</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N + <span class="number">1</span>):</span><br><span class="line">  <span class="comment"># 选择父结点</span></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">round</span> == <span class="number">1</span>:</span><br><span class="line">      parent = W_0</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      parent = SelectParent(results)</span><br><span class="line">  <span class="comment"># 加载上下文</span></span><br><span class="line">  context = LoadContext(parent, experiences)</span><br><span class="line">  <span class="comment"># 生成新工作流</span></span><br><span class="line">  W_round, modification = Optimizer(context, O)</span><br><span class="line">  <span class="comment"># 执行新工作流</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">      score, cost = Executor(W_round, G, D_V)</span><br><span class="line">      results.append((<span class="built_in">round</span>, score, cost))</span><br><span class="line">  avgScore = CalculateAverageScore(results[<span class="built_in">round</span>])</span><br><span class="line">  <span class="comment"># 创建经验</span></span><br><span class="line">  experience = CreateExperience(parent, modification, avgScore)</span><br><span class="line">  experiences.append(experience)</span><br><span class="line">  <span class="comment"># 更新最优工作流</span></span><br><span class="line">  <span class="keyword">if</span> avgScore &gt; bestScore:</span><br><span class="line">      W_star = W_round</span><br><span class="line">      bestScore = avgScore</span><br></pre></td></tr></table></figure><h4 id="算法-伪代码"><a href="#算法-伪代码" class="headerlink" title="算法(伪代码)"></a>算法(伪代码)</h4><p>整个循环流程是选择最优工作流，加载工作流上下文，生成新工作流，执行新工作流，保存经验，反向传播更新最优工作流。</p><p><img src="" alt="Detailed Explanation of the AFlow Algorithm"></p><ul><li>$D$: 数据集，$W_0$: 初始工作流, $G$: 评价函数, $N$: 循环次数, $O$: 操作符, $Top-k$: 前k个。</li><li>终止条件: 如果迭代n次都没发生变化或者到达最大迭代次数就停止算法。</li><li>$RandomSplit(D, 0.2, 0.8)$: 拆分数据集$D$，用20%作为验证集，80%作为测试集。</li></ul><p>在选择阶段，通过在验证集上评估一个空工作流创建初始工作流程。然后使用一种结合了均匀分布和基于得分的加权概率分布的软混合概率选择策略，从top-k工作流程以及初始工作流程中持续选择工作流程。不仅保证了探索新工作流程的能力，同时有助于避免陷入局部最优。</p><p><img src="" alt="一些函数的具体实现"></p><ul><li>$Execute(W_0, G, D_V)$: $D_V$数据集使用$W_0$初始工作流，运行$G$平均函数得到对应的分数。</li><li>$SelectHighVarianceInstances(D_V, scores, threshold)$: 选择$D_V$数据集中高方差的样本。</li><li>$round$: 当前迭代次数。</li><li>$parent$: 当前工作流结点。</li><li>$SelectParent(results)$: 根据评估结果选择工作流。<ul><li>根据分数降序排序，选择前k个结果，并用对应的分数计算混合概率: <script type="math/tex">P_{mixed}(i)=\lambda\cdot\frac{1}{n} + (1-\lambda)\frac{\exp(\alpha\cdot(s_i-s_{max}))}{\sum_{j=1}^{n}\exp(\alpha\cdot(s_j-s_{max}))}</script><ul><li>$n$是工作流的数量。</li><li>$s_i$是工作流$i$的分数。</li><li>$s_max$是最大分数。</li><li>$\alpha$用来控制分数的影响。</li><li>$\lambda$用来平衡均匀和加权的概率。</li></ul></li></ul></li><li>$LoadContext(parent, experience)$: 加载$parent$结点的上下文信息，包含修改和历史信息。</li><li>$Optimizer(context, O)$: 通过上下文信息和操作，生成新的工作流和修改。</li><li>$results$: 存储每轮迭代</li><li>$experiences$: 存储每轮迭代的经验，包含$parent$结点、修改和平均分数。</li><li>$context$: 上下文信息，包含修改和历史信息。</li><li>$CalculateAverageScore(results[round])$: 计算在当前工作流下的平均得分。</li><li>$CreateExperience(parent, modification, avgScore)$: 创建经验，包括$parent$结点、修改和平均分数等信息。</li></ul><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>完整数据集: <ul><li>GSM8K</li><li>HumanEval</li><li>MBPP</li></ul></li><li>部分数据集: <ul><li>HotpotQA和DROP中抽取1000条样本。</li><li>MATH从难度等级5的4个经典问题类型中挑选617个问题。<ul><li>Combinatorics: 组合数学</li><li>Probability: 概率论</li><li>Number Theory: 数论</li><li>Pre-algebra: 代数</li><li>Pre-calculus: 微积分</li></ul></li></ul></li></ul><h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ul><li>Manually designed methods<ul><li>IO(直接调用LLM)</li><li>Chain-of-Thought</li><li>Self Consistency CoT(5 answers)</li><li>MultiPersona Debate</li><li>Self-Refine(max 3 iteration rounds) and </li><li>MedPrompt(3 answers and 5 votes)</li></ul></li><li>Automated workflow optimization method<ul><li>ADAS</li></ul></li></ul><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><ul><li>Solve Rate(%): GSM8K, MATH</li><li>pass@1(for code accuracy): HumanEval, MBPP</li><li>F1 Score: HotpotQA, DROP</li><li>The cost by tracking token usage(跟踪token的使用量) to construct a pareto front: for all datasets</li></ul><h2 id="Flow-Modularized-Agentic-Workflow-Automation"><a href="#Flow-Modularized-Agentic-Workflow-Automation" class="headerlink" title="Flow: Modularized Agentic Workflow Automation"></a>Flow: Modularized Agentic Workflow Automation</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>现有的LLM多智能体框架在工作流动态调整方面具有局限性。</p><ul><li>忽略动态任务重新分配，大部分工作流都是静态的、预先设定的，对任务环境变化的适应性有限。</li><li>在工作流设计中缺乏模块化和并行性。<ul><li>例如AutoGen虽然也能自动生成子任务和智能体，但是子任务是线性执行的，无法并行，在时延和效率上有改进点。</li></ul></li></ul><h3 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h3><p>在当前已有的多智能体框架基础上，通过在任务执行期间实现动态更新工作流程，并在规划工作流程时鼓励模块化，从而进一步改进现有的通用多智能体框架。</p><ul><li>支持基于全局信息对整个流程进行局部更新，使智能体能够有效适应意外情况，同时保持系统的一致性和连贯性。</li><li>模块化能设计具有高并行性和低依赖复杂性的工作流程。能并发执行子任务和最大限度地减少由复杂相互依赖关系引起的瓶颈，从而提高效率、鲁棒性和可扩展性。</li></ul><h3 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h3><h4 id="Formulating-a-Workflow-as-an-AOV-Graph"><a href="#Formulating-a-Workflow-as-an-AOV-Graph" class="headerlink" title="Formulating a Workflow as an AOV Graph"></a>Formulating a Workflow as an AOV Graph</h4><p>AOV图全称“Activity on Vertex graph”，是一种有向无环图。图中结点表示的是子任务，边表示的是结点的优先关系。主要功能是高效地可视化依赖关系和子任务的执行顺序。</p><ul><li>$G=(V, E, A)$<ul><li>$V$表示子任务集合。</li><li>$E\subset V\times V$<ul><li>$e_{ij}=(v_i, v_j)\in E$表示子任务$v_i$必须在$v_j$之前完成。</li></ul></li><li>$A$表示用于所有子任务的智能体集合。$a_j\in A$表示负责执行子任务子集$T_j\subset V$的智能体。</li></ul></li></ul><h4 id="Modularity-in-a-Workflow"><a href="#Modularity-in-a-Workflow" class="headerlink" title="Modularity in a Workflow"></a>Modularity in a Workflow</h4><p>系统设计的模块化涉及将系统划分为独立运行(需降低互相的依赖性)的模块，每个模块负责特定的功能，从而可以专注于各个组件，而不会影响整个系统。有助于系统的可扩展性和灵活性。</p><ul><li>工作流程中额外的依赖关系会降低子任务的预期成功率。<ul><li>考虑两个拓扑排序的工作流$A$和$B$，每个工作流都包含根据执行顺序排序的$N$个子任务。<ul><li>随机失败可能性: 每个子任务$v\in\mathcal{T}$有$p_f(0\lt p_f\lt 1)$的概率失败。</li><li>工作流$B$的额外依赖: 存在至少一个子任务$v^<em>\in\mathcal{T}$和一个子任务$b\in\mathcal{T}$，使得工作流$B$中$v^</em>$的直接前置任务(依赖)的集合为$D_B(v^<em>)=D_A(v^</em>)\bigcup\{b\}$，其中$D_A(v^<em>)$是工作流$A$中$v^</em>$的直接前置任务的集合。对于所有其他子任务$v\neq v^*$满足$D_A(v)\subseteq D_B(v)$。</li><li>于是工作流$A$的已完成子任务的期望严格大于$B$: $E[S_A]\gt E[S_B]$。</li></ul></li></ul></li></ul><p>为了鼓励生成的AOV图的模块化，定义了两个评估指标。</p><ul><li>并行性: 用于衡量子任务并发执行的程度。<ul><li>假设$S_t$表示在第t步执行的子任务集合。$T$为总步数(DAG的最大深度)，给定AOV图$G=(V, E, A)$，整体的并行度定义为步骤中的平均子任务比例:<ul><li>$P_{avg} = \frac{1}{T}\sum_{t=1}^{T}S_t$</li></ul></li></ul></li><li>依赖复杂度: 分析子任务图中的度分布测量子任务依赖结构。对于每个子任务$v_i$，定义$deg(v_i)$为在图$G$上的直接连接数。依赖复杂度由直接连接数的标准差量化:<ul><li>$C_{dependency}=\sigma_{deg(v_i)}=\sqrt{\frac{1}{|V|}\sum_{v_i\in V}(deg(v_i)-\bar{d})^2}$</li></ul></li></ul><h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><p>首先根据给定的任务需求，制定初始工作流程，生成执行计划和智能体分配策略。在执行过程中，工作流程会不断完善和动态更新，直到任务完成。</p><p><img src="" alt="Flow流程"></p><h4 id="Generate-an-Initial-AOV-Graph"><a href="#Generate-an-Initial-AOV-Graph" class="headerlink" title="Generate an Initial AOV Graph"></a>Generate an Initial AOV Graph</h4><p>给定一个任务需求提示$P$，让LLM基于$P$和用于初始化的指定提示$P_{init}$生成一组候选AOV图$\{G_1, G_2, …, G_k\}=f(\mathcal{P}_{init}, \mathcal{P})$。每个候选AOV图$G_k=(V_k, E_k, A_k)$都使用并行性和依赖复杂度的度量公式进行评估，然后优先选择具有最高并行性分数的工作流。如果相同并行性分数，则选择具有最低依赖复杂度的工作流。</p><ul><li>早期强调并行性与模块化是因为:<ul><li>如果不在早期明确鼓励并行性和独立性，生成的工作流很可能会过于复杂，导致子任务执行效率低下。</li><li>在任务执行的早期阶段，缺乏足够的实际执行数据（如同“监督信息”）。这使得验证初始工作流的正确性非常具有挑战性。因此，与其尝试在没有数据的情况下完美规划，不如优先优化工作流的结构属性。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PROMPT_TEMPLATE = <span class="string">&quot;You are an intelligent workflow planner. Given the following task requirements, generate a set of necessary sub-tasks along with their dependencies and assign appropriate agents to each task. Ensure that tasks that can be executed in parallel are identified to enhance efficiency. The workflow should be represented as a dictionary where each key is a task and its value contains the task’s status, data, number of parents not completed, child tasks, and assigned agent.&quot;</span></span><br><span class="line">Task_Requirements = &#123;TASK_REQUIREMENTS&#125;</span><br><span class="line">Output_Format = &#123; <span class="string">&quot;Task_A&quot;</span>: &#123; <span class="string">&quot;status&quot;</span>: <span class="string">&quot;not started&quot;</span>, <span class="string">&quot;data&quot;</span>: null, <span class="string">&quot;num_parents_not_completed&quot;</span>: <span class="number">0</span>, <span class="string">&quot;child&quot;</span>: [<span class="string">&quot;Task_B&quot;</span>, <span class="string">&quot;Task_C&quot;</span>], <span class="string">&quot;agent&quot;</span>: <span class="string">&quot;Agent_1&quot;</span> &#125;, <span class="string">&quot;Task_B&quot;</span>: &#123;<span class="string">&quot;status&quot;</span>: <span class="string">&quot;not started&quot;</span>, <span class="string">&quot;data&quot;</span>: null, <span class="string">&quot;num_parents_not_completed&quot;</span>: <span class="number">1</span>, <span class="string">&quot;child&quot;</span>: [<span class="string">&quot;Task_D&quot;</span>], <span class="string">&quot;agent&quot;</span>: <span class="string">&quot;Agent_2&quot;</span> &#125;, ... &#125;</span><br></pre></td></tr></table></figure><h4 id="Execution-Plan-Generation-and-Agent-Allocation"><a href="#Execution-Plan-Generation-and-Agent-Allocation" class="headerlink" title="Execution Plan Generation and Agent Allocation"></a>Execution Plan Generation and Agent Allocation</h4><p>在获得最佳AOV图后，会对子任务的依赖关系图执行拓扑排序，生成子任务的先行顺序$o: V\rightarrow\{1, 2, …, |V|\}$，使得对于任何边$(v_i, v_j)\in E$，都有$o(v_i)\lt o(v_j)$。结果是一个子任务步骤序列，其中每个步骤都包含可以并行执行的子任务。此执行计划最大限度地减少了执行所需的步骤数，同时确保所有子任务在最短时间内完成，并遵守其依赖关系。</p><p>而每个智能体$a_j\in A$都与一组子任务$T_i\subseteq V$相关联，表明该智能体负责处理该组子任务。但如果两个子任务$v_p, v_q$在同一步骤$s_i$中需要相同的智能体$a_i$，则创建一个该智能体的克隆，表示为$a’_j$，以此来实现同时运行两个子任务而不增加等待时间。</p><h4 id="Workflow-Refinement-and-Dynamic-Updating"><a href="#Workflow-Refinement-and-Dynamic-Updating" class="headerlink" title="Workflow Refinement and Dynamic Updating"></a>Workflow Refinement and Dynamic Updating</h4><p>利用LLM作为全局检查器(global inspector)，持续监控任务进度，并在必要时动态修改AOV图。<br>具体来说，给定任务需求提示$P$，更新提示$P_{update}$，当前的AOV图$G_t$，以及包含子任务状态和运行子任务的智能体输出的生成数据$D_t$。与初始化过程类似，生成K个候选AOV图$\{G_{t+1}^1, G_{t+1}^2, …, G_{t+1}^K\}=f(P_{update}, P, D_t)$。遵循与初始化时相同的选择策略，该策略优先选择具有最高并行分数的工作流，如果相同并行性分数，则选择具有最低依赖复杂度的工作流。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PROMPT_TEMPLATE = <span class="string">&quot;You are an intelligent workflow updater. Based on the current workflow and the all subtasks’ progress data, update the workflow for acheving the objective by adding, removing, or modifying subtasks as necessary. Ensure that the updated workflow maintains modularity and maximizes parallel execution.&quot;</span></span><br><span class="line">Output_Format= &#123; <span class="string">&quot;Task_A&quot;</span>: &#123; <span class="string">&quot;status&quot;</span>: <span class="string">&quot;not started&quot;</span>, <span class="string">&quot;data&quot;</span>: null, ... &#125;&#125;</span><br></pre></td></tr></table></figure><p>由于具有模块化约束，所以动态更新可以很大程度上满足灵活性，允许修改子任务分配，包括删除、添加、编辑、重新运行和重新分配智能体，而无需影响其他智能体及被分配的子任务。</p><p>通过足够的数据和计算资源，我们可以通过使用强化学习对 LLM 进行微调以生成 workflow，从而进一步增强框架。</p><ul><li>将训练 LLM 以最大化围绕关键绩效指标(如任务完成速度、资源利用率和 workflow 中断最小化)设计的奖励函数。</li></ul><h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><p>具体实现AOV图是采用基于字典的结构，表示为$\widetilde{G}$，以便在多智能体框架内高效管理和动态更新工作流。每个子任务$v$在$\widetilde{G}$中表示为一个key，其对应的value是另一个字典(封装了子任务的各种属性)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&quot;subtask requirement&quot;: 任务需求的文本内容;</span></span><br><span class="line"><span class="string">&quot;status&quot;: 当前任务的状态 e.g. </span></span><br><span class="line"><span class="string">  &quot;not started&quot;, </span></span><br><span class="line"><span class="string">  &quot;in progress&quot;, </span></span><br><span class="line"><span class="string">  &quot;completed&quot;;</span></span><br><span class="line"><span class="string">&quot;data&quot;: 与此任务相关的数据;</span></span><br><span class="line"><span class="string">&quot;num_parents_not_completed&quot;: 未完成的父任务的数量，用于管理依赖关系。每个子任务的执行就绪状态由此决定，只有该属性计数为0的子任务才有资格并发运行;</span></span><br><span class="line"><span class="string">&quot;child&quot;: 依赖于当前任务完成的子任务列表;</span></span><br><span class="line"><span class="string">&quot;agent&quot;: 分配给该任务的智能体。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">G[v] = &#123;<span class="string">&quot;subtask requirement&quot;</span>, <span class="string">&quot;status&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;num_parents_not_completed&quot;</span>, <span class="string">&quot;child&quot;</span>, <span class="string">&quot;agent&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>在每个子任务完成后，会进行系统的审查，以确定工作流程是否需要改进，确保所有依赖项都得到准确的考虑，并且工作流程与项目目标保持一致。</p><p>Flow还会通过询问此子任务的所有要求是否已满足来仔细检查每个子任务的完成情况。这将大大防止因智能体报告不准确或不可预见的系统异常而导致的错误。这种严格的验证过程增强了工作流管理系统的可靠性和完整性。</p><h3 id="Benchmark-1"><a href="#Benchmark-1" class="headerlink" title="Benchmark"></a>Benchmark</h3><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>The existing multi-agent frameworks:</p><ul><li>AutoGen</li><li>Camel</li><li>MetaGPT</li></ul><p>Agents’ model:</p><ul><li>GPT-4o-mini</li><li>GPT-3.5-Turbo</li></ul><h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><ul><li><p>五子棋游戏开发：此任务需要创建一个具有用户界面和一个简单 AI 对手的五子棋游戏。玩家可以选择黑子或白子，用户界面清楚地指示轮次，并在游戏结束时宣布获胜者或平局。此任务展示了框架处理模块化设计和任务并行性的能力，因为它涉及同时协调游戏逻辑、AI 实现和用户界面开发。</p></li><li><p>LaTeX Beamer 写作：此任务侧重于生成 LaTeX 幻灯片，涵盖强化学习算法，包括动机、问题陈述、直观的解决方案和详细的数学公式。特定的页面要求是测试框架精确遵循指令的能力。该任务突出了框架在同时生成内容、格式和演示结构方面的并行处理能力。LaTeX 的结构化格式还测试了框架管理模块化和并发任务的效率。</p></li><li><p>网站设计：此任务涉及为 International Conference on Learning Representations 构建一个专业的网站，假设定于 2025 年 4 月 27 日至 5 月 1 日在旧金山举行。该网站必须具有关键要素，例如详细的会议日程和带有交互式地图的场地信息。此任务评估每个框架管理并行工作流程和模块化组件的能力，包括用户界面设计、功能以及对设计指南的遵守情况，展示了框架处理任务分解和执行的程度。</p></li></ul><h4 id="Metrics-1"><a href="#Metrics-1" class="headerlink" title="Metrics"></a>Metrics</h4><ul><li>Success Rate<ul><li>评估多智能体框架是否成功生成了完全满足任务需求的可执行输出。较高的分数表示在准确完成任务目标方面取得了更大的成功。不同的任务可能有不同的评估指标。</li></ul></li><li>Human Rating<ul><li>用于评估生成结果的质量与任务描述的一致性。召集了数十名具有编程和机器学习背景的参与者来对不同方法产生的结果进行排序。</li></ul></li></ul><h2 id="WORKFLOWLLM-ENHANCING-WORKFLOW-ORCHESTRATION-CAPABILITY-OF-LARGE-LANGUAGE-MODELS"><a href="#WORKFLOWLLM-ENHANCING-WORKFLOW-ORCHESTRATION-CAPABILITY-OF-LARGE-LANGUAGE-MODELS" class="headerlink" title="WORKFLOWLLM: ENHANCING WORKFLOW ORCHESTRATION CAPABILITY OF LARGE LANGUAGE MODELS"></a>WORKFLOWLLM: ENHANCING WORKFLOW ORCHESTRATION CAPABILITY OF LARGE LANGUAGE MODELS</h2><h2 id="GRAPH-ASSISTED-OFFLINE-ONLINE-DEEP-REINFORCEMENT-LEARNING-FOR-DYNAMIC-WORKFLOW-SCHEDULING"><a href="#GRAPH-ASSISTED-OFFLINE-ONLINE-DEEP-REINFORCEMENT-LEARNING-FOR-DYNAMIC-WORKFLOW-SCHEDULING" class="headerlink" title="GRAPH ASSISTED OFFLINE-ONLINE DEEP REINFORCEMENT LEARNING FOR DYNAMIC WORKFLOW SCHEDULING"></a>GRAPH ASSISTED OFFLINE-ONLINE DEEP REINFORCEMENT LEARNING FOR DYNAMIC WORKFLOW SCHEDULING</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Agentic-Workflow&quot;&gt;&lt;a href=&quot;#Agentic-Workflow&quot; class=&quot;headerlink&quot; title=&quot;Agentic Workflow&quot;&gt;&lt;/a&gt;Agentic Workflow&lt;/h1&gt;&lt;h2 id=&quot;Toward-Su</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>凸集、凸函数与凸优化问题</title>
    <link href="http://zjn-astonishe.github.io/2025/06/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/2025-06-08-%E5%87%B8%E9%9B%86%E3%80%81%E5%87%B8%E5%87%BD%E6%95%B0%E4%B8%8E%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <id>http://zjn-astonishe.github.io/2025/06/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/2025-06-08-%E5%87%B8%E9%9B%86%E3%80%81%E5%87%B8%E5%87%BD%E6%95%B0%E4%B8%8E%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/</id>
    <published>2025-06-08T03:23:03.000Z</published>
    <updated>2025-06-12T02:16:58.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="凸集、凸函数与凸优化问题"><a href="#凸集、凸函数与凸优化问题" class="headerlink" title="凸集、凸函数与凸优化问题"></a>凸集、凸函数与凸优化问题</h1><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>最优化(optimization)是一门运用数学方法研究各种问题的优化途径和方案，为决策者提供科学决策的学科。</li><li>研究范式:<ul><li>收集有关数据和资料，了解问题背景和需求。</li><li>建立恰当的最优化问题的数学模型:<ul><li>包括:<ul><li>确定的变量。</li><li>需要优化的目标函数。</li><li>解需要满足的约束条件。</li></ul></li><li>公式:<ul><li><script type="math/tex; mode=display">minimize\qquad f(x)\\ subject\quad to\quad x\in \Omega</script></li></ul></li></ul></li><li>分析和研究模型的相关理论性质，选择或设计合适的优化算法，分析算法的理论性质，考察算法的计算性能。</li></ul></li></ul><h3 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h3><ul><li>营养搭配问题<br><img src="" alt="img"></li><li>最小二乘回归问题(Least Squares Regression Problem)<br><img src="" alt="img"></li><li>鲁棒主成分分析(Robust Principal Component Analysis)<ul><li>假设一个数据矩阵$M\in R^{m\times n}$: $M = L_0 + S_0$<ul><li>可由以下凸优化问题的解恢复:<ul><li>$\min_{L, S} \parallel L\parallel _* + \lambda\parallel S\parallel_1$<ul><li>最小化核范数(nuclear norm)$\parallel L\parallel_*$用于诱导矩阵L的低秩性，是对$rank(L)$的“最佳”凸逼近。</li><li>最小化$\ell_1$范数$\parallel S\parallel_1$用于诱导矩阵S的稀疏性。$\parallel S\parallel_1$是$\parallel S\parallel_0$的“最佳”凸逼近，其中$\parallel S\parallel_0$表示矩阵S中非零元素的个数，$\parallel S\parallel_1$表示元素绝对值的和。</li><li>$\lambda$用于控制低秩和稀疏部分之间的权衡。</li></ul></li><li>$s.t.\quad L + S = M$</li></ul></li></ul></li><li>旨在通过$M$同时恢复低秩矩阵$L_0$和稀疏矩阵$S_0$。</li><li>例如给定一个监控视频帧序列，将视频帧拉伸成向量作为矩阵M的每一列。低秩矩阵刻画视频中变化平稳的背景，稀疏矩阵刻画视频中少量移动的物体。</li></ul></li></ul><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="向量和矩阵"><a href="#向量和矩阵" class="headerlink" title="向量和矩阵"></a>向量和矩阵</h3><ul><li>范数<ul><li>称一个从向量空间$\mathbb{R}^n$到实数域$\mathbb{R}$的非负函数$\parallel\cdot\parallel$为范数，需满足:<ul><li>正定性: $\parallel v\parallel\ge 0, \forall v\in\mathbb{R}^n$，且$\parallel v\parallel=0$当且仅当$v=0$。</li><li>齐次性: $\parallel\alpha v\parallel = \mid\alpha\mid\parallel v\parallel, \forall v\in\mathbb{R}^n, \alpha\in\mathbb{R}$。</li><li>三角不等式: $\parallel v+w\parallel\le \parallel v\parallel + \parallel w\parallel, \forall v, w\in \mathbb{R}^n$。</li></ul></li></ul></li><li><p>对于给定向量$x, y\in\mathbb{R}^n$，</p><ul><li>$x\le y$: 表示$x_i\le y_i, i=1, 2, …, n$</li><li>内积: $<x, y>=x^Ty=\sum_{i=1}^{n}x_iy_i$</li><li>$\ell_2$范数: $\parallel x\parallel_2 = (\sum_{i=1}^n\mid x_i\mid^2)^\frac{1}{2}$，可省略下标。</li><li>$\ell_1$范数: $\parallel x\parallel_1 = \sum_{i=1}^n\mid x_i\mid$</li><li>$\ell_\infin$范数: $\parallel x\parallel_\infin = \max_{1\le i\le n}\mid x_i\mid$</li><li>$\ell_p$范数($p\ge 1$): $\parallel x\parallel_p = (\sum_{i=1}^n\mid x_i\mid^p)^\frac{1}{p}$</li><li>$\ell_0$“范数”$\parallel x\parallel_0$: 向量x的所有分量中非零元素的个数(并非真正的范数)。</li></ul></li><li><p>对任意的$x\in \mathbb{R}^n$: $\parallel x\parallel_\infin\le\parallel x\parallel_2\le\parallel x\parallel_1\le n\parallel x\parallel_\infin$</p></li><li>柯西-施瓦兹不等式: 对任意的$x, y\in \mathbb{R}^n$，有$-\parallel x\parallel\parallel y\parallel\le<x, y>\le\parallel x\parallel\parallel y\parallel$</li><li>赫尔德不等式: $\mid x^Ty\mid \le \parallel x\parallel_p\parallel y\parallel_q = (\sum_{i=1}^{n}\mid x_i\mid^p)^\frac{1}{p}(\sum_{i=1}^{n}\mid y_i\mid^q)^\frac{1}{q}$<ul><li>其中$p, q$是大于1的实数，且满足$\frac{1}{p} + \frac{1}{q} = 1$</li></ul></li><li><p>闵可夫斯基不等式: 对任意的$x, y\in \mathbb{R}^n, p\in [1, \infin)$，有$\parallel x+y\parallel_p\le\parallel x\parallel_p + \parallel y\parallel_p =(\sum_{i=1}^{n}\mid x_i\mid^p)^\frac{1}{p}(\sum_{i=1}^{n}\mid y_i\mid^p)^\frac{1}{p}$</p></li><li><p>对任意的$A = (a_{ij})\in\mathbb{R}^{m\times n}$和$B = (b_{ij})\in\mathbb{R}^{m\times n}$</p><ul><li>内积: $<A, B>=tr(AB^T)=\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}b_{ij}$</li><li>Frobenius范数: $\parallel A\parallel_F = \sqrt{\sum_{i, j}\mid a_{ij}\mid^2} = \sqrt{tr(A^TA)}$ </li><li>诱导范数: $\parallel A\parallel\triangleq \sup_{\parallel x\parallel=1}\parallel Ax\parallel=\sup_{\parallel x\parallel\neq 0}\frac{\parallel Ax\parallel}{\parallel x\parallel}$</li><li>诱导$\ell_2$范数: $\parallel A\parallel_2 = \sqrt{\lambda_{max}(A^TA)}$</li><li>诱导$\ell_1$范数: $\parallel A\parallel_1=\max_{1\le j\le n}\sum_{i=1}^{n}\mid a_{ij}\mid$</li><li>诱导$\ell_\infin$范数:  $\parallel A\parallel_\infin=\max_{1\le i\le n}\sum_{j=1}^{n}\mid a_{ij}\mid$</li><li>核范数: $\parallel A\parallel_*\sum_{i=1}^{r}\sigma_i$，其中$\{\sigma_i\}$为A的非零奇异值，$r=rank(A)$。</li></ul></li><li>对任意的$A, B\in\mathbb{R}^{n\times n}$和$x\in\mathbb{R}^n$，有: $\parallel AB\parallel\le\parallel A\parallel\parallel B\parallel, \parallel Ax\parallel\le\parallel A\parallel\parallel x\parallel$</li><li>对于$A\in\mathbb{R}^{n\times n}$:<ul><li>如果$A^T=A$，则是对称的。</li><li>如果$A^T=A$且对于任意的$x\neq 0, x^TAx\gt 0$，则是对称正定的($A\succ 0$)。</li><li>如果$A^T=A$且对于任意的$x\in\mathbb{R}^n, x^TAx\ge 0$，则是对称半正定的($A\succeq 0$)。</li></ul></li></ul><h3 id="梯度和海瑟矩阵"><a href="#梯度和海瑟矩阵" class="headerlink" title="梯度和海瑟矩阵"></a>梯度和海瑟矩阵</h3><ul><li>梯度(Gradient)<ul><li>给定函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$，且$f$在点$x$的一个邻域内有意义，若存在向量$g\in\mathbb{R}^n$，满足$\lim_{d\rightarrow 0}\frac{f(x+d)-f(x)-g^Td}{\parallel d\parallel} = 0$，其中$\parallel\cdot\parallel$是任意的向量范数，就称$f$在点$x$处可微(弗雷歇可微)。此时，称$g$为$f$在点$x$处的梯度，记作$\nabla f(x)$。如果对区域$D$上的每一个点$x$都有$\nabla f(x)$存在，则称$f$在$D$上可微。</li><li>若$f$在点$x$处可微，在上式中令$d=\epsilon e_i$，其中$e_i$是第$i$个分量为1的单位向量，可知$\nabla f(x)$的第$i$个分量即为$\frac{\partial f(x)}{\partial x_i}$，有$\nabla f(x) = (\frac{\partial f(x)}{\partial x_1}, \frac{\partial f(x)}{\partial x_2}, \cdots, \frac{\partial f(x)}{\partial x_n})^T$</li></ul></li><li>海瑟矩阵(Hessian)<ul><li>如果函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$在点$x$处的二阶偏导数$\frac{\partial^2 f}{\partial x_i\partial x_j}(x), i, j=1, 2, \cdots, n$都存在，则$\nabla^2 f(x)=\left[<br>\begin{matrix}<br>\frac{\partial^2 f(x)}{\partial x_1^2} &amp; \frac{\partial^2 f(x)}{\partial x_1\partial x_2} &amp; \frac{\partial^2 f(x)}{\partial x_1\partial x_3} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_1\partial x_n}\\<br>\frac{\partial^2 f(x)}{\partial x_2\partial x_1} &amp; \frac{\partial^2 f(x)}{\partial x_2^2} &amp; \frac{\partial^2 f(x)}{\partial x_2\partial x_3} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_2\partial x_n} \\<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots<br>\\<br>\frac{\partial^2 f(x)}{\partial x_n\partial x_1} &amp; \frac{\partial^2 f(x)}{\partial x_n\partial x_2} &amp; \frac{\partial^2 f(x)}{\partial x_n\partial x_3} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_n^2}<br>\end{matrix}<br>\right]$称为$f$在点$x$处的海瑟矩阵。</li><li>当$\nabla^2f(x)$在区域$D$上的每个点$x$处都存在，且$\nabla^2f(x)$在区域$D$上连续，则$f$在$D$上二阶(次)连续可微，此时的海瑟矩阵是对称矩阵。</li><li>例子: <ul><li>给定对称矩阵$Q\in\mathbb{R}^{n\times n}$，向量$q\in\mathbb{R}^n$，以及数$r\in\mathbb{R}$，考虑二次函数$f(x)=\frac{1}{2}x^TQx+q^Tx+r$，则$\nabla f(x)=Qx+q, \nabla^2 f(x)=Q$。</li><li>给定对称矩阵$A\in\mathbb{R}^{m\times n}$，向量$b\in\mathbb{R}^m$，考虑函数$f(x)=\frac{1}{2}\parallel Ax-b\parallel^2$，则$\nabla f(x)=A^T(Ax-b), \nabla^2 f(x)=A^TA$。</li><li>先对转置算子求导。</li></ul></li></ul></li><li>泰勒展开定理(Taylor)<ul><li>若$f:\mathbb{R}^n\rightarrow\mathbb{R}$连续可微，有$f(x+d)=f(x)+\nabla f(x)^Td+o(\parallel d\parallel)$</li><li>且存在$t\in (0, 1)$，使得$f(x+d)=f(x)+\nabla f(x+td)^Td$</li><li>若$f$二阶次连续可微，有二阶展开式: $f(x+d)=f(x)+\nabla f(x)^Td+\frac{1}{2}d^T\nabla^2f(x)d+o(\parallel d\parallel^2)$，且存在$\gamma\in (0, 1)$，$f(x+d)=f(x)+\nabla f(x)^Td+\frac{1}{2}d^T\nabla^2f(x+\gamma d)d$</li></ul></li></ul><h3 id="最优化问题的基本概念"><a href="#最优化问题的基本概念" class="headerlink" title="最优化问题的基本概念"></a>最优化问题的基本概念</h3><h4 id="基本数学模型"><a href="#基本数学模型" class="headerlink" title="基本数学模型"></a>基本数学模型</h4><script type="math/tex; mode=display">minimize\qquad f(x) \\ subject\quad to\quad x\in\Omega</script><ul><li>决策变量: $x=(x_1, …, x_n)^T$</li><li>目标函数: $f:\Omega\rightarrow\mathbb{R}$</li><li>可行域(决策集): $\Omega\subset\mathbb{R}^n$，$\Omega$中的所有点或者说是满足约束条件的点为可行点。<ul><li>若$\Omega=\mathbb{R}^n$，则为无约束优化问题。否则为约束优化问题。<ul><li>$\Omega:=\{x\in\mathbb{R}^n:h(x)=0, c(x)\le 0\}$<ul><li>$h(x)=(h_1(x), …, h_m(x))^T$，$h_i(x)=0, i=1, 2, …, m$为等式约束。</li><li>$c(x)=(c_1(x), …, c_S(x))^T$，$c_j(x)\le 0, j=1, 2, …, s$为不等式约束。</li></ul></li></ul></li><li>对$x^<em>\in\Omega$，若对任意的$x\in\Omega$，都有$f(x^</em>)\le f(x)$，则$x^<em>$是问题的全局最优解，对应的目标函数值为全局最优值。$$x^</em>\in \arg\min_{x\in\Omega}f(x)$$<ul><li>若存在$x^<em>$点的邻域$\mathcal{B}(x^</em>, \delta)=\{x:\parallel x-x^<em>\parallel\le\delta\}$使得对任意的$x\in\Omega\bigcap\mathcal{B}(x^</em>, \delta)$，有$f(x^<em>)\le f(x)$，则$x^</em>$为局部最优解。</li><li>若还满足对任意的$x\in\Omega\bigcap\mathcal{B}(x^<em>, \delta), x\neq\{x^</em>\}$，有$f(x^<em>)\lt f(x)$，则$x^</em>$为严格局部最优解。</li></ul></li></ul></li><li>求解方法<ul><li>解析方法。</li><li>图解法。</li><li>遍历法。</li><li>迭代算法:<ul><li>取初始点$x_0\in\mathbb{R}^n$以及其他相关参数，令$k=0$。</li><li>验证停机准则。</li><li>求$x^k$点处的搜索方向$d^k\in\mathbb{R}^n$，要求$d^k$是$f_0$在点$x^k$处的下降方向(使得$f_0$从点$x^k$处沿着该方向在一定范围内函数值可以下降)，满足$(\nabla f_0(x^k))^Td^k\lt 0$</li><li>计算迭代步长$\alpha_k\gt 0$，通常要求$f_0(x^k+\alpha_kd^k)\lt f_0(x^k)$</li><li>产生下一个迭代点: 令$x^{k+1}:=x^k+\alpha_kd^k, k:=k+1$，转到验证停机准则。</li></ul></li></ul></li></ul><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><ul><li>约束优化和无约束优化。</li><li>连续优化和非连续优化。<ul><li>线性规划: 若$f, h, c$都是线性的。</li><li>非线性规划: 若$f, h, c$至少有一个是非线性的。</li><li>二次规划: 若$f$是二次函数，$h, c$是线性函数。</li></ul></li><li>凸优化和非凸优化。<ul><li>凸优化: 目标函数是凸函数，可行域为凸集。</li><li>非凸优化: 目标函数或可行域至少有一个是非凸的。</li></ul></li><li>光滑优化和非光滑优化。</li><li>确定优化和不确定优化。</li><li>鲁棒优化和随机优化。</li></ul><h4 id="算法评价标准"><a href="#算法评价标准" class="headerlink" title="算法评价标准"></a>算法评价标准</h4><ul><li>若$\lim_{k\rightarrow\infin}\parallel x^k-x^<em>\parallel=0$，则称$\{x^k\}$收敛于$x^</em>$。</li><li>聚点: 若存在$\{x^k\}$的一个子序列收敛于$x^<em>$，则称$x^</em>$是$\{x^k\}$的一个聚点。</li><li>全局收敛性: 从任意的初始点出发，算法产生的迭代序列$\{x^k\}$都收敛于问题的一个最优点或稳定点。</li><li>局部收敛性: 算法只有在初始点与最优点比较靠近时才能保证迭代序列$\{x^k\}$收敛于问题的一个最优点或稳定点。</li><li>收敛速度: 假设算法产生的迭代序列$\{x^k\}$收敛于$x^*$<ul><li>若存在$r\in(0, 1)$使得，对充分大的$k$有: $\frac{\parallel x^{k+1}-x^<em>\parallel}{\parallel x^{k}-x^</em>\parallel}\le r$，则$\{x^k\}$是Q(quotient)-线性收敛于$x^*$</li><li>若$\lim_{k\rightarrow\infin}\frac{\parallel x^{k+1}-x^<em>\parallel}{\parallel x^{k}-x^</em>\parallel}=0$，则$\{x^k\}$是Q(quotient)-超线性收敛于$x^*$</li><li>若存在$p\gt 1$和一个正数$\gamma$使得，对充分大的$k$有: $\frac{\parallel x^{k+1}-x^<em>\parallel}{\parallel x^{k}-x^</em>\parallel^p}\le \gamma$，则$\{x^k\}$是Q-p阶收敛于$x^<em>$，或者称为p-阶收敛于$\{x^</em>\}$，最常见是Q-2阶收敛。</li></ul></li><li><p>弱化收敛速度标准</p><ul><li>若存在一个Q-线性收敛于0的非负序列$\{v_k\}$，使得$\parallel x^k-x^<em>\parallel\le v_k, \forall k$，则$\{x^k\}$是R(root)-线性收敛于$x^</em>$。</li><li>若存在一个Q-超线性收敛于0的非负序列$\{v_k\}$，使得$\parallel x^k-x^<em>\parallel\le v_k, \forall k$，则$\{x^k\}$是R(root)-超线性收敛于$x^</em>$。</li></ul></li><li><p>其他算法评价标准:</p><ul><li>二次终止性。</li><li>稳定性。</li><li>计算复杂性。</li><li>调参复杂度。</li><li>存储消耗。</li><li>数值效果。</li></ul></li></ul><h2 id="三种组合"><a href="#三种组合" class="headerlink" title="三种组合"></a>三种组合</h2><ul><li>给定$x_1, x_2, …, x_m\in\mathbb{R}^n$，同样的加权组合，不同的权重要求。</li></ul><h3 id="凸集-convex-set"><a href="#凸集-convex-set" class="headerlink" title="凸集(convex set)"></a>凸集(convex set)</h3><ul><li>如果过集合C中</li></ul><h4 id="凸组合"><a href="#凸组合" class="headerlink" title="凸组合"></a>凸组合</h4><h4 id="凸包"><a href="#凸包" class="headerlink" title="凸包"></a>凸包</h4><h3 id="仿射集"><a href="#仿射集" class="headerlink" title="仿射集"></a>仿射集</h3><h4 id="仿射组合"><a href="#仿射组合" class="headerlink" title="仿射组合"></a>仿射组合</h4><h4 id="仿射包"><a href="#仿射包" class="headerlink" title="仿射包"></a>仿射包</h4><h3 id="凸锥"><a href="#凸锥" class="headerlink" title="凸锥"></a>凸锥</h3><h4 id="凸锥组合"><a href="#凸锥组合" class="headerlink" title="凸锥组合"></a>凸锥组合</h4><h4 id="凸锥包"><a href="#凸锥包" class="headerlink" title="凸锥包"></a>凸锥包</h4><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><h2 id="凸优化问题"><a href="#凸优化问题" class="headerlink" title="凸优化问题"></a>凸优化问题</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;凸集、凸函数与凸优化问题&quot;&gt;&lt;a href=&quot;#凸集、凸函数与凸优化问题&quot; class=&quot;headerlink&quot; title=&quot;凸集、凸函数与凸优化问题&quot;&gt;&lt;/a&gt;凸集、凸函数与凸优化问题&lt;/h1&gt;&lt;h2 id=&quot;绪论&quot;&gt;&lt;a href=&quot;#绪论&quot; class=&quot;</summary>
      
    
    
    
    <category term="最优化" scheme="http://zjn-astonishe.github.io/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="最优化" scheme="http://zjn-astonishe.github.io/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Hammer笔记</title>
    <link href="http://zjn-astonishe.github.io/2025/04/29/%E6%99%BA%E8%83%BD%E4%BD%93/2025-04-29-Hammer%E7%AC%94%E8%AE%B0/"/>
    <id>http://zjn-astonishe.github.io/2025/04/29/%E6%99%BA%E8%83%BD%E4%BD%93/2025-04-29-Hammer%E7%AC%94%E8%AE%B0/</id>
    <published>2025-04-29T11:33:05.000Z</published>
    <updated>2025-06-09T06:23:24.116Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hammer-Robust-Function-Calling-for-On-Device-Language-Models-via-Function-Masking"><a href="#Hammer-Robust-Function-Calling-for-On-Device-Language-Models-via-Function-Masking" class="headerlink" title="Hammer: Robust Function-Calling for On-Device Language Models via Function Masking"></a>Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</h1><h2 id="主要问题"><a href="#主要问题" class="headerlink" title="主要问题"></a>主要问题</h2><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/Demostraction%20of%20a%20simple%20function-calling%20process.png" alt="img"></p><p>在函数调用(Function calling)的过程中，每个候选函数包含以下组件:</p><ul><li>函数名</li><li>参数名</li><li>默认值</li><li>描述</li></ul><p>LLM的目标是输出完整而准确的函数调用代码，以实现用户的意图，或者输出一个空列表，表明给定的候选项都不能满足用户的需求。这取决于LLM是否能够准确地讲用户的意图与候选函数的功能结合起来(选择合适的函数)，以及能否理解函数中每个参数的用法(使用正确的参数填充函数)。</p><h3 id="问题-Misleadingness-by-Function-Name-and-Parameter-Name"><a href="#问题-Misleadingness-by-Function-Name-and-Parameter-Name" class="headerlink" title="问题: Misleadingness by Function Name and Parameter Name"></a>问题: Misleadingness by Function Name and Parameter Name</h3><p>函数和参数名称的格式通常非常紧凑，并受设计师个人风格和偏好的影响。当LLM试图仅从函数名称推断函数的目的时，这种紧凑性可能导致歧义，误导LLM的选择，特别是在存在复杂功能的情况下。</p><ul><li>名为<code>parse_data</code>的函数可能用于解析 JSON 数据，但相同的名称可能指的是在不同上下文中解析 CSV 文件，从而导致潜在的误解。</li></ul><p>可以分类为: </p><ul><li>Misled by Function Names：当用户意图与训练标签中出现的function name紧密一致时，LLM可能会在测试期间错误地优先考虑候选列表中的function，即使其功能与预期操作显著偏离。<ul><li>例如，如果一个名为fetch_data的函数包含在用于从数据库检索用户数据的训练对中，但在测试集中，具有相同名称的函数从外部API检索数据，则LLM可能仅根据名称错误地选择它。</li></ul></li><li>Misled by Parameter Names：在测试环境中参数的功能和描述发生变化的情况下，LLM经常会坚持其原始的参数使用模式，从而导致不正确的函数调用。<ul><li>例如，如果一个函数的参数timeout在一个上下文中被期望是一个表示秒的整数，但在另一个上下文中，它被定义为“10s”格式的字符串，则LLM对原始整数格式的依赖可能会导致错误调用。</li></ul></li><li>Disturbed by Naming Preferences：当测试环境中的函数或参数的命名约定与训练数据集中的函数或参数的命名约定不一致时，LLM的鲁棒性会降低。<ul><li>CamelCase和snake_case之间的差异可能会降低LLM的可信度，因为设备上的轻量级LLM可能难以概括不同的命名风格。</li></ul></li></ul><h3 id="The-Impact-of-Excessive-Focus-on-the-Naming"><a href="#The-Impact-of-Excessive-Focus-on-the-Naming" class="headerlink" title="The Impact of Excessive Focus on the Naming"></a>The Impact of Excessive Focus on the Naming</h3><p>为了调查现有LLM对函数和参数名称的依赖程度以及相应的影响，我们在Seal-Tools基准测试中使用xLAM-1B-fc模型进行了一个案例研究。</p><p>掩盖测试集中的函数和参数名称，即用随机字符串替换它们，并观察LLM的性能如何变化。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/The%20Impact%20of%20Excessive%20Focus%20on%20the%20Naming.png" alt="img"></p><p>在屏蔽函数和参数名称之后，即使描述包含了关于函数的目的和用法的所有必要信息，xLAM-1B-fc的性能仍会显著下降。这个结果证实了LLM对函数和参数名称的过度依赖，强调了这种行为在实际应用程序中可能造成的潜在风险。</p><p>在相同设置下，Hammer模型表现出的性能下降要小得多，这表明它在面对任意函数和参数命名模式时具有鲁棒性。这种弹性表明Hammer更多地依赖于函数描述，而不是紧凑的、可能有歧义的名称。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>尽量减少函数名和参数名的干扰，同时强制LLM根据候选函数的描述来理解它们的功能和用法。</p><ul><li>描述提供了更灵活的自然语言解释，通常封装了函数和参数名要传达的信息。</li><li>描述更准确和详细，从而减少模棱两可或误导的可能性。</li></ul><p>期望经过训练的LLM通过描述来理解函数的目的和用法，而不是试图根据函数和参数名称等可能模棱两可的紧凑组件来推断功能。</p><h3 id="方法-Function-Masking"><a href="#方法-Function-Masking" class="headerlink" title="方法: Function Masking"></a>方法: Function Masking</h3><p>Function Masking是基于屏蔽机制的FC模型调优框架，该框架旨在将LLM的注意力引向描述，从而在实践中增强LLM的泛化能力。</p><ul><li>候选函数中的函数名通过在训练期间用随机生成的字符串替换它们而被屏蔽。</li><li>候选参数名也用随机字符串替换。</li><li>候选参数中的默认参数值是随机的，并附加到参数描述中。</li><li>批量标签根据被屏蔽的候选列表更新，即用候选列表中对应的掩码字符串替换函数名和参数名。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/Hammer%20workflow.png" alt="img"></p><h3 id="方法-Irrelevance-Augmented"><a href="#方法-Irrelevance-Augmented" class="headerlink" title="方法: Irrelevance-Augmented"></a>方法: Irrelevance-Augmented</h3><p>该方法是为了避免即使在没有有效选项的情况下，LLM也可能生成不适当的函数调用。</p><p>因为微调过程中发现LLM准确执行函数调用的能力与其不相关检测能力之间的反比关系</p><ul><li>特别是在评估候选集中是否不存在与用户意图一致的函数调用的能力时。</li><li>即虽然Function Masking提高了LLM准确执行函数调用的能力，但也可能无意中损害LLM检测不相关性的能力</li></ul><p>提出了一个irrelevance-augmented dataset。</p><ul><li>正确的函数被故意从候选列表中排除，并且标签被空列表替换。</li><li>通过将模型暴露给更多需要不相关检测的实例，目标是增强其辨别何时放弃进行函数调用的能力，从而促进更明智的函数选择。</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h3><p>使用各种FC基准进行了评估:</p><ul><li>伯克利函数调用排行榜(BFCL): <ul><li>提供了一个包含1700多个实例的综合数据集。</li><li>涵盖了Python的简单函数、多个函数、并行函数和并行多个函数等任务，以及非Python环境的函数相关性检测、REST API、JavaScript和Java。</li></ul></li><li>API-bank: <ul><li>314个工具使用对话和753个API调用组成。</li><li>评估LLM基于Query正确调用已知API(L-1)的能力，以及从候选列表(L-2)中检索和调用API的能力。</li></ul></li><li>Nexus Raven API Evaluation: <ul><li>跨越65个不同API的318个测试示例。</li></ul></li><li>Tool-Alpaca: <ul><li>采用合成数据生成方法，在50个类别中具有271个工具使用实例。</li></ul></li><li>Seal-Tools: <ul><li>最广泛和最新的基准测试之一，具有较低的数据泄露风险。</li><li>各种生命领域中自动生成了4,076个api。</li></ul></li></ul><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>BFCL通过两种主要的评估方法来评估函数调用模型: </p><ul><li>抽象语法树(AST)评估<ul><li>强调生成的函数调用的语法精度，确保模型的输出在结构和参数方面符合预定义的函数文档。包括验证函数名、所需参数和适当数据类型的正确性。</li></ul></li><li>可执行函数评估<ul><li>执行生成的函数调用来评估它们的功能准确性，确保函数不仅可以编译，而且可以正确运行，产生预期的输出，这对于实际应用程序至关重要。</li></ul></li></ul><p>此外还结合了F1分数来衡量API名称和参数的精确匹配，以便在替代基准上评估模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hammer-Robust-Function-Calling-for-On-Device-Language-Models-via-Function-Masking&quot;&gt;&lt;a href=&quot;#Hammer-Robust-Function-Calling-for-On-D</summary>
      
    
    
    
    <category term="智能体" scheme="http://zjn-astonishe.github.io/categories/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
    
    <category term="智能体" scheme="http://zjn-astonishe.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>Function Calling, MCP and A2A</title>
    <link href="http://zjn-astonishe.github.io/2025/04/14/MCP/2025-04-14-Function%20Calling,%20MCP%20and%20A2A/"/>
    <id>http://zjn-astonishe.github.io/2025/04/14/MCP/2025-04-14-Function%20Calling,%20MCP%20and%20A2A/</id>
    <published>2025-04-14T12:54:40.000Z</published>
    <updated>2025-04-16T01:52:03.309Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Function-Calling-MCP-and-A2A"><a href="#Function-Calling-MCP-and-A2A" class="headerlink" title="Function Calling, MCP and A2A"></a>Function Calling, MCP and A2A</h1><h2 id="Function-Calling"><a href="#Function-Calling" class="headerlink" title="Function Calling"></a>Function Calling</h2><p>Function Calling允许大语言模型与外部工具连接，将自然语言转换为API调用。这解决了大模型在训练结束，就知识更新停滞的问题。调用外部工具和服务能准确解决实时性问题。</p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><p><img src="" alt="Function Calling 工作流程"></p><p>Function Calling的工作流程大致如下: </p><ol><li>识别需求<ul><li>识别问题类型，查找对应需要调用的API。</li></ul></li><li>选择函数<ul><li>选择可用的函数。</li></ul></li><li>准备参数<ul><li>例如要获得温度: {“location”: “Beijing”, “unit”: “celsius”} </li></ul></li><li>调用函数<ul><li>使用参数调用API。</li></ul></li><li>整合回答</li></ol><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>对开发者来说，使用LLM的Function Calling起步相对容易，只需按照API要求定义函数规格(通常JSON模式)并将其随请求发送，模型就可能按照需要调用这些函数，逻辑较直观。适用于单一模型、少量功能的简单应用。</p><p>但局限在于缺乏跨模型的一致性: 每个LLM供应商的接口格式略有差异，开发者如果想支持多个模型，需要为不同的API进行适配或使用额外的框架进行处理。</p><h2 id="MCP"><a href="#MCP" class="headerlink" title="MCP"></a>MCP</h2><p><a href="https://zjn-astonishe.github.io/2025/04/08/MCP/2025-04-08-MCP/">MCP 笔记</a></p><h2 id="Agent-to-Agent-A2A"><a href="#Agent-to-Agent-A2A" class="headerlink" title="Agent-to-Agent(A2A)"></a>Agent-to-Agent(A2A)</h2><p>A2A谷歌最新推出的开放协议，旨在实现不同Agent之间的通信和协同问题。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>Agent Card: 一个公共元数据文件，描述智能体的能力、技能、端点URL和认证需求等信息，类似AI智能体的”电子名片”。</li><li>A2A Server: 实现A2A协议的智能体服务器，接收请求并管理Task的执行。</li><li>A2A Client: 使用A2A服务的应用程序或智能体，向A2A Server发送请求。</li><li>Task: A2A协议的工作中心单元，客户端通过发送消息启动Task，具有不同状态以跟踪进度。</li><li>Message: 智能体客户端和智能体之间的通信载体，包含多种形式，多个部分。</li></ul><h3 id="工作流程-1"><a href="#工作流程-1" class="headerlink" title="工作流程"></a>工作流程</h3><p><img src="" alt="A2A协议工作流程"></p><p>首先，A2A Client（就像点餐的顾客）向A2A Server发送请求，启动一个任务。接着，服务器处理请求并返回响应，告知任务的状态。任务在执行过程中可能会经历多个状态，如已提交、处理中、需要输入等，最终完成或失败。</p><h2 id="三者的联系"><a href="#三者的联系" class="headerlink" title="三者的联系"></a>三者的联系</h2><h3 id="Function-Calling的局限性"><a href="#Function-Calling的局限性" class="headerlink" title="Function Calling的局限性"></a>Function Calling的局限性</h3><ul><li>Function Calling由于缺乏统一标准，不同LLM需要各自的函数定义格式：如果有M个不同LLM应用和N个不同工具/服务，理论上可能需要实现$M\times N$次重复的对接工作。</li><li>对于函数的链式调用，Function Calling本身并不直接支持多步调用组合，模型只能一次调用一个函数，获取结果后如果需调用下一个函数，需要由应用逻辑将结果反馈为模型下一轮对话的输入，再触发下一个函数调用。虽然在原理上可以实现函数输出作为输入形成链条，但这一切需要开发者在应用层精心编排，模型自身缺乏对跨调用流程的全局观。</li><li>MCP则通过统一的接口标准，将复杂的$M\times N$问题转化为$M+N$的问题：工具创建者只需为每个工具/系统实现一次MCP Server，应用开发者只需为每个应用实现一次MCP Client，各自遵循通用协议即可协同工作，扩展新功能的边际成本大幅降低。</li></ul><h3 id="MCP与A2A的互补关系"><a href="#MCP与A2A的互补关系" class="headerlink" title="MCP与A2A的互补关系"></a>MCP与A2A的互补关系</h3><ul><li>MCP让Agent能够使用工具，而A2A让Agent能够与其他Agent协作。一个解决”做什么”，一个解决”与谁合作”。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Function-Calling-MCP-and-A2A&quot;&gt;&lt;a href=&quot;#Function-Calling-MCP-and-A2A&quot; class=&quot;headerlink&quot; title=&quot;Function Calling, MCP and A2A&quot;&gt;&lt;/a&gt;F</summary>
      
    
    
    
    <category term="MCP" scheme="http://zjn-astonishe.github.io/categories/MCP/"/>
    
    
    <category term="MCP" scheme="http://zjn-astonishe.github.io/tags/MCP/"/>
    
    <category term="Agent" scheme="http://zjn-astonishe.github.io/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>OS Agents 笔记</title>
    <link href="http://zjn-astonishe.github.io/2025/04/14/%E6%99%BA%E8%83%BD%E4%BD%93/2025-04-14-OS%20Agents%20%E7%AC%94%E8%AE%B0/"/>
    <id>http://zjn-astonishe.github.io/2025/04/14/%E6%99%BA%E8%83%BD%E4%BD%93/2025-04-14-OS%20Agents%20%E7%AC%94%E8%AE%B0/</id>
    <published>2025-04-14T03:05:57.000Z</published>
    <updated>2025-04-14T04:16:24.867Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OS-Agent-A-Survey-on-MLLM-based-Agents-for-Computer-Phone-and-Browser-Use"><a href="#OS-Agent-A-Survey-on-MLLM-based-Agents-for-Computer-Phone-and-Browser-Use" class="headerlink" title="OS Agent: A Survey on MLLM-based Agents for Computer, Phone and Browser Use"></a>OS Agent: A Survey on MLLM-based Agents for Computer, Phone and Browser Use</h1><h2 id="Agents基础"><a href="#Agents基础" class="headerlink" title="Agents基础"></a>Agents基础</h2><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/OS%20Agents%20%E5%9F%BA%E7%A1%80_%E5%85%B3%E9%94%AE%E8%A6%81%E7%B4%A0%E5%92%8C%E6%A0%B8%E5%BF%83%E8%83%BD%E5%8A%9B.png" alt="关键要素和核心能力"></p><h3 id="关键要素"><a href="#关键要素" class="headerlink" title="关键要素"></a>关键要素</h3><ul><li>环境(Environment): 智能体操作的系统或平台。环境是智能体完成任务的舞台，支持从简单的信息检索到复杂的多步骤操作。</li><li>观察空间(Observation Space): 智能体可获取的所有信息范围。如屏幕截图、文本描述(HTML代码)或GUI界面结构，是智能体理解环境和任务的基础。</li><li>动作空间(Action Space): 智能体与环境交互的动作集合。定义了可执行操作(点击、输入文本、导航操作甚至调用外部工具)。使得智能体能够自动化完成任务并优化工作流</li></ul><h3 id="核心能力"><a href="#核心能力" class="headerlink" title="核心能力"></a>核心能力</h3><ul><li>理解(Understanding): 理解复杂的操作环境，提取关键信息，构建对任务和环境的全面认知。理解能力是处理信息检索等任务的前提。</li><li>规划(Planning): 要求将复杂的任务拆解为多个子任务，并依此制定操作序列实现目标。最好还要能够根据环境变化动态调整计划，适应复杂的操作系统环境(动态网页和实时更新的用户屏幕界面)。</li><li>操作(Grounding): 将规划转化为具体可执行的操作(点击按钮、输入文本、调用API)。</li></ul><h2 id="构建Agents"><a href="#构建Agents" class="headerlink" title="构建Agents"></a>构建Agents</h2><h3 id="基础模型-Foundation-Model"><a href="#基础模型-Foundation-Model" class="headerlink" title="基础模型(Foundation Model)"></a>基础模型(Foundation Model)</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/OS%20Agents%20%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84_%E9%A2%84%E8%AE%AD%E7%BB%83_%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E5%92%8C%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.png" alt="架构_预训练_监督微调和强化学习"></p><h4 id="架构-Architecture"><a href="#架构-Architecture" class="headerlink" title="架构(Architecture)"></a>架构(Architecture)</h4><ol><li>Existing LLMs: 直接采用开源的大语言模型架构，将结构化的屏幕界面信息以文本形式输入给LLMs，从而使得模型可以感知环境。(如何获得文本形式的屏幕界面信息？)</li><li>Existing MLLMs: 直接采用开源的多模态大语言模型架构块，整合文本和视觉处理能力，提升对GUI的理解能力，减少文本化视觉信息而造成的特征损失(优化目标)。</li><li>Concatenated MLLMs: 由LLM与视觉编码器桥接而成，灵活性更高，可以根据任务需求选择不同的语言模型和视觉模型进行组合。</li><li>Modified MLLMs: 对现有MLLM架构进行优化调整，解决特定场景的挑战<ul><li>添加额外模块，如高分辨率视觉编码器或图像分割模块，更细致地感知和理解屏幕界面细节。</li></ul></li></ol><h4 id="预训练-Pre-training"><a href="#预训练-Pre-training" class="headerlink" title="预训练(Pre-training)"></a>预训练(Pre-training)</h4><ul><li>为模型构建打下基础，通过海量数据提升对屏幕界面的理解能力。<ul><li>屏幕定位(Screen Grounding)</li><li>屏幕理解(Screen Understanding)</li><li>光学字符识别(OCR)</li></ul></li><li>数据源<ul><li>公共数据集</li><li>合成数据集</li></ul></li></ul><h4 id="监督微调-Supervised-Fine-tuning"><a href="#监督微调-Supervised-Fine-tuning" class="headerlink" title="监督微调(Supervised Fine-tuning)"></a>监督微调(Supervised Fine-tuning)</h4><ul><li>让模型更贴合GUI场景，提升智能体规划能力和执行能力。<ul><li>记录任务执行轨迹生成训练数据。</li><li>利用HTML渲染屏幕界面细节，提升模型对不同GUI的泛化能力。</li></ul></li></ul><h4 id="强化学习-Reinforcement-Learning"><a href="#强化学习-Reinforcement-Learning" class="headerlink" title="强化学习(Reinforcement Learning)"></a>强化学习(Reinforcement Learning)</h4><ul><li>用(M)LLMs作为特征提取到(M)LLM-as-Agent的范式转变，帮助智能体在动态环境中交互，根据奖励反馈，不断优化决策。提升了智能体的对齐程度，泛化能力和任务适配性。</li></ul><h3 id="智能体框架-Agent-Framework"><a href="#智能体框架-Agent-Framework" class="headerlink" title="智能体框架(Agent Framework)"></a>智能体框架(Agent Framework)</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/OS%20Agents%20%E6%A1%86%E6%9E%B6_%E6%84%9F%E7%9F%A5_%E8%A7%84%E5%88%92_%E8%AE%B0%E5%BF%86%E5%92%8C%E8%A1%8C%E5%8A%A8.png" alt="框架_感知_规划_记忆和行动"></p><h4 id="感知-Perception"><a href="#感知-Perception" class="headerlink" title="感知(Perception)"></a>感知(Perception)</h4><ul><li>文本感知: 将操作系统的状态转化为结构化文本描述。<ul><li>DOM树</li><li>HTML文件</li></ul></li><li>屏幕界面感知<ul><li>使用视觉编码器对屏幕界面截图进行理解。</li><li>通过视觉定位(按钮、菜单)和语义连接(HTML标记)精准识别关键元素。</li></ul></li></ul><h4 id="规划-Planning"><a href="#规划-Planning" class="headerlink" title="规划(Planning)"></a>规划(Planning)</h4><ul><li>作为智能体的”大脑”，负责制定任务的执行策略。<ul><li>全局规划: 一次生成完整的计划并执行。</li><li>迭代规划: 随着环境的变化动态调整计划，适应实时更新的屏幕界面和任务需求。</li></ul></li></ul><h4 id="记忆-Memory"><a href="#记忆-Memory" class="headerlink" title="记忆(Memory)"></a>记忆(Memory)</h4><ul><li>内部记忆(Internal Memory): 存储操作历史、屏幕截图、状态数据和动态环境信息，支持任务执行的上下文理解和轨迹优化。<ul><li>借助截图解析屏幕界面布局。</li><li>根据历史操作生成决策。</li></ul></li><li>外部记忆(External Memory): 提供长期知识支持。<ul><li>调用外部工具(API)</li><li>从知识库获取领域背景知识，辅助复杂任务的决策。</li></ul></li><li>特定记忆(Specific Memory): 聚焦于特定任务的知识和用户需求。<ul><li>存储子任务分解方法、用户偏好或屏幕界面交互功能。</li><li>提供高度针对性的操作支持。</li><li>记忆优化策略。</li></ul></li></ul><h4 id="行动-Action"><a href="#行动-Action" class="headerlink" title="行动(Action)"></a>行动(Action)</h4><ul><li>输入操作: 与数字屏幕界面交互的基础。<ul><li>鼠标操作</li><li>触控操作</li><li>键盘操作</li></ul></li><li>导航操作: 探索和移动于目标平台，获取执行任务所需的信息。</li><li>扩展操作: 不止与屏幕界面交互。<ul><li>代码执行</li><li>API调用</li></ul></li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="评估协议-Evaluation-Protocol"><a href="#评估协议-Evaluation-Protocol" class="headerlink" title="评估协议(Evaluation Protocol)"></a>评估协议(Evaluation Protocol)</h3><ul><li>评估的核心为: <ul><li>评估过程应如何进行。</li><li>需要对哪些方面进行评估。</li></ul></li></ul><h4 id="评估原则-Evaluation-Principle"><a href="#评估原则-Evaluation-Principle" class="headerlink" title="评估原则(Evaluation Principle)"></a>评估原则(Evaluation Principle)</h4><ul><li>客观评估(Objective): 通过标准化的数值指标，评估智能体在特定任务中的性能。<ul><li>操作的准确性</li><li>任务的成功率</li><li>语义匹配的精准度</li></ul></li><li>主观评估(Subjective): 基于人类用户的主观感受，评估智能体的输出质量。<ul><li>相关性</li><li>自然性</li><li>连贯性</li><li>整体效果</li></ul></li><li>借助(M)LLM-as-Judge评估: 提高效率和一致性。</li></ul><h4 id="评估指标-Evaluation-Metric"><a href="#评估指标-Evaluation-Metric" class="headerlink" title="评估指标(Evaluation Metric)"></a>评估指标(Evaluation Metric)</h4><p>评估指标聚焦于智能体的理解、规划和操作能力，衡量其在不同任务中的表现。</p><ul><li>步骤级指标: 评估智能体在每一步操作中的准确性。<ul><li>任务执行中动作的语义匹配程度</li><li>任务执行中操作的准确性</li></ul></li><li>任务级指标: 聚焦于整个任务完成情况。<ul><li>任务成功率</li><li>完成任务的效率</li></ul></li></ul><h3 id="评估基准-Evaluation-Benchmark"><a href="#评估基准-Evaluation-Benchmark" class="headerlink" title="评估基准(Evaluation Benchmark)"></a>评估基准(Evaluation Benchmark)</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E6%99%BA%E8%83%BD%E4%BD%93/OS%20Agents%20%E5%B9%B3%E5%8F%B0-%E5%9F%BA%E5%87%86%E4%B8%8E%E4%BB%BB%E5%8A%A1%E5%88%86%E7%B1%BB.png" alt="平台-基准与任务分类"></p><h4 id="评估平台-Evaluation-Platform"><a href="#评估平台-Evaluation-Platform" class="headerlink" title="评估平台(Evaluation Platform)"></a>评估平台(Evaluation Platform)</h4><ul><li>移动平台(Mobile)</li><li>桌面平台(Desktop)</li><li>网页平台(Web)</li></ul><h4 id="基准设置-Benchmark-Setting"><a href="#基准设置-Benchmark-Setting" class="headerlink" title="基准设置(Benchmark Setting)"></a>基准设置(Benchmark Setting)</h4><ul><li>静态环境(Static): 适用与基础任务的离线评估。</li><li>交互式环境(Interactive): 全面测试智能体在复杂动态场景中的实际能力。<ul><li>模拟环境(Simulated): 多用于功能调试验证。</li><li>真实环境(Real-World): 强调泛化能力和动态适应性，未来评估的重要方向。</li></ul></li></ul><h4 id="任务-Task"><a href="#任务-Task" class="headerlink" title="任务(Task)"></a>任务(Task)</h4><p>当前的基准测试整合了各种专业化任务，涵盖从系统级任务(安装和卸载应用程序)到日常应用任务(发送邮件和在线购物)。</p><ul><li>GUI定位(GUI Grounding): 评估智能体将指令转换为屏幕界面操作的能力。<ul><li>如何在操作系统中与指定的可操作元素交互。</li></ul></li><li>信息处理(Information Processing):评估智能体高效处理和总结信息的能力。<ul><li>尤其在动态和复杂环境中，从大量数据提取有用信息的能力。</li></ul></li><li>智能体任务(Agentic Tasks): 评估智能体的核心能力，规划和执行复杂任务。<ul><li>为智能体提供目标或指令，要求其在没有显式指导的情况下完成任务。</li></ul></li></ul><h4 id="安全-Security"><a href="#安全-Security" class="headerlink" title="安全(Security)"></a>安全(Security)</h4><ul><li>为评估 OS Agents 在不同场景下的鲁棒性，还引入了一些智能体安全基准测试，用于全面测试和改进系统的安全表现。<ul><li>ST-WebAgentBench</li><li>MobileSafetyBench</li></ul></li></ul><h2 id="挑战与未来"><a href="#挑战与未来" class="headerlink" title="挑战与未来"></a>挑战与未来</h2><h3 id="安全与隐私"><a href="#安全与隐私" class="headerlink" title="安全与隐私"></a>安全与隐私</h3><ul><li>面临多种攻击方式，包括间接提示注入攻击、恶意弹出窗口和对抗性指令生成，这些威胁可能导致系统执行错误操作或泄露敏感信息。</li><li>当前研究主要集中于设计专门应对注入攻击和后门攻击等特殊威胁的防御方案，急待开发全面的且可扩展防御框架，以提升 OS Agents 的整体安全性和可靠性。</li></ul><h3 id="个性化和自我进化"><a href="#个性化和自我进化" class="headerlink" title="个性化和自我进化"></a>个性化和自我进化</h3><ul><li>需要根据用户偏好不断调整行为和功能。<ul><li>多模态大语言模型正逐步支持理解用户历史记录和动态适应用户需求，OpenAI的Memory功能在这一方向上已经取得了一定进展。</li><li>未来将记忆机制扩展到更复杂的形式，如音频、视频、传感器数据等，从而提供更高级的预测能力和决策支持。</li></ul></li><li>进化<ul><li>让智能体通过用户交互和任务执行过程持续学习和优化，从而提升个性化程度和性能。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;OS-Agent-A-Survey-on-MLLM-based-Agents-for-Computer-Phone-and-Browser-Use&quot;&gt;&lt;a href=&quot;#OS-Agent-A-Survey-on-MLLM-based-Agents-for-Comp</summary>
      
    
    
    
    <category term="智能体" scheme="http://zjn-astonishe.github.io/categories/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
    
    <category term="智能体" scheme="http://zjn-astonishe.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>MCP</title>
    <link href="http://zjn-astonishe.github.io/2025/04/08/MCP/2025-04-08-MCP/"/>
    <id>http://zjn-astonishe.github.io/2025/04/08/MCP/2025-04-08-MCP/</id>
    <published>2025-04-08T07:31:07.000Z</published>
    <updated>2025-04-16T01:52:03.309Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MCP笔记"><a href="#MCP笔记" class="headerlink" title="MCP笔记"></a>MCP笔记</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>当前的大型语言模型(LLM)和智能体在上下文获取和多模型协作上存在诸多限制——数据和上下文孤岛</p><p>多智能体协作与多模型配合的需求，但缺少统一的通信协议。行业需要一种通用机制来让不同模型、不同智能体共享上下文、协调行动。</p><h3 id="数据孤岛"><a href="#数据孤岛" class="headerlink" title="数据孤岛"></a>数据孤岛</h3><ul><li>再强大的模型如果与数据库、知识库等外部信息源隔绝，其能力都会大打折扣。</li><li>移植性差：每接入一个新数据源往往需要定制开发插件或接口。</li></ul><h3 id="上下文孤岛"><a href="#上下文孤岛" class="headerlink" title="上下文孤岛"></a>上下文孤岛</h3><ul><li>多轮推理和长程记忆机制缺乏标准支持，智能体难以在长任务过程中保持连贯的“记忆”。</li><li>用户在不同会话或不同工具间不得不重复提供相同信息。</li></ul><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Model Context Protocol(模型上下文协议, MCP)是由 Anthropic 公司于 2024 年底提出并开源发布的一种开放标准协议，旨在提供统一的方法来管理上下文和整合多源信息，从而打破AI与外部环境之间的壁垒。类似USB-C之于硬件连接，即插即用。定义了AI助手(大型模型)与外部数据源、工具或其它服务交换上下文信息的通用方式，目的是标准化AI与外部世界交互的接口。</p><p>目前，Anthropic的Claude系列，Open AI的GPT系列、Meta的Llama系列，deepseek、阿里的通义系列，Anysphere的Cursor，各种主流模型均已接入MCP生态。</p><ul><li>统一标准接口: 不同的工具和数据源都通过统一协议与模型交互，减少过去需要针对每个系统单独开发连接器的繁琐。</li><li>安全双向通信: 允许数据源向AI提供上下文，也允许AI请求执行操作，同时确保权限和隐私可控。引入身份认证和授权机制(OAuth2.1 标准)，保证模型与工具之间的数据交换是受保护的。</li><li>模块化与可插拔: 将现有系统封装成MCP服务端(数据源侧)对外提供数据/功能，也可以构建支持MCP的AI客户端(应用侧)来消费这些服务。</li><li>上下文保留与智能: 让 AI 拥有持续的上下文。AI助手能够在多轮对话中记住用户偏好并动态学习。<ul><li>有状态性(Statefulness): 支持会话级别和长期的上下文记忆。</li><li>互操作性(Interoperability): 可跨模型、工具和数据源协同。</li><li>以智能体为中心的设计(Agent-Centric Design): 在既定边界内自主决策</li></ul></li><li>模型间通信与协作<ul><li>不仅AI模型可以和传统数据源交互，模型与模型之间也可以通过 MCP 协议进行沟通与协作。<ul><li>跨模型兼容意味着不同厂商或架构的模型都能采用 MCP 接入相同的工具生态。开发者可以“拨动开关”切换底层模型，而无需重写集成代码，从而在不同LLM之间切换自如，实现模型的解耦和替代。</li><li>可以共享同一套上下文和工具：一种协作方式是由一个主代理协调，其他代理作为特定MCP服务器提供专长能力。</li></ul></li><li>所有代理共同访问某个共享的MCP资源(如一个知识库服务器)，从而在各自任务中保持信息同步。</li></ul></li></ul><h2 id="核心原理"><a href="#核心原理" class="headerlink" title="核心原理"></a>核心原理</h2><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p><img src="" alt="MCP架构"></p><ul><li>MCP采用客户端-服务器(Client-Server)架构来组织 AI 与外部资源的交互。<ul><li>服务器: 由数据源或工具一侧实现，负责对外提供标准化的接口，相当于”工具箱”。封装了实际的数据或功能(数据库查询、调用第三方API、文件读取)。对外暴露为一组标准命令或资源访问点。<ul><li>轻量级程序</li><li>MCP的核心，连接AI模型与实际数据源。</li></ul></li><li>客户端: 运行在 AI 智能体一侧，负责向服务器发送请求并获取结果。部署在LLM应用(某个聊天机器人程序，或集成开发环境中的AI助手)。通过协议调用多个服务器以获取所需信息。客户端与每个目标服务器保持一对一的连接。<ul><li>处理通信细节，确保主机和服务器之间的数据传输顺畅。</li></ul></li><li>主机: 承载 AI 应用的环境。例如: Claude桌面应用、某IDE或Agent运行容器。运行着MCP客户端模块和LLM本身。负责管理客户端与服务器的连接和生命周期。<ul><li>需要访问数据的程序。</li><li>MCP生态系统的入口点，负责向用户提供AI功能。</li></ul></li><li>数据源<ul><li>本地数据源：<ul><li>用户计算机上的文件、数据库和服务，MCP服务器可以安全访问这些资源。</li></ul></li><li>远程服务:<ul><li>通过互联网可用的外部系统(如通过API)，MCP服务器可以连接这些系统。</li></ul></li></ul></li></ul></li></ul><h3 id="传输设计"><a href="#传输设计" class="headerlink" title="传输设计"></a>传输设计</h3><ul><li>MCP本身与传输无关。常见实现包括本地标准输入输出管道(stdio)通信、HTTP 通信等都可以用于MCP。<ul><li>在本地部署时，Claude 桌面版通过标准IO管道与本地 MCP 服务器对接</li><li>在分布式环境下，也可以采用 HTTP/HTTPS 请求和长连接来通信</li><li>最新的规范版本进行了改进，用可双向流式传输的HTTP替代了早期的HTTP+SSE，实现实时响应和更好的兼容性。</li></ul></li><li>传输层之上采用JSON-RPC 2.0，便于解析。<ul><li>定义了请求(request)、结果(result)、错误(error)等消息类型 </li></ul></li></ul><h3 id="消息协议"><a href="#消息协议" class="headerlink" title="消息协议"></a>消息协议</h3><ul><li>规定了严格的消息格式和流程，以确保模型与工具之间通信的规范性和可靠性。</li><li>基于JSON-RPC协议，每条请求都包含方法名称、参数等，服务器处理后返回结果或错误码。使不同实现之间具有一致的语言，避免歧义。</li><li>最新规范增加了批量请求(batching)能力，允许客户端一次发送多个请求并行执行，减少交互往返，提高效率。</li><li>协议引入工具注解(Tool Annotations)机制，服务器可以用元数据描述其提供的功能(如功能名称、参数说明、可能的输出等)。客户端(模型)可以读取这些描述，从而“了解”每个工具会做什么。使得智能体能够更加自主地选择和组合工具。不必完全依赖开发者硬编码调用逻辑。</li></ul><h3 id="上下文管理"><a href="#上下文管理" class="headerlink" title="上下文管理"></a>上下文管理</h3><ul><li>引入了会话上下文的概念，让交互变得有状态。维护每个会话的状态，当 AI 再次调用时，无需重复提供已有的信息。<ul><li>支持多步工作流的状态管理：智能体可以记住已执行的动作，根据上下文调整策略，根据反馈自行纠错</li></ul></li><li>允许在多次请求-响应之间保存和传递上下文<ul><li>对话历史、用户偏好、环境信息等</li></ul></li><li>防止上下文无限增长导致信息过载，还实现了上下文压缩和嵌入等机制。<ul><li>非关键性的历史内容可以被压缩为向量嵌入或摘要，保留其语义表示而减少占用。<ul><li>浓缩成要点向量存储和明文结合</li></ul></li><li></li></ul></li></ul><h3 id="与现有AI系统的集成方式"><a href="#与现有AI系统的集成方式" class="headerlink" title="与现有AI系统的集成方式"></a>与现有AI系统的集成方式</h3><ul><li>主要通过客户端和服务器两端的SDK和开放接口来实现整合：<ul><li>客户端: 使用官方提供的SDK将模型包装，使其能够发送/接收MCP定义的JSON-RPC消息。自己的大模型需加入MCP客户端模块，赋予其连接MCP服务器的能力。</li><li>服务端: Anthropic提供了开源的MCP Server示例和模板，开发者可以根据自己的数据源实现对应的接口。任何具有API或可编程接口的系统理论上都能快速变成一个MCP插件。</li></ul></li><li>与代理型框架结合: 与现有的AI Agent框架(LangChain, Haystack)结合，作为统一的工具接入层。<ul><li>LangChain等框架不再需要针对每个新工具编写特定Wrapper，只需调用 MCP 接口即可获取结果，极大简化了扩展能力。</li></ul></li><li>跨平台和多语言支持: Anthropic维护了多种语言的官方SDK，微软则贡献了C#版本以方便在.NET平台集成。</li></ul><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="多智能体协作"><a href="#多智能体协作" class="headerlink" title="多智能体协作"></a>多智能体协作</h3><p>需要多个AI代理协同工作的场景下，MCP提供了标准机制让智能体彼此交流、共享信息。MCP支持多Agent无缝协同，不同Agent可以共享上下文和工具。能够让多个AI在研究分析、客户支持、自动化流程等任务中协同作战，如同一个多才多艺的团队。</p><ul><li>一个负责规划的智能体可以通过 MCP 将任务下达给另一个执行型智能体，并获取其结果；</li><li>多个领域专家型代理(如一个擅长医学，一个擅长法律)可以共同接入同一个知识库MCP服务器，各自贡献和提取所需信息，从而为用户提供综合解答。</li></ul><h3 id="自动化工作流"><a href="#自动化工作流" class="headerlink" title="自动化工作流"></a>自动化工作流</h3><p>通过MCP，AI智能体能够触发和协调多个步骤：每个步骤可能涉及不同的工具或服务。尤其适合那些需要横跨多个IT系统的流程自动化。当环境数据变化时，AI代理可以即使调整策略，而无需人工修改流程脚本。</p><ul><li>招聘流程中，一个AI助理可以通过MCP从招聘系统获取候选人资料，调用日历API安排面试日程，接着使用邮件服务发送通知，最后将结果记录到HR数据库—all in one go。AI助理根据上下文做判断(如候选人时区、本公司面试官空闲时间等)，而具体执行操作都由相应的MCP连接器完成。工作流的上下文(比如当前进行到哪一步、已有的信息)可以在MCP框架下持续传递，避免每个步骤的信息隔断。某一步失败或用户有反馈，AI还能根据全局上下文调整流程。</li><li>在供应链管理中，AI代理可通过 MCP 同时查询库存、物流、销售预测等多个系统的数据，综合决策后再调用下单和调度接口完成操作。</li></ul><h3 id="AI助手和个人智能体"><a href="#AI助手和个人智能体" class="headerlink" title="AI助手和个人智能体"></a>AI助手和个人智能体</h3><p>过去的AI助手大多局限在对话生成，无法执行动作或获取实时信息。</p><ul><li>信息查询类助手<ul><li>调用Web搜索MCP服务器获取实时网页信息，调用数据库MCP接口检索内部资料，调用第三方API获取专门数据，综合整理后反馈给用户。</li></ul></li><li>事务处理类助手</li><li>专业领域助手<ul><li>诸如Zed、Replit、Codeium等代码平台正利用 MCP，让AI能够检索项目代码、读取Git历史，在充分上下文下给出更准确的代码建议。</li><li>通过MCP将Figma设计文件连接给代码生成AI，实现设计到前端代码的自动化转换。</li></ul></li></ul><h2 id="对AI-Agent的增强"><a href="#对AI-Agent的增强" class="headerlink" title="对AI Agent的增强"></a>对AI Agent的增强</h2><h3 id="上下文感知能力显著增强"><a href="#上下文感知能力显著增强" class="headerlink" title="上下文感知能力显著增强"></a>上下文感知能力显著增强</h3><p>借助MCP，智能体获得了持久的”记忆”，可以跨会话、跨工具调用保持对用户和环境的了解。环境感知也同步提升——通过 MCP，智能体可以获知用户当前设备、位置等环境信息并做出相应调整。</p><h3 id="记忆协调与自主学习"><a href="#记忆协调与自主学习" class="headerlink" title="记忆协调与自主学习"></a>记忆协调与自主学习</h3><p>即时上下文存在对话窗口，长程记忆存储在MCP服务器（如外部知识库或日志）中，中程记忆则通过嵌入进行压缩保留。智能体能够主动在不同层次记忆间进行取舍——例如近期对话重点直接保留，久远内容压缩存档以供需要时参考。这种协调机制避免了模型因上下文超长而遗忘关键信息，也控制了噪音干扰。更重要的是，MCP 使智能体可以在任务过程中动态更新其知识。它可以将新获得的信息写回MCP上下文，实现持续的知识学习；也可以根据用户反馈，通过MCP接口调用纠错或优化模块来自我改进。具备了一定程度的自主适应能力——不仅是被动响应，还能总结经验、调整行为，在长期互动中变得更加智能。</p><h3 id="多模型协作与工具融合能力"><a href="#多模型协作与工具融合能力" class="headerlink" title="多模型协作与工具融合能力"></a>多模型协作与工具融合能力</h3><p>一个单一的大模型Agent，通过MCP可以调用其他模型或工具的能力，实质上形成了能力的扩展和组合。不需要内置所有能力，而是可以按需调用外部专长模型(视觉的、语音的、计算的等)，从而成为一个灵活的多才智能体。</p><p>可以无缝切换底层模型，根据任务需求或负载，在不改变智能体行为逻辑的情况下切换所使用的大模型(GPT系列、Claude系列、本地模型等)</p><h3 id="自主决策和智能体性能提升"><a href="#自主决策和智能体性能提升" class="headerlink" title="自主决策和智能体性能提升"></a>自主决策和智能体性能提升</h3><p>标准化的协议和丰富的元信息使智能体可以更加自主地规划和行动。工具注解提供的功能描述等信息，相当于给了智能体一份“操作指南”，使其能够自主发现可用工具并推断使用方法。</p><p>更加可监控和调试。每一步交互都是规范的请求/响应，开发者或运维可以轻易记录并审查智能体的决策过程，针对问题进行优化。</p><h2 id="启示"><a href="#启示" class="headerlink" title="启示"></a>启示</h2><h3 id="学习途径"><a href="#学习途径" class="headerlink" title="学习途径"></a>学习途径</h3><p><a href="https://modelcontextprotocol.io/introduction">开发文档</a></p><h3 id="选择兼容-MCP-的工具和平台"><a href="#选择兼容-MCP-的工具和平台" class="headerlink" title="选择兼容 MCP 的工具和平台"></a>选择兼容 MCP 的工具和平台</h3><p>优先选择那些已经提供 MCP 接口的模型服务或软件系统。目前 Anthropic Claude、OpenAI 的 Agents SDK 等都已兼容MCP。</p><h3 id="评估业务场景，制定集成路线图"><a href="#评估业务场景，制定集成路线图" class="headerlink" title="评估业务场景，制定集成路线图"></a>评估业务场景，制定集成路线图</h3><ul><li><p>出那些最能够从AI智能体和上下文整合中受益的场景。</p><ul><li>客服问答是否需要连接内部知识库，</li><li>业务决策是否需要综合多系统数据，</li><li>研发流程是否因上下文切换效率低下等。</li></ul></li><li><p>接着制定 MCP 集成的路线图：</p><ul><li>第一步可以选取一个痛点明显且技术可行性高的用例进行试点，实现AI代理通过MCP访问一两个关键系统；</li><li>在获得成果和经验后，再逐步扩展到更多系统和复杂流程。</li><li>这样的循序渐进有助于风险可控地引入MCP，同时向组织内部展示价值以获得更多支持。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MCP笔记&quot;&gt;&lt;a href=&quot;#MCP笔记&quot; class=&quot;headerlink&quot; title=&quot;MCP笔记&quot;&gt;&lt;/a&gt;MCP笔记&lt;/h1&gt;&lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="MCP" scheme="http://zjn-astonishe.github.io/categories/MCP/"/>
    
    
    <category term="MCP" scheme="http://zjn-astonishe.github.io/tags/MCP/"/>
    
    <category term="Agent" scheme="http://zjn-astonishe.github.io/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>算法分析与问题的计算复杂度</title>
    <link href="http://zjn-astonishe.github.io/2025/03/10/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/2025-03-10-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
    <id>http://zjn-astonishe.github.io/2025/03/10/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/2025-03-10-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6/</id>
    <published>2025-03-10T11:53:48.000Z</published>
    <updated>2025-06-12T02:16:58.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法分析与问题的计算复杂度"><a href="#算法分析与问题的计算复杂度" class="headerlink" title="算法分析与问题的计算复杂度"></a>算法分析与问题的计算复杂度</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="解决一个计算问题的过程"><a href="#解决一个计算问题的过程" class="headerlink" title="解决一个计算问题的过程"></a>解决一个计算问题的过程</h3><ul><li>是否可计算</li><li>是否能行可计算(在有限的复杂度内完成计算)</li><li>算法设计与分析</li><li>用计算机语言实现算法</li><li>软件系统</li></ul><h3 id="可计算理论"><a href="#可计算理论" class="headerlink" title="可计算理论"></a>可计算理论</h3><ul><li>计算模型</li><li>可计算问题与不可计算问题</li><li>计算模型的等价性——图灵/Church命题</li></ul><h3 id="计算复杂性理论"><a href="#计算复杂性理论" class="headerlink" title="计算复杂性理论"></a>计算复杂性理论</h3><p>在给定的计算模型下研究问题的复杂性</p><ul><li>固有复杂性</li><li>上界</li><li>下界</li><li>平均</li><li>复杂性问题的分类: P=NP?</li><li>抽象复杂性研究</li></ul><h3 id="算法设计和分析"><a href="#算法设计和分析" class="headerlink" title="算法设计和分析"></a>算法设计和分析</h3><ul><li>可计算问题的算法的设计与分析</li><li>设计高效算法，分析算法性能，两者交互验证<ul><li>设计算法的理论、方法和技术(设计正确高效的算法)</li><li>分析算法的理论、方法和技术(判断算法是否足够高效)</li></ul></li></ul><h3 id="算法的定义"><a href="#算法的定义" class="headerlink" title="算法的定义"></a>算法的定义</h3><p>算法是一个满足下列条件的计算: </p><ul><li>输入: 有一个满足给定约束条件的输入</li><li>输出: 满足给定约束条件的结果</li><li>有穷性/终止性: 有限步内必须终止</li><li>可行性: 每一个动作都能够被精确地机械执行<ul><li>反例: 将大元素放数组后，小元素放数组前。但是无法精确执行，因为没有确定一个元素是大元素还是小元素的指标</li></ul></li></ul><h3 id="计算机问题求解的实例——补丁与bug问题"><a href="#计算机问题求解的实例——补丁与bug问题" class="headerlink" title="计算机问题求解的实例——补丁与bug问题"></a>计算机问题求解的实例——补丁与bug问题</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>错误就是人们所说的bug。用户在试用软件时总是希望其错误越少越好，最好是没有错误的。但是推出一个没有错误的软件几乎不可能，所以很多软件公司都在疯狂地发放补丁(有时这种补丁甚至是收费的)。T公司就是其中之一。上个月，T公司推出了一个新的字处理软件，随后发放了一批补丁。最近T公司发现其发放的补丁有致命的问题，那就是一个补丁在排除某些错误的同时，往往会加入另一些错误，此字处理软件中只可能出现n个特定的错误，这n个错误是由软件本身决定的。T公司目前共发放了m个补丁，对于每一个补丁，都有特定的适用环境，某个补丁只有在当前软件中包含某些错误而同时又不不包含另一些错误时才可以使用，如果它被使用，它将修复某些错误而同时加入某些错误。另外，使用每个补丁都要耗一定的时间(即补丁程序的运行时间)。</p><p>现在T公司的问题很简单: 其字处理软件的初始版本不幸地包含了全部n个错误，有没有可能通过使用这m个补丁(任意顺序地使用，一个补丁可使用多次)，使此字处理软件成为一个没有错误的软件。如果可能，希望找到总耗时最少的方案。</p><h4 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h4><p>T公司的T公司的字处理软件中可能出现的n个错误为集合$B=\{b_1, b_2, ···, b_n\}$中的元素，T公司目前共发放了m个补丁，记为；$P=\{p_1, p_2, ···, p_m\}$。对于每一个补丁$P_i$，假设存在错误集合: $B_{i+}$，$B_{i-}$，当软件包含了$B_{i+}$中的所有错误，而没有包含$B_{i-}$中的任何错误时，补丁$P_i$才可以被使用，否则不能使用，显然$B_{i+}$，$B_{i-}$交集为空。补丁$P_i$将修复某些错误而同时加入某些错误，设错误集合$F_{i+}$，$F_{i-}$使用过补丁$P_i$之后，$F_{i-}$中的任何错误都不会在软件中出现，而软件将包含$F_{i+}$中的所有错误，同样$F_{i+}$，$F_{i-}$交集为空。另外，使用每个补丁都要耗一定的时间$T_i$。</p><p>现在T公司的字处理软件的初始版本不幸地包含了全部n个错误，试编写程序判断有没有可能通过使用这m个补丁(任意顺序地使用，一个补丁可使用多次)，使此字处理软件成为一个没有错误的软件。如果可能，希望找到总耗时最少的方案。</p><h4 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h4><p>把n个bug的状态(存在和不存在)的组合用一个0-1字符串(模式串)表示，执行一个补丁本质上就是模式串的转换。<br>模式串可以作为一个图的定点，那么补丁就是图中的边。于是问题则转换为从全1模式串$\{1, 1, …, 1\}$到全0模式串$\{0, 0, …, 0\}$的转换。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/bug%E4%B8%8E%E8%A1%A5%E4%B8%81%E9%97%AE%E9%A2%98.png" alt="img"></p><h4 id="设计最短路径算法"><a href="#设计最短路径算法" class="headerlink" title="设计最短路径算法"></a>设计最短路径算法</h4><ul><li>Dijkstra最短路径算法求解</li></ul><h2 id="寻找最优算法的途径"><a href="#寻找最优算法的途径" class="headerlink" title="寻找最优算法的途径"></a>寻找最优算法的途径</h2><ul><li>设计算法A，求$W(n)$，得到算法类最坏情况下时间复杂度的一个上界</li><li>寻找函数$F(n)$，使得对任何算法都存在一个规模为n的输入，并且该算法在这个输入下至少要做$F(n)$次基本运算，得到该算法类最坏情况下时间复杂度的一个下界<ul><li>给定问题和基本运算后就能确定一个算法类</li></ul></li><li>如果$W(n)=F(n)$或$W(n)=\Theta(F(n))$，则A是最优的，下界是紧的</li><li>如果$W(n)\gt F(n)$，A不是最优的或者$F(n)$的下界过低。需要: <ul><li>改进原算法A或设计新的算法A’，使得$W’(n)\lt W(n)$</li><li>重新证明新下界$F’(n)$，使得$F’(n)\gt F(n)$</li><li>重复以上两个步骤，直到最终得到$W’(n)=F’(n)$或者$W’(n)=\Theta(F’(n))$</li></ul></li></ul><h3 id="平凡下界"><a href="#平凡下界" class="headerlink" title="平凡下界"></a>平凡下界</h3><ul><li>算法的输入规模和输出规模是其平凡下界<ul><li>n阶置换: 求解的时间复杂度下界为$\Omega(n!)$</li><li>求n次实系数多项式在给定x时的值: 求解的时间复杂度下界为$\Omega(n)$</li><li>两个n阶矩阵的乘积: 求解的时间复杂度下界为$\Omega(n^2)$</li><li>货郎问题: 求解的时间复杂度下界为$\Omega(n^2)$</li></ul></li></ul><h3 id="以找最大问题为例"><a href="#以找最大问题为例" class="headerlink" title="以找最大问题为例"></a>以找最大问题为例</h3><ul><li>获得某基本运算的算法类的上界<ul><li>以比较作为基本运算的算法类的上界为: n-1</li></ul></li><li>所设计算法的复杂度<ul><li>在n个数的数组中找最大的数，以比较做基本运算的算法类中的任何算法在最坏情况下至少要做n-1次比较<ul><li>因为最大的数是唯一的，其他n-1个数必须在比较后被淘汰，一次比较至多淘汰一个数，所以至少需要n-1次比较</li></ul></li><li>下界为: n-1</li></ul></li><li>于是所设计的算法便是最优算法</li></ul><h2 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树(Decision Tree)"></a>决策树(Decision Tree)</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>决策树是一棵二叉树。<ul><li>在二叉树的t层至多$2^t$个结点，根节点是第0层</li><li>深度为d的二叉树至多$2^{d+1}-1$个结点</li><li>n个结点的二叉树的深度至少为$\lfloor logn\rfloor$</li><li>设t为二叉树的树叶个数，d为树的深度，如果树的每个内结点都有2个儿子结点，则$t\lt 2^d$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%80%A7%E8%B4%A8%E5%BD%92%E7%BA%B3%E6%B3%95.png" alt="img"></li></ul></li><li>对于给定问题(以比较运算作为基本运算)规定一个决策树的构造规则。求解这个问题的不同算法所构造的决策树结构不一样。<ul><li>一个问题决定了一类决策树，具有相同的构造规则，决定了求解该问题的一个算法类。</li></ul></li><li>给定一个算法的决策树，对于任何输入实例，算法将从树根开始，沿一条路径向下，在每个结点做一次基本操作(例如比较操作)，然后根据比较结果到达某个儿子节点或者在该节点直接停机。对于给定实例的计算恰好对应了一条从树根到树叶或者某个内部结点的路径。</li></ul><h3 id="输入规模"><a href="#输入规模" class="headerlink" title="输入规模"></a>输入规模</h3><ul><li>给定一个算法，对于不同的输入，算法将在对应决策树的某个结点(树叶或者内部结点)停机，将该结点标记为输入，问题的输入规模对应于决策树的结点总数或者叶结点数。</li></ul><h3 id="时间复杂度-确定问题的难度"><a href="#时间复杂度-确定问题的难度" class="headerlink" title="时间复杂度(确定问题的难度)"></a>时间复杂度(确定问题的难度)</h3><ul><li>最坏情况下的时间复杂度对应于决策树的深度。</li><li>平均情况下的时间复杂度对应于决策树的平均路径长度。</li></ul><h2 id="检索算法的时间复杂度分析"><a href="#检索算法的时间复杂度分析" class="headerlink" title="检索算法的时间复杂度分析"></a>检索算法的时间复杂度分析</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>检索问题是给定按递增顺序排列的数组L(项数$n\gt 1$)和数x，如果x在L中，输出x的下标，否则输出0</p><h3 id="顺序检索算法时间复杂度"><a href="#顺序检索算法时间复杂度" class="headerlink" title="顺序检索算法时间复杂度"></a>顺序检索算法时间复杂度</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 顺序检索算法</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">search_inde</span><span class="params">(<span class="type">int</span>* L, <span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> j = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span>(j &lt;= <span class="built_in">len</span>(L) &amp;&amp; L[j] != x)</span><br><span class="line">    j += <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span>(j &gt; <span class="built_in">len</span>(L))</span><br><span class="line">    j = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> j;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设x在L每个位置(n)和空隙(n+1)的概率都是$\frac{1}{2n+1}$。</p><ul><li><p>最坏复杂度: $W(n) = n$</p></li><li><p>平均复杂度: $A(n) = \frac{[(1+2+…+n)+n(n+1)]}{(2n+1)} \approx \frac{3n}{4}$</p></li></ul><h3 id="二分检索算法时间复杂度"><a href="#二分检索算法时间复杂度" class="headerlink" title="二分检索算法时间复杂度"></a>二分检索算法时间复杂度</h3><ul><li>最坏复杂度: $W(n) = \lfloor log n\rfloor + 1, n\gt 1$<ul><li>$W(n) = 1 + W(\lfloor \frac{n}{2} \rfloor) = 1 + \lfloor log\lfloor \frac{n}{2} \rfloor \rfloor + 1 = 1 = \lfloor log\lfloor n \rfloor \rfloor - 1 + 1$(对数除法运算法则)</li></ul></li><li>平均复杂度: <ul><li>令$n=2^k - 1$，$S_t, 1\lt t\lt k$为算法做t次比较时的输入个数 $k \lt logn$</li><li>$S_1 = 1 = 2^0, S_2 = 2 = 2^1, S_3 = 2^2, …, S_t=2^{t-1}, …, S_k=2^{k-1} + n + 1$(n+1)为缝隙中情况</li><li>$A(n) = \frac{1S_1 + 2S_2 + … + kS_k}{2n+1} = \frac{\sum_{t=1}^{k}t2^{t-1}+k(n+1)}{2n+1}=\frac{(k-1)2^k + 1 + k(n+1)}{2n+1}\approx \frac{k-1}{2} + \frac{k}{2} = k - \frac{1}{2} = \lfloor logn \rfloor + \frac{1}{2}$<ul><li>$\sum_{t=1}^{k}t2^{t-1}$拆分为$\sum_{t=1}^{k}t2^{t} - \sum_{t=1}^{k}t2^{t-1} = \sum_{t=1}^{k}t2^t - \sum_{t=0}^{k-1}(t+1)2^t$</li></ul></li></ul></li></ul><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>对于给定输入规模n, 决策树是一棵二叉树, 其结点被标记为 1, 2, … , n, 且标记规则是:</p><ul><li>首先与 x 比较的 L 的项的下标标记为树根。</li><li>假设某结点被标记为 i:<ul><li>i 的左儿子是：当 $x &lt;L(i)$ 时，下一步与 x 比较的项的下标。</li><li>i 的右儿子是：当 $x &gt;L(i)$ 时，下一步与 x 比较的项的下标。</li></ul></li><li>若 $x &lt; L(i)$ 时算法停止, 则 i 没有左儿子</li><li>若 $x &gt; L(i)$ 时算法停止, 则 i 没有右儿子</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%A3%80%E7%B4%A2%E7%AE%97%E6%B3%95%E5%86%B3%E7%AD%96%E6%A0%91.png" alt="img"></p><h4 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><ul><li>给定输入，算法将从根开始，沿一条路径前进，直到某个结点为止。所执行的基本运算次数是这条路径的结点个数，最坏情况下的基本运算次数是树的深度+1。<ul><li>对于任何一个搜索算法存在某个规模为 n 的输入使得该算法至少要做 $\lfloor logn \rfloor + 1$次比较。</li></ul></li><li>对于有序表搜索问题, 在以比较作为基本运算的算法类中, 二分法在最坏情况下是最优的.</li></ul><h2 id="排序算法的时间复杂度分析"><a href="#排序算法的时间复杂度分析" class="headerlink" title="排序算法的时间复杂度分析"></a>排序算法的时间复杂度分析</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bubbleSort</span></span><br><span class="line"><span class="comment">// 交换只发生在相邻的元素之间</span></span><br><span class="line"><span class="function"><span class="type">int</span>* <span class="title">bubbleSort</span><span class="params">(<span class="type">int</span>* L, n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> index = n;</span><br><span class="line">  <span class="keyword">while</span>(index &gt; <span class="number">1</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    k = index - <span class="number">1</span>;</span><br><span class="line">    index = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>; j &lt;= k; j++)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span>(L[j] &gt; L[j+<span class="number">1</span>])</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="type">int</span> tmp = L[j];</span><br><span class="line">        L[j] = L[j+<span class="number">1</span>];</span><br><span class="line">        L[j+<span class="number">1</span>] = tmp;</span><br><span class="line">        index = j;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="置换与逆序"><a href="#置换与逆序" class="headerlink" title="置换与逆序"></a>置换与逆序</h4><ul><li>逆序<ul><li>令$L=\{1, 2, …, n\}$，排序的任何输入为L上的置换。在置换$a_1a_2…a_n$中若$i<j$，但$a_i>a_j$，则称($a_i, a_j$)为该置换的一个逆序。</li></ul></li><li>逆序序列<ul><li>在$i$的右边，并且小于i的元素的个数记作$b_i$，$i=1, 2, …, n$。</li><li>($b_1, b_2, …, b_n$)为置换的逆序序列。<ul><li>总共可以有$n!$个不同的逆序序列(1, 2, 3, …, n个)。</li><li>置换与逆序序列构成一一对应。</li></ul></li></ul></li><li>逆序数<ul><li>置换中的逆序总数($\sum_{i=1}^{n}b_i$)</li></ul></li><li><p>实例</p><ul><li>3, 1, 6, 5, 8, 7, 2, 4</li><li>逆序数: 12</li><li><p>降序逆序序列: </p><p>1|2|3|4|5|6|7|8<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>0|0|2|0|2|3|2|3</p></li></ul></li></ul><h4 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><ul><li>最坏复杂度<ul><li>$W(n) = O(n^2)$，至多巡回$O(n)$次，每次$O(n)$</li><li>对换只发生在相邻元素之间，每次相邻元素交换只消除1个逆序，比较次数不少于逆序数，而最大逆序数为$\frac{n(n-1)}{2}$，于是$W(n)=\Omega(n^2)$</li></ul></li><li>平均复杂度<ul><li>假设各种输入是等可能的，置换$\alpha$的逆序序列是($b_1, b_2, …, b_n$)，置换$\alpha’$的逆序序列为($0-b_1, 1-b_2, …, n-1-b_n$)，$\alpha$和$\alpha’$的逆序数之和为$\frac{n(n-1)}{2}$。将$n!$种不同的置换分成$\frac{n!}{2}$组，每组的逆序之和为$\frac{n(n-1)}{2}$<ul><li>$\alpha’$是补集置换，记录的是顺序数，因为经过置换后，其顺序数的逆序贡献等价于原置换的逆序数的逆序贡献。</li></ul></li><li>平均逆序数为$\frac{n(n-1)}{4}$</li><li>平均交换次数为$\Omega(\frac{n(n-1)}{4})$</li></ul></li><li>因此均为$\Theta(n^2)$</li></ul><h3 id="快速排序与二分归并排序"><a href="#快速排序与二分归并排序" class="headerlink" title="快速排序与二分归并排序"></a>快速排序与二分归并排序</h3><h4 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><ul><li>快速排序<ul><li>最坏情况: $O(n^2)$</li><li>平均情况: $O(nlogn)$</li></ul></li><li>二分归并排序<ul><li>最坏情况: $O(nlogn)$</li><li>平均情况: $O(nlogn)$</li></ul></li></ul><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>设T是一棵深度为d的二叉树，结点为L中的元素。若能满足以下条件，则为堆:</p><ul><li>所有内结点(可能一点除外)的度数为2</li><li>所有树叶至多在相邻的两层</li><li>d-1层的所有树叶在内结点的右边</li><li>d-1层最右边的内结点可能度数为1(只有左儿子，没有右儿子)</li><li>每个结点的元素不小于儿子的元素(不满足这条的称作堆结构或者伪堆)</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86.png" alt="img"></p><h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><ul><li>根据二叉树的性质和堆的定义<ul><li>$left(i)=2i$</li><li>$right(i)=2i+1$</li></ul></li><li><p>n个结点的堆恰好有$\lceil \frac{n}{2}\rceil$片树叶<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E6%A0%91%E5%8F%B6%E8%AF%81%E6%98%8E1.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E6%A0%91%E5%8F%B6%E8%AF%81%E6%98%8E2.png" alt="img"></p></li><li><p>结点的高度: 结点到树叶的距离</p><ul><li>同一高度的结点分布在树的不同层<ul><li>结点的深度: 结点到树根的距离</li></ul></li><li>同一深度的结点分布在树的同一层</li></ul></li></ul><h4 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h4><ul><li><p>整理</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%95%B4%E7%90%86.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%95%B4%E7%90%86%E5%AE%9E%E4%BE%8B.png" alt="img"></p></li><li><p>建堆</p><ul><li>从下往上整理，堆有n个结点，树叶有$\lceil\frac{n}{2}\rceil$片，最大的内结点标号为$n-\lceil\frac{n}{2}\rceil=\lfloor\frac{n}{2}\rfloor$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E5%BB%BA%E5%A0%86.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E5%BB%BA%E5%A0%86%E5%AE%9E%E4%BE%8B.png" alt="img"></li></ul></li><li><p>排序</p><ul><li>建堆后从后往前每个结点下沉<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%8E%92%E5%BA%8F.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%8E%92%E5%BA%8F%E5%AE%9E%E4%BE%8B1.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%8E%92%E5%BA%8F%E5%AE%9E%E4%BE%8B2.png" alt="img"></li></ul></li></ul><h4 id="复杂度分析-3"><a href="#复杂度分析-3" class="headerlink" title="复杂度分析"></a>复杂度分析</h4><ul><li><p>整理</p><ul><li>每次调用为$O(1)$，子堆大小至多为原来的$\frac{2}{3}$<ul><li><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E6%95%B4%E7%90%86%E5%A4%8D%E6%9D%82%E5%BA%A6.png" alt="img"></li></ul></li><li>递推不等式$T(n)\lt T(\frac{2n}{3}) + \Theta(1)$<ul><li>$T(n) = \Theta(logn)$($T(h) = \Theta(h)$)<ul><li>h为结点i的高度(距离树叶的最大距离)，n为以i为根的子树的结点数。</li></ul></li></ul></li></ul></li><li><p>建堆</p><ul><li>按照高度来计数结点数，乘以$O(h)$，再求和。</li><li>而整理的复杂度依赖于所整理结点的高度。</li><li>$T(n) = \sum_{\lfloor logn \rfloor}^{h=0}高为h的结点数 \times O(h)$<ul><li>n个元素的堆高度为h的层至多存在$\lceil\frac{n}{2^{h+1}}\rceil$个结点(归纳法证明)。<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E7%9A%84%E9%AB%98%E5%BA%A6%E5%92%8C%E7%BB%93%E7%82%B9%E6%95%B01.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E7%9A%84%E9%AB%98%E5%BA%A6%E5%92%8C%E7%BB%93%E7%82%B9%E6%95%B02.png" alt="img"></li></ul></li><li>n个结点的建堆算法的时间复杂度是$O(n)$<ul><li>对高为h的结点调用整理算法的时间是$O(h)$: $T(n) = \sum_{h=0}^{\lfloor logn \rfloor}\lceil\frac{n}{2^{h+1}}\rceil O(h) = O(n\sum_{h=0}^{\infin}\frac{h}{2^{h+1}} = O(n)$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E5%BB%BA%E5%A0%86%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%8E%A8%E5%AF%BC1.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%A0%86%E8%BF%90%E7%AE%97%E5%BB%BA%E5%A0%86%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%8E%A8%E5%AF%BC2.png" alt="img"></li></ul></li></ul></li><li><p>排序</p><ul><li>$O(nlogn)$<ul><li>建堆复杂度为$O(n)$</li><li>整理需要n-1次，每次$O(logn)$</li></ul></li></ul></li></ul><h3 id="排序问题的决策树"><a href="#排序问题的决策树" class="headerlink" title="排序问题的决策树"></a>排序问题的决策树</h3><p>考虑以比较运算作为基本运算的排序算法类<br>任取算法，输入$L=\{x_1, x_2, …, x_n\}$</p><ul><li>第一次比较的元素为$x_i, x_j$，作为决策树根为$(i, j)$</li><li><p>假设结点k已经标记为$(m, n)$</p><ul><li>$x_m &lt; x_n$: <ul><li>如果算法结束了，则将k的左儿子标记为输出。</li><li>如果下一步是比较$x_p, x_q$，则k的左儿子标记为$(p, q)$</li></ul></li><li>$x_m &gt; x_n$<ul><li>如果算法结束了，则将k的右儿子标记为输出</li><li>如果下一步是比较$x_p, x_q$，则k的右儿子标记为$(p, q)$</li></ul></li></ul></li><li><p>任意输入对应了决策树中从树根到树叶的一条路径，算法最坏情况下的比较次数则为树的深度。</p></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91.png" alt="img"></p><ul><li>删除非二叉的内结点(灰色结点)，得到的二叉树为B-树，其深度肯定不超过决策树的深度，有$n!$片树叶</li></ul><ul><li>设t为B-树中的树叶数，d为树的深度，$t\lt 2^d$(归纳法证明)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/B%E6%A0%91%E5%8F%B6%E5%AD%90%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%85%B3%E7%B3%BB.png" alt="img"></li></ul><h4 id="最坏情况复杂度的下界"><a href="#最坏情况复杂度的下界" class="headerlink" title="最坏情况复杂度的下界"></a>最坏情况复杂度的下界</h4><ul><li>对于给定的n，任何通过比较对n个元素排序的算法的决策树的深度至少为$\lceil logn!\rceil$(从上两条结论得到)</li><li>任何通过比较对n个元素排序的算法在最坏情况下的时间复杂度是$\lceil logn! \rceil$，可近似为$nlogn - 1.5n$<ul><li>最坏情况下的比较次数是树的深度。</li><li>证明思路: 用对数乘加变换<ul><li>$log n! = \sum_{j=1}^{n}log j \gt \int_1^n logx dx = loge\int_{1}^{n}lnx dx = loge(nlnn - n + 1) = nlogn - nloge + loge \approx nlogn - 1.5n$<ul><li>$x = e^{lnx}$</li></ul></li></ul></li></ul></li><li>堆排序算法在最坏情况阶达到最优。<ul><li>$log(n!) = \sum_{k=1}^{n}logk \gt \int_{1}^{n}logxdx=nlnn - n + 1 = \Omega(nlogn)$</li><li>$log(n!) = \sum_{k=1}^{n}logk \gt \int_{2}^{n+1}logxdx=O(nlogn)$</li></ul></li></ul><h4 id="平均情况复杂度的下界"><a href="#平均情况复杂度的下界" class="headerlink" title="平均情况复杂度的下界"></a>平均情况复杂度的下界</h4><ul><li>epl(T): 假设所有的输入等概率分布，令epl(T)表示B-树中从根到树叶的所有路径长度之和，$\frac{epl(T)}{n!}$的最小值对应平均情况时间复杂性的下界。<ul><li>分析具有最小epl(T)值的树的结构求得最小值。</li></ul></li><li>在具有t片树叶的所有B-树中，树叶分布在两个相邻层上的树的epl值最小。<ul><li>反证法?: 假设树T的深度为d，假设树叶x在第k层，$k&lt;d-1$。取d-1层的某个结点y，y有两个儿子是第d层的树叶，y的两个儿子作为x的儿子得到树T’<ul><li>$epl(T)-epl(T’)=(2d+k) - (d-1) + 2(k+1) = d-k-1 &gt; 0$</li><li>T’的树叶相距层数小于T的树叶相距层数，且T’的epl值小于T的epl值</li></ul></li></ul></li><li><p>epl的下界</p><ul><li>具有t片树叶且epl值最小的B-树T满足: $epl(T) = t\lfloor logt\rfloor + 2(t-2^{\lfloor logt \rfloor})$<ul><li>$d\gt \lceil logt\rceil$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/epl%E4%B8%8B%E7%95%8C%E8%AF%81%E6%98%8E1.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/epl%E4%B8%8B%E7%95%8C%E8%AF%81%E6%98%8E2.png" alt="img"></li></ul></li></ul></li><li><p>在输入等概分布下任何通过比较对n个项排序的算法平均比较次数至少为$\lfloor log n!\rfloor$，近似为$nlogn - 1.5n$</p><ul><li>算法类中任何算法的平均比较次数是该算法决策树T的$\frac{epl(T)}{n!}$<ul><li>$A(n)\gt \frac{epl(T)}{n!} = \frac{n!\lfloor logn!\rfloor + 2(n! - 2^{\lfloor logn!\rfloor})}{n!}=\lfloor logn!\rfloor +\epsilon\approx nlogn - 1.5n(0\lt \epsilon &lt; 1)$</li></ul></li></ul></li><li><p>堆排序在平均情况下阶达到最优。</p></li></ul><h3 id="排序算法的比较"><a href="#排序算法的比较" class="headerlink" title="排序算法的比较"></a>排序算法的比较</h3><div class="table-container"><table><thead><tr><th style="text-align:center">算法</th><th style="text-align:center">最坏情况</th><th style="text-align:center">平均情况</th><th style="text-align:center">占用空间</th><th style="text-align:center">时间最优性</th></tr></thead><tbody><tr><td style="text-align:center">冒泡排序</td><td style="text-align:center">$O(n^2)$</td><td style="text-align:center">$O(n^2)$</td><td style="text-align:center">原地</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">快速排序</td><td style="text-align:center">$O(n^2)$</td><td style="text-align:center">$O(nlogn)$</td><td style="text-align:center">$O(logn)$</td><td style="text-align:center">平均最优</td></tr><tr><td style="text-align:center">归并排序</td><td style="text-align:center">$O(nlogn)$</td><td style="text-align:center">$O(nlogn)$</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">最优</td></tr><tr><td style="text-align:center">堆排序</td><td style="text-align:center">$O(nlogn)$</td><td style="text-align:center">$O(nlogn)$</td><td style="text-align:center">原地</td><td style="text-align:center">最优</td></tr></tbody></table></div><h2 id="选择算法的时间复杂度分析"><a href="#选择算法的时间复杂度分析" class="headerlink" title="选择算法的时间复杂度分析"></a>选择算法的时间复杂度分析</h2><p><img src="" alt="img"></p><h3 id="管道问题"><a href="#管道问题" class="headerlink" title="管道问题"></a>管道问题</h3><ul><li>某区域有n口油井，需要修建输油管道。根据设计要求，水平方向有一条主管道，每口油井修一条垂直方向的支管道通向主管道。问如何选择主管道的垂直方向位置，使得支管道长度的综合最小？</li><li><p>最优解是垂直方向上的所有支管道坐标的中位数</p></li><li><p>简单算法</p><ul><li>调用k次选最小算法，复杂度为$O(kn)$</li><li>先排序，然后输出第k小的数，复杂度为$O(nlogn)$</li></ul></li><li>分治算法<ul><li>假设元素彼此不等</li><li>用某个元素$m^<em>$作为标准将总集合$S$划分为$S_1$($&lt;m^</em>$), $S_2$($&gt;m^*$)。</li><li>分治:<ul><li>如果$k\lt |S_1|$，则在$S_1$中找第k小</li><li>如果$k = |S_1| + 1$，则$m^*$是第k小</li><li>如果$k\gt |S_1| + 1$，则在$S_2$中找第$k-|S_1|-1$小</li></ul></li><li>伪码<ul><li>行2: $O(n)$，每5个数找中位数构成M。</li><li>行3: $W(\frac{n}{5})$，在M中再找中位数$m^*$。</li><li>行4: $O(n)$，用$m^*$划分集合$S$。</li><li>行8-9: $W(\frac{7n}{10})$，进行递归</li><li>$W(n)\lt W(\frac{n}{5}) + W(\frac{7n}{10}) + O(n)$<br><img src="" alt="img"></li></ul></li><li>该算法的效率取决于子问题的规模<ul><li>算法工作量为$O(n)$<ul><li>$W(n) \lt cn(1+0.9+0.9^2+…) = O(n)$</li><li>$|M|$与归约后子问题的规模之和小于n，递归树每行的工作量便能构成公比小于1的等比级数，于是算法复杂度为$O(n)$<br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"></li></ul></li></ul></li><li>分组时如果不是5个元素一组，而是3个或7个<ul><li>求$m^*$的工作量与$|M|=\frac{n}{t}$相关，t为每组的元素数，如果t大，那么$|M|$就小。</li><li>归约后的子问题大小与分组元素t有关，t大，子问题规模就大。<ul><li>复杂度满足方程$W(n) = W(\frac{n}{3}) + W(\frac{4n}{6}) + cn = \Theta(nlogn)$<br><img src="" alt="img"></li></ul></li></ul></li></ul></li></ul><h3 id="下界的证明方法-构造最坏的输入"><a href="#下界的证明方法-构造最坏的输入" class="headerlink" title="下界的证明方法: 构造最坏的输入"></a>下界的证明方法: 构造最坏的输入</h3><ul><li>任意给定一个算法，对于任意输入x都存在一个确定的操作序列$\tau$<ul><li>$\tau$中的操作分成两类:<ul><li>决定性的: 能够对确定输出结果提供有效的信息。</li><li>非决定性的: 对确定结果没有帮助的冗余操作。</li></ul></li></ul></li><li>最坏情况便是: <ul><li>根据算法构造某个输入实例x，使得算法对x的操作序列$\tau$包含尽量多的冗余非决定性操作。</li><li>给出冗余操作+必要操作的计数公式便能得到下界。</li></ul></li></ul><h3 id="选最大与最小的算法"><a href="#选最大与最小的算法" class="headerlink" title="选最大与最小的算法"></a>选最大与最小的算法</h3><ul><li>任何通过比较找最大和最小的算法至少需要$\lceil\frac{3n}{2}\rceil - 2$次比较。<ul><li>任意给定一个算法，根据算法的比较结果构造输入T，使得算法对T至少做$\lceil\frac{3n}{2}\rceil-2$次比较。</li><li>不妨假设n彼此不等的数，max代表最大，min代表最小。<ul><li>基于比较的暴力算法要找到最大必须确定有n-1个数比max小。找到最小的数必须确定有n-1个数比min小。共$2n-2$次比较。</li><li>FindMaxMin算法将数两两分组比较，是最优算法，是复杂度的下界。<br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"><br><img src="" alt="img"></li></ul></li></ul></li></ul><h3 id="找第二大问题"><a href="#找第二大问题" class="headerlink" title="找第二大问题"></a>找第二大问题</h3><ul><li><p>元素x的权: $w(x)$，表示以x为根的子树中的结点数。</p><ul><li>初始，$w(x_i)=1, i=1, 2, 3, …, n$</li><li>赋值原则: 在比较的时候进行赋值或者调整赋值。只对没有失败过的元素(权大于0)进行赋值。权大者胜，原来胜的次数多的仍旧是胜，输入值也大。<ul><li>$w(x), w(y) &gt; 0$<ul><li>权大者胜: 若$w(x) &gt; w(y)$，那么x的值大于y的值，$w(x)\leftarrow w(x) + w(y), w(y)\leftarrow 0$。</li><li>权等则任意分配: 若$w(x) = w(y)$，那么可考虑为x的值大于y的值的情况，也可考虑为y的值大于x的值。</li><li>无意义不变: 若$w(x) = w(y) = 0$，x和y值不变。</li></ul></li></ul></li></ul></li><li><p>构造树</p><ul><li>初始是森林，包含n个结点。</li><li>若x, y是子树的树根，则算法比较x, y。</li><li>若x, y以前没有参加过比较，任意赋值给x, y，比如$x&gt;y$，那么y作为x的儿子。</li><li>若x, y已经在前面的比较中赋过值，且$x&gt;y$，那么把y作为x的儿子，以y为根的子树作为x的子树。<br><img src="" alt="img"></li></ul></li><li><p>任何找第二大算法，针对上述赋值规则产生的输入，通过与最大元素的比较直接淘汰的元素数至少是$\lceil logn\rceil$。</p><ul><li>$w_k$表示max在第k次比较后形成以max为根子树的结点总数。$w_k\lt 2w_{k-1}$。</li><li>设K为max最终与权不为0的结点的比较次数。<ul><li>$n=w_K\lt 2^K\Rightarrow K\gt logn \Rightarrow K \gt \lceil logn\rceil$</li></ul></li></ul></li><li>任何通过比较找第二大的算法至少需要$n+\lceil logn\rceil-2$次比较。<ul><li>确定最大需要n-1次比较。</li><li>确定第二大还要淘汰K-1个元素，至少用$\lceil logn\rceil - 1$次比较。</li></ul></li><li>锦标赛算法是找第二大的最优算法。</li></ul><h3 id="找中位数问题"><a href="#找中位数问题" class="headerlink" title="找中位数问题"></a>找中位数问题</h3><ul><li>设n为奇数，任何通过比较运算找n个数的中位数(median)的算法在最坏情况下至少做$\frac{3n}{2} - \frac{3}{2}$次比较。<ul><li>决定性的比较: 建立x与median的关系的比较，比较时y和median的关系可以不知道。<ul><li>$\exist y (x&gt;y \&amp; y\gt median)$，x满足上述条件的第一次比较。</li><li>$\exist y (x&lt;y \&amp; y\lt median)$，x满足上述条件的第一次比较。</li></ul></li><li>非决定性的比较<ul><li>$x&gt;median, y<median$，这时$x>y$的比较不是决定性的，因为根据median本可知道x和y的大小。</li></ul></li></ul></li><li><p>为了找到中位数，必须要做n-1次决定性的比较，针对算法构造输入，使得非决定性比较达到$\frac{n-1}{2}$次。</p></li><li><p>对于求中位数的任意算法，构造输入</p><ul><li>分配一个值给中位数median。</li><li>如果算法比较x与y，且x与y没有被赋值，那么赋值x, y使得$x&gt;median, y&lt;median$。</li><li>如果算法比较x与y，且$x&gt;median$，y没有被赋值，则赋值y使得$y&lt;median$。</li><li>如果算法比较x与y，且$x<median$，y没有被赋值，则赋值y使得$y>median$。</li><li>如果存在$\frac{n-1}{2}$个元素已得到小于median的值，则对未赋值的全部元素分配大于median的值。</li><li>如果存在$\frac{n-1}{2}$个元素已得到大于median的值，则对未赋值的全部元素分配小于median的值。</li><li>如果只剩下1个元素，则分配为median。<br><img src="" alt="img"></li></ul></li><li><p>复杂度分析</p><ul><li>Select算法在阶上达到最优。<br><img src="" alt="img"></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><div class="table-container"><table><thead><tr><th style="text-align:center">问题</th><th style="text-align:center">算法</th><th style="text-align:center">最坏情况</th><th style="text-align:center">问题下界</th><th style="text-align:center">最优性</th></tr></thead><tbody><tr><td style="text-align:center">找最大</td><td style="text-align:center">Findmax</td><td style="text-align:center">$n-1$</td><td style="text-align:center">n-1</td><td style="text-align:center">最优</td></tr><tr><td style="text-align:center">找最大最小</td><td style="text-align:center">FindMaxMin</td><td style="text-align:center">$\lceil\frac{3n}{2}\rceil-2$</td><td style="text-align:center">$\lceil\frac{3n}{2}\rceil-2$</td><td style="text-align:center">最优</td></tr><tr><td style="text-align:center">找第二大</td><td style="text-align:center">锦标赛</td><td style="text-align:center">$n+\lceil logn\rceil - 2$</td><td style="text-align:center">$n+\lceil logn\rceil - 2$</td><td style="text-align:center">最优</td></tr><tr><td style="text-align:center">找中位数</td><td style="text-align:center">Select(找第$\frac{k}{2}$小)</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$\frac{3n}{2}-\frac{3}{2}$</td><td style="text-align:center">阶最优</td></tr><tr><td style="text-align:center">找第k小</td><td style="text-align:center">Select</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$n+\min\{k, n-k+1\}-2$</td><td style="text-align:center">阶最优</td></tr></tbody></table></div><h2 id="归约确认问题"><a href="#归约确认问题" class="headerlink" title="归约确认问题"></a>归约确认问题</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;算法分析与问题的计算复杂度&quot;&gt;&lt;a href=&quot;#算法分析与问题的计算复杂度&quot; class=&quot;headerlink&quot; title=&quot;算法分析与问题的计算复杂度&quot;&gt;&lt;/a&gt;算法分析与问题的计算复杂度&lt;/h1&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; cla</summary>
      
    
    
    
    <category term="高级算法设计" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="高级算法设计" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>消息序和组通信</title>
    <link href="http://zjn-astonishe.github.io/2025/01/11/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2025-01-11-%E6%B6%88%E6%81%AF%E5%BA%8F%E5%92%8C%E7%BB%84%E9%80%9A%E4%BF%A1/"/>
    <id>http://zjn-astonishe.github.io/2025/01/11/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2025-01-11-%E6%B6%88%E6%81%AF%E5%BA%8F%E5%92%8C%E7%BB%84%E9%80%9A%E4%BF%A1/</id>
    <published>2025-01-11T11:43:17.000Z</published>
    <updated>2025-04-14T03:05:11.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="消息序和组通信"><a href="#消息序和组通信" class="headerlink" title="消息序和组通信"></a>消息序和组通信</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>消息顺序<ul><li>非FIFO和FIFO</li><li>因果顺序(causal order)</li><li>同步顺序(synchronous order)</li></ul></li><li>多播组通信<ul><li>因果顺序(causal order)</li><li>完全顺序(total order)</li></ul></li><li>故障发生时的预期行为语义</li><li>多播<ul><li>应用层和网络层参与</li></ul></li></ul><h3 id="一些符号"><a href="#一些符号" class="headerlink" title="一些符号"></a>一些符号</h3><ul><li>网络$(N, L)$</li><li>事件集合$(E, \prec)$</li><li>消息$m^i$: <ul><li>发送该消息的事件: $s^i$</li><li>接收该消息的事件: $r^i$</li></ul></li><li>发送事件: $s$</li><li>接收事件: $r$</li><li>消息$M$:<ul><li>发送: send(M)</li><li>接收: receive(M)</li></ul></li><li>对应事件: $a\sim b$，表示两个事件在同一个进程中发生</li><li>发送接收对: $T=\{(s, r)\in E_i\times E_j | s\quad corresponds\quad to\quad r\}$</li></ul><h3 id="异步和FIFO执行"><a href="#异步和FIFO执行" class="headerlink" title="异步和FIFO执行"></a>异步和FIFO执行</h3><h4 id="异步执行"><a href="#异步执行" class="headerlink" title="异步执行"></a>异步执行</h4><ul><li>$(E, \prec)$，因果关系是一个偏序关系。</li><li>没有因果循环</li><li>逻辑链路上的交付不一定遵循FIFO顺序，例如，网络层IPv4无连接服务。</li></ul><h4 id="FIFO执行"><a href="#FIFO执行" class="headerlink" title="FIFO执行"></a>FIFO执行</h4><ul><li>对于所有的$(s, r)\in T$和$(s’, r’)\in T$<ul><li>如果$s\sim s’ \And r\sim r’ \And s\prec s’$，则$r\prec r’$($\prec$的意思是顺序，前面先发生，后面后发生)</li></ul></li><li>所有物理链路都遵循FIFO顺序。</li><li>逻辑链路本质上是非FIFO的。</li><li>为了在非FIFO链路上实现FIFO: <ul><li>每条消息中使用$⟨seq_num,conn_id⟩$(序号，连接号)。</li><li>接收者使用缓冲区排序消息</li><li>例如传输层的面向连接的服务TCP。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%20and%20FIFO%20Executions.png" alt="img"></p><ul><li>左图是非FIFO执行，右图是FIFO执行</li></ul><h2 id="因果序-Causal-Order-CO"><a href="#因果序-Causal-Order-CO" class="headerlink" title="因果序(Causal Order, CO)"></a>因果序(Causal Order, CO)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>对于所有$(s, r)\in T$和$(s’, r’)\in T$，如果$r\sim r’$且$s\prec s’$，则$r\prec r’$<ul><li>如果发送事件$s$和$s’$通过因果关系相关联(不是物理时间顺序)，那么对应的接收事件$r$和$r’$在所有共同目的地以相同的顺序发生</li></ul></li><li><p>如果$send(m^1)\prec send(m^2)$，则对于消息$m^1, m^2$的每个共同目的地$d$，必须满足$deliver_d(m^1)\prec deliver_d(m^2)$</p><ul><li>当两个消息由同一进程发送时，CO 退化为 FIFO。</li></ul></li><li><p>用途: 更新共享数据，实现分布式共享内存、公平资源分配、协作应用、事件通知系统、分布式虚拟环境。</p></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Causal%20Order.png" alt="img"></p><h3 id="消息的到达与交付"><a href="#消息的到达与交付" class="headerlink" title="消息的到达与交付"></a>消息的到达与交付</h3><ul><li>消息到达<ul><li>到达操作系统缓冲区的消息$m$在$P_i$可能需要延迟，直到$m$发送之前发送给$P_i$的消息(被超越的)到达</li><li>应用程序处理到达消息的事件被称为交付事件(接收不等于交付)</li></ul></li><li>没有消息被一系列消息超越: 同一发送者和接收者对之间没有消息被一系列消息超越</li></ul><h3 id="CO的其他特征"><a href="#CO的其他特征" class="headerlink" title="CO的其他特征"></a>CO的其他特征</h3><h4 id="消息顺序"><a href="#消息顺序" class="headerlink" title="消息顺序"></a>消息顺序</h4><ul><li>对于所有$(s, r)$和$(s’, r’)\in T$，如果$s\prec s’$，则$r\prec r’$</li></ul><h4 id="空区间属性-Empty-Interval-EI"><a href="#空区间属性-Empty-Interval-EI" class="headerlink" title="空区间属性(Empty-Interval, EI)"></a>空区间属性(Empty-Interval, EI)</h4><ul><li><p>$(E, \prec)$是一个EI执行，如果对于所有$(s, r)\in T$，在偏序关系中的开区间集合$\{x\in E | s\prec x\prec r\}$为空</p><ul><li>对于$EI<s, r>$，存在某个线性扩展使得相应的区间$\{x\in E | s\prec x\prec r\}$为空<ul><li>线性扩展是一个偏序关系$(E, \prec)$的任何全序关系$(E, \lt)$，其中偏序关系的所有排序关系都被保留</li><li>线性扩展中，一个空的$(s, r)$区间意味着$s$和$r$是可以任意接近的，即发送和接收是可以接近的，由垂直箭头表示</li></ul></li><li>执行$E$是CO的，当且仅当对于每个$M$，存在某个时空图，在该图中该消息$M$可以被画为垂直的箭头<ul><li>但是CO不能推出所有的消息都可以在同一个时空图中被画成垂直箭头，否则所有的发送和接收$(s, r)$区间在相同的线性扩展中为空，同步执行<ul><li>在因果顺序中，事件之间的关系是基于因果关系的偏序关系。这意味着，并非所有事件都可以被直接比较；有些事件可能是并行发生的，即它们之间没有因果关系。如果所有消息都能在同一时空图中画为垂直箭头，那么这将意味着存在一个全序关系，可以对所有事件进行排序。然而，在实际的分布式系统中，由于不同的进程可能有不同的时钟，并且不是所有的事件都具有因果关系，所以无法保证所有事件能够被如此排序。</li><li>如果所有消息可以在同一时空图中画为垂直箭头，那意味着我们实际上是在描述一种同步执行的情况。在这种情况下，每个消息的发送和接收事件在所有进程中看起来都是按相同的顺序发生的。然而，现实中的分布式系统通常是异步的，即不同进程的时间线是独立的，消息传递可能会有延迟，而且不同进程的时钟也可能不同步。因此，不可能保证所有消息的发送和接收事件能按照同一个时间线来表示。</li><li>对于一个满足因果顺序的执行来说，它并不需要满足空区间属性，意味着，在两个事件s和r之间可能存在其他事件x，这些事件既不在s的过去也不在r的未来。这样的情况就不能简单地通过一条垂直箭头来表示，因为那样会暗示s和r是连续的、中间没有任何其他事件发生。</li><li>满足因果顺序的执行允许存在弱共同过去和未来的概念，即一个事件可以发生在消息的发送事件和接收事件之间。如果所有消息都在同一时空图中画为垂直箭头，则意味着不存在这样的中间事件，这与分布式系统的实际情况不符。</li></ul></li></ul></li></ul><h4 id="共同过去和未来-Common-Past-and-Future"><a href="#共同过去和未来-Common-Past-and-Future" class="headerlink" title="共同过去和未来(Common Past and Future)"></a>共同过去和未来(Common Past and Future)</h4><ul><li>一个执行$(E, \prec)$是CO当且仅当对于每对$(s, r)\in T$和每个事件$e\in E$<ul><li>弱共同过去: $e\prec r \Rightarrow \lnot(s\prec e)$</li><li>弱共同未来: $s\prec e\Rightarrow \lnot(e\prec r)$</li><li>如果$s$和$r$的过去是相同的(或者未来是相同的)，则$e\prec r\Rightarrow e\prec s$和$s\prec e\Rightarrow r\prec e$，得到CO执行的一个子类，即同步执行(synchronous executions)</li></ul></li></ul></li></ul><h2 id="同步执行-Synchronous-executions"><a href="#同步执行-Synchronous-executions" class="headerlink" title="同步执行(Synchronous executions)"></a>同步执行(Synchronous executions)</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><ul><li>同步因果关系$&lt;&lt;$在$E$上是最小的传递关系，满足以下条件<ul><li>如果事件x在同一进程中发生在事件y之前，则$x&lt;&lt;y$，事件发生在之前则因果在前</li><li>如果$(s, r)\in T$，则对于所有$x\in E$，$[(x &lt;&lt; s \Leftrightarrow x &lt;&lt; r)]$，且$[(s &lt;&lt; x \Leftrightarrow r &lt;&lt; x)]$，如果s和r有因果关系，则与s或r有因果关系的事件x也必须保持相同的因果顺序</li><li>如果$x&lt;&lt;y \And y &lt;&lt; z$则$x &lt;&lt; z$，因果顺序是传递的</li></ul></li><li>同步执行<ul><li>$(E, &lt;&lt;)$是同步执行，其中因果关系$&lt;&lt;$是一个偏序关系</li></ul></li><li>对同步执行进行时间戳<ul><li>执行$(E, \prec)$是同步的当且仅当存在从$E$到$T$(标量时间戳)的映射，使得<ul><li>对于任何消息$M$，$T(s(M))=T(r(M))$，即发送事件和接收事件具有相同的时间戳</li><li>对于每个进程$P_i$，如果$e_i\prec e_i’$，则$T(e_i)\lt T(e_i’)$，之前发生的事件时间戳更小</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Synchronous%20Executions1.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/deadlocks%20when%20using%20synchronous%20primitives.png" alt="img"></p><h2 id="异步执行与同步通信-Asynchronous-Execution-with-Synchronous-Communication"><a href="#异步执行与同步通信-Asynchronous-Execution-with-Synchronous-Communication" class="headerlink" title="异步执行与同步通信(Asynchronous Execution with Synchronous Communication)"></a>异步执行与同步通信(Asynchronous Execution with Synchronous Communication)</h2><ul><li>一个异步系统编写的程序使用同步原语运行(发送和接收必须配对)可能会导致死锁<ul><li>死锁发生在两个或多个进程互相等待对方的操作完成，从而无法继续执行。</li></ul></li><li>非RSC执行是指在同步通信下不可实现的异步执行。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/non-RSC%20A-executions.png" alt="img"></p><h3 id="RSC执行"><a href="#RSC执行" class="headerlink" title="RSC执行"></a>RSC执行</h3><ul><li><p>Realizable with Synchronous Communication (RSC) 是一个概念，用于描述异步系统中的执行是否可以被解释为等效于同步通信环境下的执行。如果一个异步执行(A-execution)可以通过某种方式重新排序，使得它看起来就像是在一个所有消息传递都是即时完成的同步系统中发生的，那么这个异步执行就是可由同步通信实现的(RSC)。</p></li><li><p>非分离线性扩展$(E, \prec)$</p><ul><li>$(E, \prec)$的一个线性扩展，使得对于每一对$(s, r)\in T$，区间$\{x\in E | s\prec x\prec r\}$是空的</li></ul></li><li>RSC执行 <ul><li>一个异步执行是一个RSC执行，当且仅当存在一个非分离线性扩展的偏序关系$(E, \prec)$(在所有可能的线性扩展中，至少有一个满足非分离条件。)<ul><li>为了确定一个异步执行是否是RSC，我们需要找到一个线性扩展(即对事件的一个全序排列)，在这个排列中，对于每一个消息$m$的发送事件$s(m)$和接收事件$r(m)$，在它们之间的开区间$\{x\in E | s(m)\prec x\prec r(m)\}$是空的。这意味着没有其他事件发生在消息的发送和接收之间，就像同步系统中那样。</li><li>检查所有线性扩展具有指数级成本！</li><li>实际测试使用Crown皇冠特征化方法。</li></ul></li></ul></li></ul><h3 id="Crown测试法"><a href="#Crown测试法" class="headerlink" title="Crown测试法"></a>Crown测试法</h3><h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><ul><li>设$E$是一个执行，在$E$中，大小为k的Crown是一个序列$&lt;(s^i, r^i), i\in \{0, 1, …, k-1\}&gt;$，其中包含对应发送事件和接收事件配对，满足以下条件:<ul><li>$s^0\prec r^1$</li><li>$s^1\prec r^2$</li><li>…</li><li>$s^{k-2}\prec r^{k-1}$</li><li>$s^{k-1}\prec r^0$</li><li>即在一个大小为k的Crown中，每个发送事件$s^i$都发生在下一个接收事件$r^{i+1}$之前，直到最后一个接收事件$r^0$</li></ul></li><li>在一个Crown中，$s^i$和$r^{i+1}$可能在同一个进程，也可能在不同进程</li><li>非CO执行必须有一个Crown</li><li>CO执行(非同步的)也会有一个Crown</li><li>Crown中的循环依赖关系意味着无法串行地调度消息，因此不是RSC</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Crown%20Test%20for%20RSC%20executions.png" alt="img"></p><h4 id="Crown准则"><a href="#Crown准则" class="headerlink" title="Crown准则"></a>Crown准则</h4><ul><li>一个执行是RSC的，即它可以实现在具有同步通信的系统上，当且仅当它不包含任何的Crown</li><li>复杂度<ul><li>$O(|E|)$，$E$为通信事件的数量</li></ul></li></ul><h4 id="RSC执行的时间戳"><a href="#RSC执行的时间戳" class="headerlink" title="RSC执行的时间戳"></a>RSC执行的时间戳</h4><ul><li>执行$(E, \prec)$是RSC的当且仅当存在从$E$到$T$(标量时间戳)的映射，使得<ul><li>对于任何消息$M$，$T(s(M))=T(r(M))$</li><li>对于每个$(a, b)\in (E\times E)/T$，$a\prec b \Rightarrow T(a)\lt T(b)$</li></ul></li></ul><h3 id="消息排序范式的层次结构"><a href="#消息排序范式的层次结构" class="headerlink" title="消息排序范式的层次结构"></a>消息排序范式的层次结构</h3><h4 id="RSC条件"><a href="#RSC条件" class="headerlink" title="RSC条件"></a>RSC条件</h4><ul><li>一个A-execution(这是异步执行)是RSC的当且仅当它是一个S-execution</li></ul><h4 id="层次结构"><a href="#层次结构" class="headerlink" title="层次结构"></a>层次结构</h4><ul><li>$RSC\subset CO\subset FIFO\subset A$<ul><li>更小的类对可能的消息排序有更多的限制</li><li>并发度最高的是A，最低的是SYNC</li><li>使用同步通信的程序最容易开发和验证</li><li>使用非FIFO通信的程序，导致A-execution，最难设计和验证</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Hierarchy%20of%20Message%20Ordering%20Paradigms.png" alt="img"></p><h3 id="异步原语在同步系统上模拟执行"><a href="#异步原语在同步系统上模拟执行" class="headerlink" title="异步原语在同步系统上模拟执行"></a>异步原语在同步系统上模拟执行</h3><ul><li><p>使发送者与接收者解耦，但这种实现成本较高。</p></li><li><p>按照消息在同步程序(S-program)中出现的顺序进行调度。</p><ul><li>如果同步程序中消息$m_1$先于消息$m_2$出现，则在异步系统上也应按此顺序调度这些消息。</li></ul></li><li>同步执行(S-execution)中的偏序关系保持不变。如果在同步执行中消息$m_1$必须先于消息$m_2$发送，则在异步系统上也必须保持</li><li>在异步系统上使用异步原语进行通信。发送和接收无需立即完成，可以在后台异步处理</li><li>当同步发送被调度时: 等待确认(ack)后再完成发送。发送方阻塞直到收到确认</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Async%20Programs%20on%20Sync%20Systems.png" alt="img"></p><h3 id="异步系统上模拟同步程序的顺序执行"><a href="#异步系统上模拟同步程序的顺序执行" class="headerlink" title="异步系统上模拟同步程序的顺序执行"></a>异步系统上模拟同步程序的顺序执行</h3><ul><li>确定性程序<ul><li>确定性接收: 重复运行产生相同的偏序关系。意味着确定性执行，即$(E, \prec)$是固定的</li></ul></li><li>非确定性(除了由于不可预测的消息延迟): <ul><li>接收调用不指定发送者。</li><li>多个发送和接收在一个进程中启用；可以以可交换的顺序执行。<ul><li>$G_1\rightarrow CL_1 || G_2\rightarrow CL_2 || \dots|| G_k\rightarrow CL_k$</li></ul></li></ul></li><li>如何在异步系统上调度非确定性的同步通信调用？<ul><li>用相应的事件匹配发送或接收</li></ul></li><li>二元会合(Binary rendezvous, 使用令牌实现)<ul><li>每个启动的交互有一个令牌</li><li>在线、原子地、分布式地进行调度</li><li>无Crown调度(安全性): 也保证进度</li><li>调度中的公平性和效率</li></ul></li></ul><h4 id="Bagrodia-的二元会合-Binary-Rendezvous-算法"><a href="#Bagrodia-的二元会合-Binary-Rendezvous-算法" class="headerlink" title="Bagrodia 的二元会合(Binary Rendezvous)算法"></a>Bagrodia 的二元会合(Binary Rendezvous)算法</h4><ul><li>假设: <ul><li>接收总是启用的，这意味着任何进程都可以随时接收消息</li><li>发送一旦启用，就保持启用状态。这意味着一旦进程开始发送消息，它将继续发送直到完成</li><li>为了打破死锁，使用进程标识符(PIDs)引入不对称性</li><li>每个进程一次只调度一个发送</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Bagrodia%20Algorithm%20for%20Binary%20Rendezvous%200.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Bagrodia%20Algorithm%20for%20Binary%20Rendezvous.png" alt="img"></p><ul><li>消息类型:<ul><li>$M$: 普通消息</li><li>$ack(M)$: 确认消息</li><li>$request(M)$: 请求发送消息</li><li>$permission(M)$: 响应发送请求</li></ul></li><li>进程在知道它可以成功同步当前消息时阻塞</li><li>防止消息循环死锁的措施:<ul><li>高优先级阻塞: 高优先级进程发送消息后，阻塞自身等待低优先级进程的确认。</li><li>低优先级非阻塞: 低优先级进程发送要发送消息的请求后，等待高优先级进程的许可。不需要阻塞等待<ul><li>高优先级进程返回许可后得阻塞直到收到低优先级进程发送的消息</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Bagrodia%20Algorithm%20for%20Binary%20Rendezvous%201.png" alt="img"></p><h2 id="组通信-Group-Communication"><a href="#组通信-Group-Communication" class="headerlink" title="组通信(Group Communication)"></a>组通信(Group Communication)</h2><h3 id="单播、多播和广播"><a href="#单播、多播和广播" class="headerlink" title="单播、多播和广播"></a>单播、多播和广播</h3><ul><li>单播: 消息从一个发送者发送给一个接收者。</li><li>多播: 消息从一个发送者发送给一组接收者。</li><li>广播: 消息从一个发送者发送给所有接收者。</li></ul><h3 id="网络层或硬件辅助多播难以提供的功能"><a href="#网络层或硬件辅助多播难以提供的功能" class="headerlink" title="网络层或硬件辅助多播难以提供的功能"></a>网络层或硬件辅助多播难以提供的功能</h3><ul><li>应用特定的消息传递顺序语义: 不同的应用程序可能需要不同的消息传递顺序</li><li>适应动态成员变化: 组成员可能会动态加入或离开</li><li>每次发送时将多播消息发送到任意进程集: 发送者可以选择不同的接收者集合</li><li>提供多种容错语义: 不同的容错策略可能适用于不同的应用场景</li></ul><h3 id="封闭组与开放组"><a href="#封闭组与开放组" class="headerlink" title="封闭组与开放组"></a>封闭组与开放组</h3><ul><li>封闭组: 组成员固定，不允许动态加入或离开</li><li>开放组: 组成员可以动态加入或离开</li></ul><h3 id="Raynal-Schiper-Toueg-RST-算法"><a href="#Raynal-Schiper-Toueg-RST-算法" class="headerlink" title="Raynal-Schiper-Toueg (RST) 算法"></a>Raynal-Schiper-Toueg (RST) 算法</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Raynal-Schiper-Toueg%20(RST" alt="img">%20Algorithm.png)</p><ul><li><p>假设</p><ul><li>FIFO通道</li><li>活跃性: 假设没有故障，传播时间是有限的</li></ul></li><li><p>数据结构</p><ul><li>SENT: 二维数组$SENT[1…n, 1…n]$，其中n是系统的进程数量。$SENT[i, j]$表示进程$P_i$发送给进程$P_j$的最后一条消息的序列号</li><li>DELIV: 一维数组，表示每个进程已经本地交付的消息数量</li></ul></li><li><p>消息格式</p><ul><li>每条消息不仅包含本身内容，还附带发送者的$SENT$数组副本，接收者可以了解到所有其他进程的最新消息状态</li></ul></li><li><p>发送事件: 当进程$P_i$想要向进程$P_j$发送消息时</p><ul><li>更新自身的$SENT$数组，增加$SENT[i, j]$</li><li>构建消息为$(M, SENT)$，其中$M$是实际消息内容，$SENT$是当前进程$P_i$的整个$SENT$数组</li></ul></li><li><p>接收事件: 当进程$P_i$收到一条来自进程$P_j$的消息$(M, ST)$</p><ul><li>首先检查是否可以立即交付这条消息，对于每一个进程$x$，如果$DELIV[x] &gt;= ST[x, i]$，那么就可以安全地交付消息$M$给进程$P_i$<ul><li>因为是异步的，所以可能后续先到达，如果没有因果序可以先交付，就会有这个情况。</li></ul></li><li>更新自己的$SENT$数组反映从$P_j$接收到的新信息，设置$max(SENT[x, y], ST[x, y])$</li><li>更新$DELIV[j]$，因为已经接收到并准备交付来自$P_j$的新消息</li></ul></li><li><p>复杂度</p><ul><li>空间复杂度<ul><li>每个进程: $O(n^2)$</li><li>每条消息: $O(n^2)$</li></ul></li><li>时间复杂度<ul><li>每次发送和接收事件: $O(n^2)$</li></ul></li></ul></li></ul><h3 id="最优的-KS-算法-Optimal-KS-Algorithm"><a href="#最优的-KS-算法-Optimal-KS-Algorithm" class="headerlink" title="最优的 KS 算法(Optimal KS Algorithm)"></a>最优的 KS 算法(Optimal KS Algorithm)</h3><ul><li>$M_{i, a}$: 进程$P_i$发送的第a个多播消息</li></ul><h4 id="正确性交付条件"><a href="#正确性交付条件" class="headerlink" title="正确性交付条件"></a>正确性交付条件</h4><ul><li>消息$M’$携带信息$d\in M.Dests$，其中消息$M$在$Send(M’)$的因果过去被发送到$d$，如果$M$尚未被交付给$d$，则$M’$也不会交付给$d$</li></ul><h4 id="优化的必要和充分条件"><a href="#优化的必要和充分条件" class="headerlink" title="优化的必要和充分条件"></a>优化的必要和充分条件</h4><ul><li>存储和捎带信息的事件<ul><li>考虑信息$d\in M_{i, a}.Dests$应该在进程的日志中存储多长时间，并且应该捎带在信息上</li><li>只要且只有当以下两个传播约束成立时，需要捎带:<ul><li>约束1: 向后不可达性，确保消息不会过早被交付。未知消息$M_{i, a}$是否已经被交付给$d$</li><li>约束2: 向前可达性，未知是否已经向$d$发送了在$Send(M_{i, a})$因果未来的消息，因此不能保证使用基于传递性的推理来确保消息$M_{i, a}$将被交付给$d$</li></ul></li><li>如果两个约束为假，则信息$d\in M.Dests$必须不被存储或传播</li></ul></li><li>关于消息的两个约束条件使用$(source, ts, dest)$显示跟踪<ul><li>一旦有约束变为假，立即删除</li><li>每个多播消息显式跟踪$(source, timestamp, destination)$信息，并存储在日志Log和$O_M$中。</li></ul></li><li>关于消息两个约束的隐式跟踪无需存储或传播<ul><li>可以从显示信息推导</li><li>用于确定何时两个约束的显示信息变为假，该消息被存储/捎带</li><li>类型1: 存在$d\in M_{i, a}.Dests$且$d\notin l_{i, a}.Dests\vee d\notin o_{i, a}.Dests$<ul><li>当$l_{i, a}.Dests=\empty$或$o_{i, a}.Dests = \empty$</li></ul></li><li>类型2: 如果$a_1\lt a_2$且$l_{i, a_2}\in LOG_j$，则$l_{i, a_1}\in LOG_j$<ul><li>形如$l_{i, a}.Dests=\empty$的条目可以通过缺失推断，不应存储</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Optimal%20KS%20Algorithm%20for%20CO.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Optimal%20KS%20Algorithm%20for%20CO%20Code1.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Optimal%20KS%20Algorithm%20for%20CO%20Code2.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Optimal%20KS%20Algorithm%20for%20CO%20Example.png" alt="img"></p><h2 id="Total-Order"><a href="#Total-Order" class="headerlink" title="Total Order"></a>Total Order</h2><h3 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h3><ul><li>对于每一对进程$P_i$和$P_j$，以及每一对消息$M_x$和$M_y$，如果两个消息都被两个进程接收，则<ul><li>$P_i$在交付$M_x$之前交付$M_y$，当且仅当$P_j$在交付$M_x$之前交付$M_y$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Group%20Communication.png" alt="img"></p><h3 id="集中式算法"><a href="#集中式算法" class="headerlink" title="集中式算法"></a>集中式算法</h3><ul><li>当$P_i$想要广播消息$M$到组$G$时<ul><li>发送$M(i, G)$到协调器</li></ul></li><li>当$M(i, G)$从$P_i$到达协调器<ul><li>发送$M(i, G)$到组G的各成员</li></ul></li><li>当$M(i, G)$从协调器到达$P_j$<ul><li>交付$M(i, G)$到应用</li></ul></li><li>复杂度<ul><li>时间复杂度: 每次传输要2次hop</li><li>消息数量复杂度: n</li></ul></li></ul><h3 id="分布式算法"><a href="#分布式算法" class="headerlink" title="分布式算法"></a>分布式算法</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Total%20Message%20Order%203-phase%20Algorithm%20Code.png" alt="img"></p><p> <img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Total%20Order%20Distributed%20Algorithm%20Example.png" alt="img"> </p><ul><li>复杂度<ul><li>三个阶段</li><li>n-1个目的地需要$3(n-1)$条消息</li><li>延迟: 3个消息跳数</li></ul></li><li>也能实现因果序</li></ul><h2 id="多播"><a href="#多播" class="headerlink" title="多播"></a>多播</h2><h3 id="目的地关系四种分类"><a href="#目的地关系四种分类" class="headerlink" title="目的地关系四种分类"></a>目的地关系四种分类</h3><ul><li>SSSG (Single Source Single Group): 单个源和单个目的地组，容易实现</li><li>MSSG (Multiple Sources Single Group): 多个源和单个目的地组，容易实现(集中式算法)</li><li>SSMG (Single Source Multiple Groups): 单个源和多个，可能重叠的目的地组，容易实现</li><li>MSMG (Multiple Sources Multiple Groups): 多个源和多个，可能重叠的目的地组，半集中式的传播树算法</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/A%20Nomenclature%20for%20Multicast.png" alt="img"></p><h3 id="用于多播的传播树-Propagation-Trees"><a href="#用于多播的传播树-Propagation-Trees" class="headerlink" title="用于多播的传播树(Propagation Trees)"></a>用于多播的传播树(Propagation Trees)</h3><h4 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h4><ul><li>组集: $G = \{G_1, …, G_g\}$</li><li>元组集: $MG = \{MG_1, \dots, MG_h\}$<ul><li>每个进程属于一个元组，并且在该元组中的每个其他进程具有完全相同的组成员资格</li><li>元组之外没有其他进程具有该确切的组成员资格</li></ul></li><li>转换<ul><li>多源多组(MSMG)到组 $\rightarrow$多源单组(MSSG)到元组</li></ul></li><li><p>管理节点</p><ul><li>每个元组中有一个区分的节点作为管理者</li></ul></li><li><p>传播森林/树</p><ul><li>所有元组组织成一个传播森林/树，满足以下条件<ul><li>对于用户组$G_i$, $PM(G_i)$位于树的最低可能就级别(离根最远)，使得所有目标包含$G_i$中任何节点的元组都属于以$PM(G_i)$为根的子树</li></ul></li></ul></li><li>传播树不是唯一的<ul><li>具有来自更多用户组成员的元组作为根节点，使得树高度较低</li></ul></li></ul><h4 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h4><ul><li><p>主元组($PM(G)$)</p><ul><li>对于每个用户组$G_i$，选择其中一个元组作为其主元组(PM)，记为$PM(G_i)$   </li><li>传播树中所有其他元组的祖先   </li><li>唯一定义的   </li><li>对于任何元组$MG$，从该元组所属的任何用户组的主元组到$MG$都有一条唯一的路径</li><li>任何$PM(G_1)$和$PM(G_2)$要么位于同一棵树的同一分支上，要么位于不同的树上。在后一种情况下，它们的组成员集合是不相交的。</li><li>多播消息首先发送到元组$PM(G_i)$，因为只有以$PM(G_i)$为根的子树可以包含$G_i$中的节点。随后消息沿着以$PM(G_i)$为根的子树向下传播</li></ul></li><li><p>元组之间的关系   </p><ul><li>$MG_1$包含$MG_2$，如果$MG_1$是每个用户组$G$的子集，而$MG_2$也是这些用户组的子集</li><li>$MG_1$和$MG_2$相交，如果两者都不包含对方，且存在某个组$G$，使得$MG_1, MG_2\subset G$</li></ul></li></ul><h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h5><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Propagation%20Trees%20for%20Multicast%20Example.png" alt="img"></p><h4 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Propagation%20Trees%20for%20Multicast%20(CO%20and%20TO" alt="img">%20Code.png)</p><ul><li>每个进程都知道传播树</li><li>每个元组都有一个用于区分的进程(manager)</li><li>$SV_i[k]$在每个$P_i$处: 由$P_i$多播的消息数量，这些消息通过$PM(G_k)$传播。附带在每个由$P_i$发送的多播消息上</li><li>$RV_{manager(PM(G_z))}[k]$: 由$P_k$发送并被$PM(G_z)$接收的消息的数量</li><li>$manager(PM(G_z))$: 如果$SV_i[z]==RV_{manager(PM(G_z))}[i]$，则处理来自$P_i$的消息$M$，否则缓冲消息$M$直到条件成立</li><li>在非主元组的管理者处: 消息顺序已经确定，因为它从未直接从多播发送者接收消息。转发(伪代码的2d-2g)。</li></ul><h3 id="Total-Order的正确性"><a href="#Total-Order的正确性" class="headerlink" title="Total Order的正确性"></a>Total Order的正确性</h3><ul><li>考虑$MG_1, MG_2\subset G_x, G_y$<ul><li>$PM(G_x), PM(G_y)$都包含$MG_1, MG_2$，并且位于传播树的同一个分支上，终点是$MG_1$或$MG_2$</li><li>由树下主元组(+FIFO)看到的顺序等于由被它包含的元组中的进程看到的顺序</li></ul></li></ul><h3 id="因果序的正确性-Causal-order"><a href="#因果序的正确性-Causal-order" class="headerlink" title="因果序的正确性(Causal order)"></a>因果序的正确性(Causal order)</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Propagation%20Trees%20for%20Multicast%20Correctness%20for%20CO.png" alt="img"></p><h3 id="应用级多播算法"><a href="#应用级多播算法" class="headerlink" title="应用级多播算法"></a>应用级多播算法</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Classification%20of%20Application-Level%20Multicast%20Algorithms.png" alt="img"></p><ul><li>基于通信历史的算法: RST, KS, Lamport, NewTop</li><li>基于特权的算法: Token-holder多播<ul><li>进程按 seq_no 的顺序传递消息</li><li>通常是封闭组，并且保证因果排序(CO)和总排序(TO)</li><li>示例：Totem, On-demand</li></ul></li><li>移动排序器算法: Change-Maxemchuck, Pinwheel<ul><li>排序器的令牌包含 seq_no 和已分配 seq_no 的消息列表(这些是发送的消息)</li><li>在接收令牌时，排序器为收到但未排序的消息分配 seq_nos，并将新排序的消息发送到目的地</li><li>目的地按 seq_no 的顺序传递消息</li></ul></li><li>固定排序器算法: 简化移动排序器的方法<ul><li>传播树、ISIS、Amoeba、Phoenix、Newtop-asymmetric</li></ul></li><li>目的地协议算法: 目的地接收有限的排序信息<ul><li>基于时间戳的(Lamport 的三阶段)</li><li>基于协议的，目的地之间达成一致</li></ul></li></ul><h3 id="容错多播的语义"><a href="#容错多播的语义" class="headerlink" title="容错多播的语义"></a>容错多播的语义</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><ul><li>多播是非原子的！</li><li>在故障期间有明确定义的行为 $\Rightarrow$ 明确定义的恢复动作<ul><li>(统一)规范：指定故障进程的行为(良性故障模型)</li></ul></li></ul><h4 id="统一可靠的多播"><a href="#统一可靠的多播" class="headerlink" title="统一可靠的多播"></a>统一可靠的多播</h4><ul><li>有效性: 如果一个正确进程多播了消息 $M$，那么所有正确进程最终都会交付$M$。</li><li>(统一)一致性: 如果一个正确(或故障)进程交付了消息$M$，那么所有正确进程最终都会交付$M$。</li><li>(统一)完整性: 每个正确(或故障)进程最多交付一次消息$M$，并且仅当$M$之前由发送者多播时。</li><li>(统一)FIFO顺序: 如果一个进程在广播$M$之前广播了$M’$，那么没有正确(或故障)进程会在未先交付$M$的情况下交付$M’$</li><li>(统一)因果顺序: 如果一个进程在因果上先广播$M$再广播了$M’$，那么没有正确(或故障)进程会在未先交付$M$的情况下交付$M’$</li><li>(统一)总顺序: 如果正确(或故障)进程a和b都交付了$M$和$M’$，那么a在交付$M$之前交付$M’$当且仅当b在交付$M$之前交付$M’$</li></ul><h4 id="基于全局时钟或本地时钟的规范-需要时钟同步"><a href="#基于全局时钟或本地时钟的规范-需要时钟同步" class="headerlink" title="基于全局时钟或本地时钟的规范(需要时钟同步)"></a>基于全局时钟或本地时钟的规范(需要时钟同步)</h4><ul><li>(统一)Real Time $\Delta$-Timeliness: 对于某个已知的常数$\Delta$，如果$M$在实际时间$t$被多播，那么没有正确(或故障)进程在实际时间$t+\Delta$之后交付$M$，要求在此之前交付</li><li>(统一)Local $\Delta$-Timeliness: 对于某个已知的常数$\Delta$，如果$M$在本地时间$t_m$被多播，那么没有正确(或故障)进程在其本地时间$t_m+\Delta$之后交付$M$，要求在此之前交付</li></ul><h3 id="反向路径转发-Reverse-Path-Forwarding-RPF"><a href="#反向路径转发-Reverse-Path-Forwarding-RPF" class="headerlink" title="反向路径转发(Reverse Path Forwarding, RPF)"></a>反向路径转发(Reverse Path Forwarding, RPF)</h3><ul><li>用于受限泛洪</li><li>网络层多播利用拓扑结构<ul><li>例如，桥接局域网(LANs)使用生成树学习目的地并分发信息</li><li>IP层RPF近似于DVR/LSR类似算法，但成本更低</li></ul></li><li>主要特点<ul><li>广播被限制以近似生成树</li><li>近似的根生成树被识别出来，而无需计算或存储</li><li>消息数量更接近$|N|$节点数，而不是$|L|$边数</li></ul></li><li>具体步骤<ul><li>当进程$P_i$想要向组Dest发送消息$M$<ul><li>在所有出站链路上发送$M(i, Dest)$</li></ul></li><li>当节点i从节点j接收到消息$M(x, Dest)$<ul><li>如果$Next_hop(x) = j$，则这将是一个新消息</li><li>向除(i, j)以外的所有其他入站链路转发$M(x, Dest)$</li><li>否则忽略该消息</li></ul></li></ul></li></ul><h3 id="Kou-Markowsky-Berman（KMB）启发式算法"><a href="#Kou-Markowsky-Berman（KMB）启发式算法" class="headerlink" title="Kou-Markowsky-Berman（KMB）启发式算法"></a>Kou-Markowsky-Berman（KMB）启发式算法</h3><ul><li>应用于Steiner树问题 </li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Kou-Markowsky-Berman%20Heuristic%20for%20Steiner%20Tree.png" alt="img"></p><h3 id="受约束的-延迟受限的-Steiner树-Constrained-Delay-bounded-Steiner-Trees"><a href="#受约束的-延迟受限的-Steiner树-Constrained-Delay-bounded-Steiner-Trees" class="headerlink" title="受约束的(延迟受限的)Steiner树(Constrained Delay-bounded Steiner Trees)"></a>受约束的(延迟受限的)Steiner树(Constrained Delay-bounded Steiner Trees)</h3><ul><li>对于给定的延迟容忍度$\Delta$，给定的源节点$s$和目标节点集$Dest$，其中$\{s\}\bigcup Dest = N’\subset N$，识别一个覆盖所有$N’$中节点的生成树$T$，满足以下约束条件: <ul><li>$\sum_{l\in T} C(l)$最小化，其中$C(l)$是边的成本</li><li>对于所有$v\in N’$，路径$path(s, v)$上的所有边l的延迟之和$\sum_{l\in path(s, v)}D(l)\lt \Delta$，其中$path(s, v)$表示从s到v在T中的路径<ul><li>路径的成本和延迟分别表示为$C(x, y), D(x, y)$</li></ul></li><li>受约束的最便宜路径是从x到y的路径，其延迟要小于$\Delta$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Constrained%20(Delay-Bounded" alt="img">%20Steiner%20Trees%20Algorithm.png)</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Constrained%20(Delay-Bounded" alt="img">%20Steiner%20Trees%20Example.png)</p><ul><li>启发式算法<ul><li>启发式$CST_{CD}$: 尝试选择低成本的边，同时尽量最大化剩余允许的延迟</li><li>启发式$CST_C$: 确保满足延迟界限的同时最小化成本</li></ul></li><li>时间复杂度<ul><li>找到所有节点上的受约束最短路径，$O(n^3\Delta)$(主导步骤)</li><li>在闭合图上构建受约束的最小生成树，该图有k个节点，$O(k^3)$</li><li>扩展受约束的生成树<ul><li>涉及将k条边扩展到最多n-1条边，并消除环路，$O(kn)$</li></ul></li></ul></li></ul><h3 id="基于核心的多播树"><a href="#基于核心的多播树" class="headerlink" title="基于核心的多播树"></a>基于核心的多播树</h3><ul><li>多播树动态构建，按需增长</li><li>每个组都有一个或多个核心节点</li><li>希望加入树作为接收者的节点<ul><li>发送单播加入消息到核心节点</li></ul></li><li>加入过程标记边<ul><li>加入消息在传输过程中标记边；它要么到达核心节点，要么到达已经属于树的一部分的某个节点</li><li>从加入消息开始到核心/多播树的路径被嫁接到多播树上</li></ul></li><li>树上的节点进行多播<ul><li>使用核心树上的泛洪来多播消息</li></ul></li><li>不在树上的节点<ul><li>向核心节点发送消息；一旦消息到达树上的任何节点，它就在树上泛洪</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;消息序和组通信&quot;&gt;&lt;a href=&quot;#消息序和组通信&quot; class=&quot;headerlink&quot; title=&quot;消息序和组通信&quot;&gt;&lt;/a&gt;消息序和组通信&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概</summary>
      
    
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>术语和基本算法</title>
    <link href="http://zjn-astonishe.github.io/2025/01/11/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2025-01-11-%E6%9C%AF%E8%AF%AD%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/"/>
    <id>http://zjn-astonishe.github.io/2025/01/11/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2025-01-11-%E6%9C%AF%E8%AF%AD%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/</id>
    <published>2025-01-11T11:42:55.000Z</published>
    <updated>2025-04-14T03:05:11.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="术语和基本算法"><a href="#术语和基本算法" class="headerlink" title="术语和基本算法"></a>术语和基本算法</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="拓扑-Topology"><a href="#拓扑-Topology" class="headerlink" title="拓扑(Topology)"></a>拓扑(Topology)</h3><ul><li>System: 无向(加权)图<ul><li>(N, L): N为图中的点，L为边</li></ul></li><li>物理拓扑Physical topology<ul><li>节点(Nodes): 网络节点，路由器，所有的终端主机(无论是否参与)</li><li>边(Edges): 所有的LAN，WAN的链接，在终端主机之间的直接边</li></ul></li><li>逻辑拓扑(Logical topology, 从应用内容判断)<ul><li>节点(Nodes): 应用所在运行的终端主机</li><li>边(Edges): 节点之间的逻辑信道</li><li>逻辑拓扑的叠加(Superimposed topology or topology overlay)<ul><li>目标是更有效地进行信息的聚集、分发和查找(P2P)</li><li>环、树、网格(mesh)、超立方体(hypercube)</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%92%8C%E9%82%BB%E5%9F%9F%E8%A7%86%E5%9B%BE.png" alt="img"></p><ul><li>图a为邻域视图(neighborhood view)<ul><li>子图(subgraph)，部分系统视图</li><li>需要多跳路径，易于维护(因为不是全连接，所以稳定性更好，出故障不会瘫痪一片)</li></ul></li><li>图b为全连接(All-to-all fully connected)</li></ul><h3 id="应用执行和控制算法执"><a href="#应用执行和控制算法执" class="headerlink" title="应用执行和控制算法执"></a>应用执行和控制算法执</h3><ul><li>两者都有各自自己的事件(event)</li><li>控制算法:<ul><li>用于监控和辅助的功能，例如创建ST、MIS、CDS、达成共识、全局状态检测(是否发生死锁、终止)，检查点(checkpoint)</li><li>叠加在应用执行上，但不会干扰应用的执行</li><li>发送、接收和内部事件都是对应用程序的执行是透明的</li><li>需要特殊的协议</li></ul></li></ul><h3 id="集中式和分布式算法"><a href="#集中式和分布式算法" class="headerlink" title="集中式和分布式算法"></a>集中式和分布式算法</h3><ul><li><p>集中式算法：</p><ul><li>其中的角色是不对称的，客户端-服务端式配置</li><li>瓶颈是传输带宽和服务端的处理能力</li><li>会出现点失效的问题，即服务端出问题，所有客户端都不可用</li></ul></li><li><p>分布式算法</p><ul><li>节点的角色更加平衡</li><li>难以设计完美的分布式算法(例如快照算法、基于树的算法(tree-based algorithms))</li></ul></li></ul><h3 id="对称-symmetric-和非对称-asymmetric-算法"><a href="#对称-symmetric-和非对称-asymmetric-算法" class="headerlink" title="对称(symmetric)和非对称(asymmetric)算法"></a>对称(symmetric)和非对称(asymmetric)算法</h3><h3 id="匿名算法"><a href="#匿名算法" class="headerlink" title="匿名算法"></a>匿名算法</h3><ul><li>进程的id或处理器的id不用于在运行时做出任何执行决策<ul><li>结构很优雅但是很难设计，几乎是不可能的(无法匿名选举通信的领导人)</li></ul></li></ul><h3 id="统一算法"><a href="#统一算法" class="headerlink" title="统一算法"></a>统一算法</h3><ul><li>不能在代码中使用进程数n作为参数</li><li>匀速可扩展性，使得进程离开或加入变得更加容易，只需要让邻居知道逻辑拓扑的变化即可</li></ul><h3 id="自适应算法"><a href="#自适应算法" class="headerlink" title="自适应算法"></a>自适应算法</h3><ul><li>让k($k&lt;n$)作为执行X时参与问题X上下文的进程数量，计算复杂度的时候应该表示为k的函数而不是n的函数</li><li>例如互斥，对关键段争用的开销可以表示为此时争用段的进程的数量(k)</li></ul><h3 id="非确定性接收和确定性接收"><a href="#非确定性接收和确定性接收" class="headerlink" title="非确定性接收和确定性接收"></a>非确定性接收和确定性接收</h3><ul><li>非确定行接收: 可以接收来自任何来源的消息</li><li>确定性接收: 只能接收固定来源的消息</li></ul><h3 id="确定性和非确定性的执行"><a href="#确定性和非确定性的执行" class="headerlink" title="确定性和非确定性的执行"></a>确定性和非确定性的执行</h3><ul><li>确定性执行<ul><li>没有非确定性的接收</li><li>异步系统重新执行确定性程序将对事件产生相同的偏序(用于调试、不稳定谓词检测)</li></ul></li><li>非确定性执行<ul><li>至少包含1个非确定性的接收</li><li>异步系统重新执行非确定性程序可能会产生不同的偏序(无限的交付事件和不可预测的拥塞，可变的本地CPU调度延迟等)</li></ul></li></ul><h3 id="执行抑制-Execution-inhibition-freezing"><a href="#执行抑制-Execution-inhibition-freezing" class="headerlink" title="执行抑制(Execution inhibition/freezing)"></a>执行抑制(Execution inhibition/freezing)</h3><ul><li>要求在某些规定的操作发生之前暂停正常执行的协议都是抑制性的<ul><li>非抑制性协议是指在任何执行中都不禁用任何事件</li><li>局部抑制性协议是指在任何执行中，任何延迟事件都是本地延迟事件，即本地控制下的抑制，不依赖于任何接收事件</li><li>全局抑制性协议是指在某些执行中，某些延迟事件不会在本地延迟</li></ul></li><li>但和阻塞非阻塞原语不同</li><li>抑制性的其他分类<ul><li>发送抑制(send inhibitory)</li><li>接收抑制(receive inhibitory)</li><li>内部事件抑制(internal event inhibitory)</li></ul></li></ul><h3 id="同步和异步系统"><a href="#同步和异步系统" class="headerlink" title="同步和异步系统"></a>同步和异步系统</h3><ul><li>同步系统<ul><li>有消息延迟上限</li><li>已知时钟相对于真实事件的漂移率有界</li><li>已知进程执行的逻辑步骤的上限</li></ul></li><li>异步系统<ul><li>同步系统的3个条件都不满足</li><li>分布式系统本质上是异步系统，同步系统是特殊的异步系统</li></ul></li></ul><h3 id="On-line算法和off-line算法"><a href="#On-line算法和off-line算法" class="headerlink" title="On-line算法和off-line算法"></a>On-line算法和off-line算法</h3><ul><li><p>On-line:</p><ul><li>‌不需要预先知道全部输入信息，可以逐步接收和处理每个数据元素。它适用于需要实时处理和决策的场景，如股市交易、网络路由等‌，对于调试和调度有优势</li></ul></li><li><p>Off-line:</p><ul><li>需要预先知道所有的输入信息，然后进行全面分析和处理。它适用于可以预先收集所有数据的情况，如大规模数据集上的机器学习训练‌</li></ul></li></ul><h3 id="无等待-Wait-free-算法"><a href="#无等待-Wait-free-算法" class="headerlink" title="无等待(Wait-free)算法"></a>无等待(Wait-free)算法</h3><ul><li>这是用于同步操作</li><li>对$n-1$个进程故障具有弹性，即任何进程的操作都必须在有限的步骤内完成，而与其他进程无关<ul><li>无等待是一种比无锁更强的条件，无等待算法要求在无锁算法的定义基础上，增加一个条件：所有上下文的执行都必须在有限的步骤内可以完成，而不依赖于其他上下文的状态。</li></ul></li><li>非常稳定，但是成本很高</li><li>可能用来设计互斥操作</li><li>可能并不总是能够设计出来的(例如生产者消费者问题)</li></ul><h3 id="通信信道-Communication-channels"><a href="#通信信道-Communication-channels" class="headerlink" title="通信信道(Communication channels)"></a>通信信道(Communication channels)</h3><ul><li>点到点: FIFO、non-FIFO<ul><li>在应用层FIFO通常是由网络栈(network stack)提供</li></ul></li></ul><h3 id="进程故障-严重程度依次增加"><a href="#进程故障-严重程度依次增加" class="headerlink" title="进程故障(严重程度依次增加)"></a>进程故障(严重程度依次增加)</h3><ul><li>故障停止(Fail-stop): 正常运行的进程停止执行，其他进程了解到失败的进程(通过某种机制) </li><li>Crash: 正常运行的进程停止执行，其他进程无法了解哪个是失败的进程</li><li>Receive omission(接收遗漏): 正常运行的进程会因为接收到已经发送过给它的一些消息而失败或crash。</li><li>Send omission(发送遗漏): 正常运行的进程会因为发送了已经发送过的消息而失败或crash，不过和接收遗漏模型不同</li><li>General omission(普遍遗漏): 接收遗漏+发送遗漏</li><li>Byzantine(or malicious) failiure(拜占庭式(恶意)失败): 即进程可能做任何行为，甚至是错误的行为，包括发生虚假信息 <ul><li>with authentication(带有验证): 如果一个有缺陷的线程声称收到了来自正确线程的消息，那么这是可以验证的。因为可以往正确线程查证</li><li>without authentication(没有验证)</li><li>注意: 非恶意故障模型(non-malicious failure models)是良性的</li></ul></li></ul><h4 id="进程故障的条件"><a href="#进程故障的条件" class="headerlink" title="进程故障的条件"></a>进程故障的条件</h4><ul><li>时间错误(对于同步系统)<ul><li>普遍遗漏故障(General omission)</li><li>违反指定漂移率的时钟(clocks violating specified drift rates)</li><li>违反执行步骤时间限制(process violating bounds on time to execute a step)</li></ul></li><li>链接错误<ul><li>Crash故障: 正常运行的链接停止传输消息</li><li>Omission故障: 链接只携带其中发送的一些消息，不携带其他消息</li><li>拜占庭式(恶意)故障: 链接表现出任意行为，包括创建虚假消息和篡改在链接上发送的消息</li></ul></li><li>链接错误导致的时间错误(对于同步系统):<ul><li>消息传递速度比指定的行为更快或更慢</li></ul></li></ul><h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><h3 id="表示方法"><a href="#表示方法" class="headerlink" title="表示方法"></a>表示方法</h3><ul><li>下界(lower bound, $\Omega$)</li><li>上界(upper bound, $O$)</li><li>上确界(exact bound, $\theta$)</li></ul><h3 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h3><ul><li>每个节点的空间复杂度</li><li>全系统的空间复杂度(不等价于系统中所有节点的空间复杂度之和)，因为最坏的情况通常不会同时发生在所有节点</li></ul><h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><ul><li>每个节点的时间复杂度</li><li>全系统的时间复杂度，取决于节点是否完全并发执行</li></ul><h3 id="消息复杂度-Message-complexity"><a href="#消息复杂度-Message-complexity" class="headerlink" title="消息复杂度(Message complexity)"></a>消息复杂度(Message complexity)</h3><ul><li>消息的数量: 影响消息负载的空间复杂度</li><li>消息的大小: 影响消息负载的空间复杂度和通过增加传输时间影响时间复杂度的分量</li><li>消息时间复杂度: 取决于消息的数量、大小和发送接收消息的并发性</li></ul><h3 id="其他指标"><a href="#其他指标" class="headerlink" title="其他指标"></a>其他指标</h3><ul><li>发送和接收事件(send and receive events)</li><li>广播(multicasts)</li><li>共享存储系统(Shared memory systems): 共享存储的大小，通过原子操作实现同步</li></ul><h2 id="程序结构-Program-Structure"><a href="#程序结构-Program-Structure" class="headerlink" title="程序结构(Program Structure)"></a>程序结构(Program Structure)</h2><ul><li>通信顺序进程(Communicating Sequential Process, CSP)<ul><li>$*[G_1\leftarrow CL_1 || G_2\leftarrow CL_2|| … || G_k\leftarrow CL_k]$<ul><li>重复命令”*”表示无限循环</li><li>选择命令”||”是在受保护命令上进行的。指定执行其组成受保护命令中的一个。</li><li>受保护(guarded)命令的语法: $G\leftarrow CL$<ul><li>Guard $G$是布尔表达式</li><li>$CL$是如果$G$为真时要执行的命令的列表</li><li>Guard(门)检查来自另一进程到达的消息</li><li>如果所有的guard都是false，则选择命令失败</li><li>如果多个guard都是true，则非确定性地选择一个执行(随机？)<ul><li>$G_m\leftarrow CL_m$, $CL_m, G_m$是原子化操作</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h2><h3 id="Sync-1-initiator-ST-flooding"><a href="#Sync-1-initiator-ST-flooding" class="headerlink" title="Sync 1-initiator ST (flooding)"></a>Sync 1-initiator ST (flooding)</h3><ul><li>ST: Spanning tree<ul><li>生成树是没有循环并且连通的。对于任何连通图，至少存在一棵生成树。如果图中有 n 个顶点，则生成树将有 n-1 条边。最坏情况高度为n-1</li></ul></li><li>1个启动器的生成树同步算法(泛洪)</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Sync%201-initiator%20ST%20(flooding" alt="img">.png)</p><ul><li>流程<ul><li>随机选择根节点作为启动器</li><li>每个节点标识父节点(随机挑选)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Sync%201-initiator%20ST%20(flooding" alt="img">2.png)</p><ul><li>终止条件: 迭代轮次到达直径(即节点间的最长距离)</li><li>复杂度<ul><li>本地<ul><li>空间复杂度: $O(degree)$</li><li>时间复杂度: $O(degree+diameter)$</li></ul></li><li>全局<ul><li>空间复杂度: $O(\sum local_ space)$</li></ul></li><li>消息<ul><li>时间复杂度: 直径或者消息的跳数</li><li>数量复杂度: 每条边的消息数量$\geq 1, \leq 2$。$[l, 2l]$</li></ul></li><li>类似于广度优先搜索</li></ul></li></ul><h3 id="Asynchronous-1-init-Spanning-Tree"><a href="#Asynchronous-1-init-Spanning-Tree" class="headerlink" title="Asynchronous 1-init Spanning Tree"></a>Asynchronous 1-init Spanning Tree</h3><ul><li>1个启动器的生成树异步算法</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%201-init%20Spanning%20Tree.png" alt="img"></p><ul><li>流程<ul><li>根节点启动泛洪<code>QUERY</code>识别生成树的边缘</li><li>父节点: 接收到的第一个<code>QUERY</code>对应的节点，先到先得<ul><li>发送<code>ACCEPT(+rsp)</code>给挑选的父节点作为回应</li><li>发送<code>REJECT(-rsp)</code>给非父节点的<code>QUERY</code></li><li>继续发送<code>QUERY</code>给其他的邻居节点</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%201-init%20Spanning%20Tree1.png" alt="img"></p><ul><li>本地终止条件: 当接收到所有来自邻居非父节点的<code>ACCEPT</code>或者<code>REJECT</code>时</li><li>复杂度<ul><li>本地<ul><li>空间复杂度: $O(degree)$</li><li>时间复杂度: $O(degree)$</li></ul></li><li>全局<ul><li>空间复杂度: $O(\sum local_space)$</li></ul></li><li>消息<ul><li>时间复杂度: 直径+1的跳数(因为有个最终回复)</li><li>数量复杂度: 每条边的消息数量$\geq 2, \leq 4$，$[2l, 4l]$</li></ul></li></ul></li></ul><h3 id="Asynchronous-Spanning-Tree-Concurrent-Initiators"><a href="#Asynchronous-Spanning-Tree-Concurrent-Initiators" class="headerlink" title="Asynchronous Spanning Tree: Concurrent Initiators"></a>Asynchronous Spanning Tree: Concurrent Initiators</h3><ul><li><p>多个并发启动器的生成树异步算法</p></li><li><p>没有预先设计规定的根节点</p><ul><li>可选1: 合并部分生成树，但是很难只根据本地知识完成，会导致循环</li><li>可选2: 只允许一个生成树继续进行计算，停止其他的生成树(算法使用该思路，选择具有更大进程id的节点继续)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%20Spanning%20Tree%20Concurrent%20Initiators1.png" alt="img"></p><ul><li><p>流程</p><ul><li>节点可以自发启动算法并成为根(启动器)</li><li>每个根启动1个启动器算法生成树异步算法的变体，在中间节点抑制较低优先级的根。<ul><li>当<code>QUERY</code>、<code>ACCEPT</code>和<code>REJECT</code>到达节点时，行动取决于当前节点的根节点和新节点的优先级。</li></ul></li></ul></li><li><p>终止条件: 只有根节点探测到终止才能终止，要求发送额外的消息通知其他节点终止。</p></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%20Spanning%20Tree%20Concurrent%20Initiators.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%20Spanning%20Tree%20Concurrent%20Initiators0.png" alt="img"></p><ul><li>复杂度<ul><li>时间复杂度: $O(l)$</li><li>消息数量复杂度: $O(nl)$</li></ul></li></ul><h3 id="Asynchronous-DFS-Spanning-Tree"><a href="#Asynchronous-DFS-Spanning-Tree" class="headerlink" title="Asynchronous DFS Spanning Tree"></a>Asynchronous DFS Spanning Tree</h3><ul><li>和非DFS算法一样处理并发启动器<ul><li>当<code>QUERY</code>、<code>ACCEPT</code>和<code>REJECT</code>到达节点时，行动取决于当前节点的根节点和新节点的优先级。</li></ul></li><li>终止条件: 只有当根节点检测到终止时，用生成树的路径通知其他节点终止。</li><li>时间复杂度: $O(l)$</li><li>消息数量复杂度: $O(nl)$</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Asynchronous%20DFS%20Spanning%20Tree.png" alt="img"></p><h3 id="Broadcast-and-Convergecast-on-a-Tree"><a href="#Broadcast-and-Convergecast-on-a-Tree" class="headerlink" title="Broadcast and Convergecast on a Tree"></a>Broadcast and Convergecast on a Tree</h3><ul><li>生成树上消息的广播和聚合算法</li><li>广播: 分发消息<ul><li>根节点向所有子节点发送要广播的消息，根节点终止。</li><li>当一个非根节点从其父节点接收到消息时，将其复制并转发给其他子节点，该非根节点终止</li></ul></li><li>聚合: 在根节点收集消息，以计算全局函数<ul><li>用途：计算最小值、最大值、领导者选举、计算全局状态函数</li><li>叶子节点将消息发送给父节点，叶子节点终止</li><li>非叶节点接收到来自所有子节点的消息后，将收集的消息发送给父节点，该非叶子节点终止</li><li>根节点接收到所有子节点的消息后，全局函数完成计算后，根节点终止</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Broadcast%20and%20Convergecast%20on%20a%20Tree.png" alt="img"></p><ul><li>时间复杂度: $O(h)$</li><li>消息数量复杂度: $n-1$</li></ul><h3 id="Single-Source-Shortest-Path-Sync-Bellman-Ford"><a href="#Single-Source-Shortest-Path-Sync-Bellman-Ford" class="headerlink" title="Single Source Shortest Path: Sync Bellman-Ford"></a>Single Source Shortest Path: Sync Bellman-Ford</h3><ul><li>单源最短路径的同步算法<ul><li>整体为加权图，没有负权重的环</li><li>没有节点具有全局视图，仅拥有本地拓扑</li></ul></li><li>流程<ul><li>假设节点知道n轮迭代后，算法终止<ul><li>经过k轮之后，对于任意给定的节点，它应该已经知道所有可以通过最多k跳到达的其他节点的最短路径长度。</li></ul></li></ul></li><li>终止: 第n轮迭代</li><li>时间复杂度: $n-1$</li><li>消息数量复杂度: $(n-1)l$</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Single%20Source%20Shortest%20Path%20Sync%20Bellman-Ford.png" alt="img"> </p><h3 id="Distance-Vector-Routing"><a href="#Distance-Vector-Routing" class="headerlink" title="Distance Vector Routing"></a>Distance Vector Routing</h3><ul><li>距离向量路由算法<ul><li>用于互联网路由(流行到20世纪80年代中期)，具有动态变化的图，其中链接权重模拟延迟/负载</li><li>Sync Bellman-Ford的变体: 外层循环改为无限，就是说没有终止</li><li>追踪通往每个目的地的最短路径</li><li>长度替换为$LENGTH[1..n]$；父节点替换为$PARENT[1..n]$</li><li>第k个分量表示从某个源节点(通常是执行算法的那个节点)到网络中第k个节点的当前最佳已知距离，$LENGTH[k]$</li></ul></li><li>每次迭代的流程<ul><li>对每个目的地独立应用三角不等式<ul><li>$LENGTH[k]\gt (LENGTH_j[k] + weight_{j, i})$</li><li>节点i用RTT或者到邻居节点j的排队延迟评估$weight_{i, j}$</li></ul></li></ul></li></ul><h3 id="Single-Source-Shortest-Path-Async-Bellman-Ford"><a href="#Single-Source-Shortest-Path-Async-Bellman-Ford" class="headerlink" title="Single Source Shortest Path: Async Bellman-Ford"></a>Single Source Shortest Path: Async Bellman-Ford</h3><ul><li>单源最短路径的异步算法<ul><li>整体为加权图，没有负权重的环   </li><li>没有节点具有全局视图，仅拥有本地拓扑</li></ul></li><li>复杂度(c是常数)<ul><li>时间复杂度: 最坏情况$\Omega(c^n\cdot d)$</li><li>消息数量复杂度: 最坏情况有$\Omega(C^n)$条消息。<ul><li>如果所有的边有相同的权重，算法计算最小跳数，需要用$O(n^2\cdot l)$</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Single%20Source%20Shortest%20Path%20Async%20Bellman-Ford.png" alt="img"></p><h3 id="All-All-Shortest-Paths-Floyd-Warshall"><a href="#All-All-Shortest-Paths-Floyd-Warshall" class="headerlink" title="All-All Shortest Paths: Floyd-Warshall"></a>All-All Shortest Paths: Floyd-Warshall</h3><ul><li>所有节点的最短路径<ul><li>就是为每个节点跑一次最短路径，加一个外循环</li></ul></li><li>复杂度<ul><li>时间复杂度: $O(n^3)$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/All-All%20Shortest%20Paths%20Floyd-Warshall1.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/All-All%20Shortest%20Paths%20Floyd-Warshal.png" alt="img"></p><h3 id="Distributed-Floyd-Warshall"><a href="#Distributed-Floyd-Warshall" class="headerlink" title="Distributed Floyd-Warshall"></a>Distributed Floyd-Warshall</h3><ul><li>$LENGTH[1…n, 1…n], VIA[1…n, 1…n]$的第i行存储在第i个节点，负责更新第i行，i作为源节点</li><li>对应于上个集中式算法的第4行<ul><li>节点i如何在每次迭代中访问远程数据$LENGTH[pivot, t]?$<ul><li>分布式的动态汇点树: 在每次迭代中选择一个枢轴节点(pivot)，所有与枢轴节点有非无穷大路径的节点都会形成一个汇点树，其中枢轴节点是汇点。可以确保节点能够高效访问所需的远程数据。</li></ul></li><li>如何在不同节点之间同步外循环的迭代执行(否则算法出错)<ul><li>模拟”同步器”的操作: 使用接收操作确保节点之间的同步(节点可以通过接收来自其父节点的数据$LENGTH[pivot, *]$来确保同步执行)</li></ul></li></ul></li><li>重命名$LENGTH[i, j], VIA[i, j]$为$LEN[j], PARENT[j]$<ul><li>例如$LENGTH[i, pivot]$为$LEN[pivot]$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Distributed%20Floyd-Warshall1.png" alt="img"></p><ul><li>流程<ul><li>在任何节点i中，迭代pivot:<ul><li>如果$LEN[pivot]\neq \infin$，则枢轴节点将$LEN[*]$分发给所有在枢轴节点的汇点树中的节点，包括节点i</li><li>汇点树中的父子边需要被标识<ul><li>节点向$PARENT[pivot]$发送<code>IN_TREE</code>，向其他邻居节点发送<code>NOT_IN_TREE</code></li><li>接收到某个节点的<code>IN_TREE</code>回复，说明该节点是pivot在汇点树中的子节点</li></ul></li><li>等待每个邻居的<code>IN_TREE</code>或<code>`NOT_IN_TREE</code>，发送和接收是同步的</li><li>枢轴节点向下广播$LEN[*]$到汇点树中，发送和接收也是同步的</li><li>所有节点以伪锁定步骤执行三角不等式</li></ul></li><li>复杂度<ul><li>时间复杂度: 每个节点的执行时间复杂度$O(n^2)$再加上n次广播的时间</li><li>消息数量复杂度: n次迭代<ul><li>每次迭代:<ul><li>每条边发送2条<code>IN_TREE</code>或<code>NOT_IN_TREE</code>消息，每条消息大小为$O(1)$，总$O(l)$</li><li>最多n-1条<code>PIV_LEN</code>消息，每条消息大小为$O(n)$(n个节点的长度信息)，总共$O(n)$</li></ul></li><li>总共$O(n(l+n))$条消息，占用$O(nl+n^3)$的消息空间</li></ul></li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Distributed%20Floyd-Warshall.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Distributed%20Floyd-Warshall0.png" alt="img"></p><h3 id="Constrained-flooding-no-ST"><a href="#Constrained-flooding-no-ST" class="headerlink" title="Constrained flooding(no ST)"></a>Constrained flooding(no ST)</h3><ul><li>约束泛洪算法(无生成树)</li><li>使用FIFO通道，并通过序列号检测重复的消息</li></ul><h4 id="Async-constrained-flooding-no-ST"><a href="#Async-constrained-flooding-no-ST" class="headerlink" title="Async, constrained flooding(no ST)"></a>Async, constrained flooding(no ST)</h4><ul><li>异步约束泛洪算法(无生成树)<ul><li>被IPV4中的链路状态路由(Link State Routing)使用</li><li>复杂度<ul><li>时间复杂度: 直径d个连续跳数</li><li>消息数量复杂度: 最坏情况$2l$条消息</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Async%20constrained%20flooding.png" alt="img"></p><h4 id="Sync-constrained-flooding-no-ST"><a href="#Sync-constrained-flooding-no-ST" class="headerlink" title="Sync, constrained flooding(no ST)"></a>Sync, constrained flooding(no ST)</h4><ul><li>同步约束泛洪算法(无生成树)<ul><li>$STATEVEC[k]$是对k的数据的统计</li></ul></li><li>复杂度: <ul><li>时间复杂度: 直径d轮迭代</li><li>消息数量复杂度: 发送$2ld$条消息，每条消息大小为$n$(每轮$2l$，d轮)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Sync%20constrained%20flooding.png" alt="img"></p><h3 id="MST-sync"><a href="#MST-sync" class="headerlink" title="MST, sync"></a>MST, sync</h3><ul><li>最小生成树同步算法</li><li>假设无向加权图，如果权重不是唯一的，假设使用某种打破平局的方法，如节点ID，来对边的权重施加全序。</li><li>Kruskal 的 MST 算法:<ul><li>假设图的组件森林</li><li>维护排序的边列表</li><li>在每次迭代中，识别连接两个不同组件的最小权重边</li><li>将该边包含在 MST 中</li><li>时间复杂度：$O(l*logl)$</li></ul></li><li>Prim的 MST算法<ul><li>从一个单节点组件开始</li><li>在每次迭代中，选择与当前组件相邻的最小权重边。组件通过这条选定的边扩展。</li><li>时间复杂度：O(n^2) (或$O(n*logn)$使用 Fibonacci 堆在稠密图中) </li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Minimum%20Weight%20Outgoing%20Edge.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/MST%20Example.png" alt="img"></p><ul><li><p>Gallagher-Humblet-Spira 分布式 MST </p><ul><li>使用 Kruskal 的策略，从图的组件森林开始。</li><li>MWOE(minimum weight outgoing edge): <ul><li>最小权重的出边: D”outgoing” 是逻辑上的，表示组件扩展的方向。<ul><li>对于图$G$的任何生成森林$\{(N_i, L_i)|i=1, …, k\}$，考虑其中的任意一个组件$(N_j, L_j)$。记$\lambda_j$为那些仅与$N_j$中的一个节点相邻的所有边中权重最小的边。包含生成森林中所有边$L_i$的$G$的最小生成树，也必须包含边$\lambda_j$</li></ul></li><li>连通组件的生成树与 MWOEs 结合，以在合并后的组件中仍然保留生成树属性。</li><li>并发地合并 MWOEs：最多有$\frac{n}{2^k}$个组件，因此最多需要$log n$次迭代</li><li>每个组件在每次迭代中都有一个领导者节点。</li><li>每个组件内的每次迭代包含 5 个步骤，由领导者触发：<ul><li>广播-收敛阶段：领导者识别 MWOE。</li><li>广播阶段：确定下一次迭代的潜在领导者。</li><li>广播阶段：在合并组件中选择一个领导者；它向新组件中的所有节点标识自己。</li></ul></li></ul></li></ul></li><li><p>流程</p><ul><li>根节点广播<code>SEARCH_MWOE</code>以查找最小权重出边（MWOE）。</li><li>收敛广播<code>REPLY_MWOE</code>: 邻居节点收到 SEARCH_MWOE 后，将回复 REPLY_MWOE 消息，报告它们找到的 MWOE。</li><li>根节点广播<code>ADD_MWOE</code>: 根节点收集所有 REPLY_MWOE 消息后，选择一个 MWOE 并广播 ADD_MWOE 消息，指示其他节点添加该边到生成树中。</li><li>如果 MWOE 也被 MWOE 的另一端组件选为 MWOE，则具有较高 ID 的事件过程是下一次迭代的领导者，并广播 <code>NEW_LEADER</code>。</li></ul></li><li><p>复杂度</p><ul><li>时间复杂度: $O(n*logn)$($logn$次迭代，因为并发地合并)</li><li>消息复杂度: <ul><li>每轮迭代，每条树的边$O(n)$条消息</li><li>每轮迭代，1条<code>EXAMINE</code>消息用来确定MWOEs</li><li>因此为$O((n+l)logn)$条消息</li></ul></li><li>正确性要求同步操作<ul><li>在步骤 (2) 中，使用 <code>EXAMINE</code> 来确定未标记的邻居是否属于同一组件。如果未标记边的节点处于不同的级别，则会出现问题！</li><li>考虑在边$(j, k)$上发送<code>EXAMINE</code>，该边实际属于同一组件。但是k可能还没有发现它属于新组件和新的领导者ID，并且回复为<code>+ve</code>，说两者属于不同组件，于是j又继续发<code>EXAMINE</code>给k。</li><li>这可能导致循环。</li></ul></li></ul></li><li>需要使用额外的消息/步骤<ul><li>新的领导者在新的组件的标记边执行广播和收敛广播<ul><li>步骤2中，如果接收者还处于旧的迭代轮次，则可能延迟对<code>EXAMINE</code>消息的响应</li><li>总共需要$n*logn$条额外消息</li></ul></li><li>在参与新的迭代轮次时，需要通知每个邻居节点<ul><li>当所有未标记边上的邻居在同一轮次时发送<code>EXAMINE</code>消息。</li><li>总共需要$l*logn$条额外消息</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Sync%20GHS%20Message%20Types.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Sync%20GHS%20Code.png" alt="img"></p><h3 id="MST-async"><a href="#MST-async" class="headerlink" title="MST, async"></a>MST, async</h3><ul><li>最小生成树异步算法</li><li>复杂度<ul><li>时间复杂度: $O(n*logn(l+d))$</li><li>消息复杂度: $O(n*logn + l)$</li></ul></li><li>挑战<ul><li>确定相邻节点的级别</li><li>与单个组件反复合并导致$logn$变成$n$</li><li>如果组件处于不同级别，需要协调搜索最小权重出边MWOEs和合并</li></ul></li></ul><h3 id="Synchronizers-simple-alpha-beta-gamma"><a href="#Synchronizers-simple-alpha-beta-gamma" class="headerlink" title="Synchronizers: simple, $\alpha$, $\beta$, $\gamma$"></a>Synchronizers: simple, $\alpha$, $\beta$, $\gamma$</h3><ul><li><p>同步器</p><ul><li>一类转换算法，允许为同步系统设计的同步程序在异步系统运行</li><li>假设系统无故障，从头开始设计定制的异步算法可能比使用同步器更高效</li></ul></li><li><p>进程安全性</p><ul><li>如果进程i发送的所有消息都已被接收，则进程i在第r轮迭代是安全的，</li><li>实现的关键点是向每个进程发出信号，告知何时可以进入下一轮(即当所有要接收消息都已到达时，可以进入下一轮)</li></ul></li><li>公式<ul><li>$M_a=M_s + M_{init} + rounds * M_{round})$(所有的消息)<ul><li>$M_s$: 同步算法中的消息数量</li><li>$M_{init}$: 初始化异步系统所需的顺序消息数量</li><li>$rounds$: 同步算法的迭代轮数</li><li>$M_{round}$: 一次迭代中需要执行的消息数量</li></ul></li><li>$T_a = T_S + T_{init} + rounds * T_{round}$(所有的时间)<ul><li>$T_s$: 同步算法的时间，假设每轮只发送一个单位消息，则该值为轮数</li><li>$T_init$: 初始化异步系统所需的时间(顺序消息跳数)</li><li>$rounds$: 迭代次数</li><li>$T_round$: 一次迭代的耗时(顺序消息跳数)</li></ul></li></ul></li><li><p>复杂度<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Synchronizers%20Complexity.png" alt="img"></p></li><li><p>Simple Synchronizer</p><ul><li>每个进程每轮向每个邻居发送一条消息(合并消息或发送虚拟消息)</li><li>在接收到每个邻居的消息后，节点进入下一轮</li><li>邻居$P_i, P_j$之间可能仅相差一轮，因此$P_i$在轮次$round_i$只能接收来自$round_i$或$round_{i+1}$的消息</li><li>初始化<ul><li>任何进程都可以开始轮次</li><li>在直径(d个时间单位)内，所有进程都会处于该轮次</li><li>$T_{init} = d, M_{init} = 0$</li></ul></li><li>复杂度<ul><li>$M_{round}=2|L|, T_{round}=1$</li></ul></li></ul></li><li><p>$\alpha$ Synchronizer</p><ul><li>$P_i$从轮次$r$中移动到$r+1$，当且仅当所有邻居在轮次$r$中都是安全的。</li><li>当邻居$P_j$接收到它发出的每个消息的ack后，它会告知$P_i$以及其他邻居它$P_j$是安全的</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/alpha%20synchronizer.png" alt="img"></p><ul><li>复杂度<ul><li>每条信息都有ack，对于传输层来说，ack可能是自带的，因为建立连接就有握手</li><li>每次迭代$2|L|$条消息以通知邻居自己是安全的:<ul><li>$M_{round} = O(|L|)$</li><li>$T_{round} = O(1)$</li></ul></li><li>无需初始化，每个进程都可以自发地唤醒并开始执行</li><li>$\beta$ Synchronizer</li></ul></li><li>初始化<ul><li>有根的生成树进行初始化</li><li>消息数量为$O(n*logn+|L|)$</li><li>时间复杂度$O(n)$</li></ul></li><li>操作<ul><li>安全节点发起汇聚传播(<code>CvgC</code>)</li><li>中间节点在其子树安全时传播<code>CvgC</code></li><li>当根节点变得安全并从所有子节点接收<code>CvgC</code>后，发起树广播通知所有节点进入下一轮</li></ul></li><li>复杂度<ul><li>确认消息是免费的，理由同上</li><li>每轮迭代消息数量: $M_{round} = 2(n-1)$</li><li>每轮迭代时间复杂度:<ul><li>平均: $T_{round} = 2 logn $</li><li>最坏: $T_{round} = 2n$</li></ul></li><li>$\gamma$ Synchronizer</li></ul></li><li>集群集合: 每个集群有一个生成树</li><li>集群内: $\beta$同步器沿着生成树的边传播</li><li>集群间: 集群稳定后，$\alpha$同步器沿着指定的集群间边传播(对于两个相邻的集群之间有1条集群边)<ul><li>为了传达集群间α同步器的稳定状态，在集群内部通过树进行CvgC汇聚传播(叶节点发起)和BC广播。</li></ul></li><li>复杂度<ul><li>每轮的消息数量$M_{round}=O(L_c)$, $L_c$是集群中的链接数</li><li>每轮的时间复杂度$T_{round}=O(h_c)$, $h_c$是集群中的树的高度</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/gamma%20synchronizer.png" alt="img"></p><h3 id="MIS-async-randomized"><a href="#MIS-async-randomized" class="headerlink" title="MIS, async, randomized"></a>MIS, async, randomized</h3><ul><li>MIS异步随机算法(最大独立集, Maximal Independent Set)<ul><li>对于一个图$(N, L)$，节点集合$N’\subset N$，是一个独立集，如果对于$N’$中的每个节点i和j，它们之间不存在边</li><li>一个独立集是最大独立集当且仅当不存在严格超集是独立集(即不可能比它大的独立集)。</li><li>一个图可能有多个最大独立集，大小可能不同</li><li>是个NP完全问题，可用于无线广播的频带分配(互斥的)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Maximal%20Independent%20Set%20Example.png" alt="img"></p><ul><li>Luby’s 异步随机算法<ul><li>流程<ul><li>节点随机选择自己的数字并与邻居交换</li><li>邻域中最小的数字获胜选入最大独立集</li><li>如果邻居被选中，则自己要被淘汰</li><li>只有被选中节点的邻居会被淘汰</li></ul></li><li>复杂度<ul><li>在每次迭代中，至少有一个节点被选中，至少有一个节点被淘汰，因此最多需要$\leq \frac{n}{2}$次迭代。</li><li>由于算法具有随机性质，预期迭代次数为$O(logn)$</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Luby%20Maximal%20Independent%20Set%20Code.png" alt="img"></p><h3 id="CDS"><a href="#CDS" class="headerlink" title="CDS"></a>CDS</h3><ul><li>支配集: 图$(N, L)$的一个支配集$N’\subset N$，使得每个不在$N’$中的节点都与$N’$中的某个节点有边相连</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Compact%20Routing%20Tables.png" alt="img"></p><ul><li>连通支配集(Connected Dominating Set)<ul><li>图$(N, L)$的一个支配集$N’$，使得由$N’$中的节点诱导的子图是连通的</li><li>属于NP完全问题<ul><li>寻找最小的连通支配集</li><li>判断是否存在大小为$k\lt |N|$的支配集</li></ul></li></ul></li><li>多项式时间启发式算法:<ul><li>使用近似因子和拉伸因子进行度量</li><li>创建生成树(ST)，然后删除叶子节点的边</li><li>创建最大独立集，然后添加边创建连通支配集</li></ul></li><li>应用<ul><li>广播的骨干，用于网络中的高效数据传输</li></ul></li></ul><h3 id="Compact-routing-tables"><a href="#Compact-routing-tables" class="headerlink" title="Compact routing tables"></a>Compact routing tables</h3><ul><li>紧凑路由表<ul><li>要避免大小为n的表格(大尺寸的路由表会导致更多的处理时间。)   - 层次路由: 层次化的集群网络，IPV4。使用层次路由来减少路由表的大小。</li><li>树标签方案     - 用于路由的逻辑树拓扑     - 节点标签表示可以通过连续地址标记的链路到达的目的地。<ul><li>这种方法可以生成较小的路由表，但可能会导致流量不平衡。</li></ul></li></ul></li><li>区间路由<ul><li>节点标签$B$对$N$是一比一映射</li><li>边标签: I标记L中的每一条边，使用$B(N)$的某个子集作为节点标签<ul><li>所有的目的地都会被覆盖($\bigcup_{y\in Neighbors}I(x, y)\cup B(x)=N$)</li><li>没有覆盖重复$(I(x, w)\cap I(x, y)=\empty)$对于$w, y\in Neighbors$</li></ul></li><li>对于任何$s, t$，存在一条路径$<s=x_0, x_1, ..., x_{k-1}, x_k = t>$，其中$B(t)\in I(x_{i-1}, x_i)$对于每个$i\in [1, k]$</li><li>区间标签适用于所有图</li><li>路径长度没有保证，对拓扑变化不鲁棒</li></ul></li><li>前缀路由<ul><li>节点、通道标签来自同一域，视为字符串。</li><li>路由：使用标签为目的地最长前缀的通道。</li></ul></li><li>伸缩因子$r$<ul><li>$max_{i, j\in N}\{\frac{distance_r(i, j)}{distance_{opt}(i, j)}\}$<ul><li>定义为所有节点对$i, j$中的最大值，$distance_r(i, j)$是使用路由方案r从节点i到节点j的距离，而$distance_{opt}(i, j)$是最佳路径的距离</li></ul></li><li>设计紧凑的路由方案<ul><li>这是一个涉及丰富图算法的问题。</li><li>需要识别并证明路由效率的界限。</li><li>不同的专用拓扑结构(例如，网格、环、树)可以提供更容易的结果范围。</li></ul></li></ul></li></ul><h3 id="Leader-election-LCR-algorithm"><a href="#Leader-election-LCR-algorithm" class="headerlink" title="Leader election: LCR algorithm"></a>Leader election: LCR algorithm</h3><ul><li>领导者选举<ul><li>所有进程就一个共同的进程(领导者)达成一致</li><li>分布式算法不完全对称: 需要一个启动者和结束进程；例如，最小生成树MST用于BC和CvgC来计算全局函数。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Leader%20Election.png" alt="img"></p><ul><li>LeLang Chang Roberts (LCR) 算法<ul><li>异步单向环</li><li>所有进程具有唯一的ID</li><li>进程循环它们的ID，最大的ID获胜</li><li>尽管有明显的优化，但<ul><li>消息复杂度: $\frac{n(n-1)}{2}$</li><li>时间复杂度: $O(n)$</li></ul></li></ul></li><li>不存在匿名环上的确定性领导者选举算法</li><li>算法可能是统一的，即适用于所有节点。</li><li>Hirschberg-Sinclair Algorithm<ul><li>双向二分搜索: 基于令牌的环形搜索</li><li>每轮迭代k时，每个活跃的进程执行以下操作<ul><li>令牌传递给两侧的$2^k$个邻居</li><li>进程$P_i$在第k轮后成为领导者，当且仅当i时两侧$2^k$个邻居中具有最高ID的进程<ul><li>在第k轮后，任何一对领导者之间的距离至少为$2^k$</li><li>领导者的数量随着$\frac{n}{2^k}$按对数级减少</li></ul></li><li>只有获胜的领导者才能进入下一轮迭代</li></ul></li><li>复杂度<ul><li>每轮最多发送n条消息，类似于LCR的抑制机制。共$logn$轮</li><li>消息复杂度: $O(n*logn)$</li><li>时间复杂度: $O(n)$</li></ul></li></ul></li></ul><h3 id="Dynamic-object-replication"><a href="#Dynamic-object-replication" class="headerlink" title="Dynamic object replication"></a>Dynamic object replication</h3><ul><li><p>对象复制问题</p><ul><li>加权图: $(N, L)$，其中k个用户位于$N_k\subset N$节点，r个对象副本位于$N_r\subset N$</li><li>如果$k\gt r$并且访问是只读的，副本的最佳放置是最小化从用户到最近副本的距离总和($min(\sum_{i\in N_k, r_i\in N_r}dist_{i, r_i})$，其中的$dist_{i, r_i}$为节点到最近副本的成本)</li><li>如果来自每个用户$N_k$的读取访问具有一定的频率(权重)，则最小化函数发生变化</li><li>需要考虑每条边的带宽限制</li><li>假设用户访问是读取的概率为x，更新的概率为1-x。更新需要所有副本都进行更新</li></ul></li><li><p>动态对象复制: 自适应数据复制</p><ul><li>网络$(V, E)$，$V$是点集，$E$是边集</li><li>复制方案: V的子集R，其中每个节点在R中都有一个副本</li><li>$r_i, w_i$: 节点i发出的读取和写入速率</li><li>$c_r(i), c_w(i)$: 节点i发出的读取和写入成本</li><li>$R$: 所有可能的复制方案集合</li><li>目标：最小化复制方案的成本：<ul><li>$\min_{R’\in R}[\sum_{i\in V}r_i\cdot c_r(i) + \sum_{i\in V}w_i\cdot c_w(i)]$</li></ul></li><li>成本是NP完全问题</li><li>假设单个副本可串行化，通过Read-One-Write-All(ROWA)策略实现</li></ul></li><li><p>基于树形覆盖</p><ul><li>所有通信: 在树形覆盖$T$上设置$R$<ul><li>$R$: 类似变形虫的子图，移动到活动的重心<ul><li>当读取成本较高时扩展</li><li>当写入成本较高时收缩</li><li>平衡状态$R$是最优的，一旦读写模式稳定，则在d+1步内收敛</li><li>动态活动: 算法在周期中重新执行</li></ul></li></ul></li><li>读取: 从最近的副本，沿着$T$，使用父指针</li><li>写入: 到最近的副本，沿着$T$，然后在$R$中传播，使用$R$的邻居集</li><li>实现<ul><li>判断是否在$R$中</li><li>寻找$R$的邻居</li><li>寻找父节点</li></ul></li></ul></li><li><p>收敛性</p></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Adaptive%20Data%20Replication%20Convergence.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Adaptive%20Data%20Replication%20Tests.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Adaptive%20Data%20Replication.png" alt="img"></p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Adaptive%20Data%20Replication0.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;术语和基本算法&quot;&gt;&lt;a href=&quot;#术语和基本算法&quot; class=&quot;headerlink&quot; title=&quot;术语和基本算法&quot;&gt;&lt;/a&gt;术语和基本算法&lt;/h1&gt;&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Sia</title>
    <link href="http://zjn-astonishe.github.io/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Sia/"/>
    <id>http://zjn-astonishe.github.io/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Sia/</id>
    <published>2024-10-27T01:20:24.000Z</published>
    <updated>2025-04-14T03:05:11.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sia-Heterogeneity-aware-goodput-optimized-ML-cluster-scheduling"><a href="#Sia-Heterogeneity-aware-goodput-optimized-ML-cluster-scheduling" class="headerlink" title="Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling"></a>Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文提出了一个分布式调度器——Sia，能够为弹性的资源自适应job高效地分配异构的深度学习集群资源，是一个支持混合(异构)并行Job弹性地扩展的集群调度器</p><ul><li>Sia提出了一个新的调度公式(scheduling formulation)来扩大搜索空间的大小搜索集群资源，合理地配置运行job的GPU类型和GPU数量。</li><li>Sia提出了一个低分析开销(low-profiling-overhead)的方法为每个新的job引导(bootstrapping)吞吐量模型(throughput models)，用于评估可能的资源分配配置。</li></ul><p>实验结果表明，Sia的性能优于目前最先进的调度器，至少可以扩展到大小为2000个GPU的分布式集群，为每个job提供了更好的公平性，并且对调度器参数的初始化设置不太敏感</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>大小可变的深度学习集群(DL clusters)被多个用户共享为多个不同的问题训练各自的深度学习模型。</p><ul><li>深度学习集群是由混合类型的GPU构成的</li><li>调度器用来为job分配集群的资源</li></ul><h3 id="Sia"><a href="#Sia" class="headerlink" title="Sia"></a>Sia</h3><p>Sia是一个为自适应资源(resource-adaptive)的DL训练job和异构资源设计的调度器</p><ul><li>在每个调度轮次，Sia会考虑为当前job分配GPU数量和类型的可能性，评估这些job的goodput(包括了job的重新分配大小的花费)，选择在下一个轮次运行的最好的集群资源分配</li><li>要实现非常有挑战性，有以上两个原因：<ul><li>对于一个可变大小的集群来说，搜索空间很大</li><li>不同的job对于不同的GPU类型和不同的GPU数量的性能反应不同，因为不同类型的GPU本身就具有不同的计算网络带宽比。GPU的多种类型，不同数量，可以有不同的组合，因此要遍历所有的分配情况是不现实的，昂贵且耗时</li></ul></li><li>Sia解决上述挑战的方法是: <ul><li>提出一个新的求解公式(solver formulation)以处理规模问题<ul><li>Sia的新ILP公式: 即使负载(load)和集群(cluster)大小增加，能够有效找到所有待处理job的GPU类型、GPU计数和数据批量大小的配置方式</li></ul></li><li>提出一个新方法在线学习(online-learning)每个job，每种GPU类型的吞吐量模型(throughput model)<ul><li>Sia的吞吐量模型是关于GPU类型、GPU数量和数据批量大小的函数，能够避免大规模的分析(大规模分析可能会使得优秀的调度算法反而性能变坏)，实际就是处理批量大小数据花费的时间</li><li>因为Sia对于每种GPU类型仅仅使用一个最小规模的配置文件(开销就低了)引导每个新job的吞吐量模型，然后再进行缩放/投影(scaling/projection)，最后在job运行的时候动态优化吞吐模型</li></ul></li></ul></li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>揭示目前最先进的调度器的不足之处，指出研究方向: 异构性(heterogeneity)和弹性(elastic)</li><li>提出第一个能够弹性扩展，具有混合类型异构的，并行job的集群(cluster)调度器Sia<ul><li>提出新的ILP公式，解决了异构GPU类型和job自适应的复合复杂性: 要兼顾GPU的不同类型、GPU的不同数量和不同的数据批量大小</li><li>提出新的吞吐量模型，预先观察几个小批量在每个GPU类型的运行结果作为引导(bootstrap)，然后随着job按Sia的优化配置运行而且在运行过程中快速有效地改进</li></ul></li><li>表明Sia在其目标领域中与最先进的调度器相匹配，并且在结合其域的复杂性方面优于它们，结果还显示了Sia的可扩展性(scalability)、公平性(fairness)和参数化鲁棒性(parameterization robustness)</li></ul><h2 id="DL-cluster-scheduling"><a href="#DL-cluster-scheduling" class="headerlink" title="DL cluster scheduling"></a>DL cluster scheduling</h2><p>深度学习(DL)训练job是以迭代的方式在多个周期根据数据集上的数据训练DNN模型。对于每个迭代周期中的每个小批量(minibatch)，优化器会根据小批量样本计算损失函数，以最小化损失函数为目的更新模型的参数</p><ul><li>对于耗时长的训练(如果不是整个训练)，minibatch的大小通常是固定的，大多数的DL jobs总共花费固定且可预测的时间处理一个minibatch</li><li>jobs通常也是可抢占的，可以在处理任何minibatch后检查job的状态(包括模型和优化器的状态)，从checkpoint恢复jobs，而不会损失太多的进度</li><li>易于扩展，因为梯度计算可以在单个节点上的多个GPU或多个节点进行并行化</li></ul><h3 id="Data-Parallelism"><a href="#Data-Parallelism" class="headerlink" title="Data Parallelism"></a>Data Parallelism</h3><p>大多数的training job使用的是同步的数据并行(synchronous data parallelism)，即给定一组GPU，每个GPU上都会运行一个完整的模型，而不同的GPU运行例如all-reduce算法根据不同的minibatch(与GPU本身的内存大小相关)的数据计算梯度，然后更新各自GPU上的模型的参数</p><ul><li>对于每个小批量的数据，梯度计算阶段在各个GPU内部独立完成，而在reduce阶段进行同步</li></ul><p>给定的DL job的扩展程度取决于job的特性</p><ul><li>计算强度</li><li>模型参数的个数</li><li>GPU的性能</li><li>连接GPU的内部网络？(inter-GPU network, 和通信相关？)</li></ul><h3 id="Model-Parallelism"><a href="#Model-Parallelism" class="headerlink" title="Model Parallelism"></a>Model Parallelism</h3><p>在被训练的模型太大，一个GPU的内存无法容纳的时候，可以把模型分割到几个GPU上运行</p><ul><li>流水线(Pipeline Model Parallelism, PMP)</li><li>Tensor Model Parallelism(TMP)</li></ul><h3 id="弹性和资源自适应的DL-jobs"><a href="#弹性和资源自适应的DL-jobs" class="headerlink" title="弹性和资源自适应的DL jobs"></a>弹性和资源自适应的DL jobs</h3><p>数据并行的jobs可以随着时间的推移弹性地重新调整整体batch_size的大小，通过checkpoint保留上下文，然后在不同数量的GPU上重新启动。(例如在更多的GPU上训练以扩大整体bacth_size)<br>数据并行的实现也可以不重新启动，通过拷贝原始配置进行缩放</p><h3 id="异构资源"><a href="#异构资源" class="headerlink" title="异构资源"></a>异构资源</h3><p>GPU类型有很多种，在内存大小、计算和通信性能方面不同。<br>DL集群通常会包含多种GPU。</p><ul><li>因为集群是随着时间的推移进行部署和增长的，每次购买添加的新硬件时都可以选择最具成本效益的选项。</li></ul><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>DL jobs作为请求提交到共享的集群中，调度器为job分配资源以实现集群范围的目标。目前许多调度器只允许指定固定数量的相同类型GPU的请求，忽略了弹性、资源自适应和异构性。</p><h3 id="异构感知-Heterogeneity-aware-的调度器"><a href="#异构感知-Heterogeneity-aware-的调度器" class="headerlink" title="异构感知(Heterogeneity-aware)的调度器"></a>异构感知(Heterogeneity-aware)的调度器</h3><p>考虑集群的不同GPU类型，但现有的调度器只适用于刚性(rigid)job，无资源的自适应</p><ul><li>rigid job必须使用用户指定数量的GPU运行，不允许弹性缩放，也不能自适应地分配资源(也就是非动态的分配)，做其他调整<br>其中的代表是Gavel，使用一个可以扩展到大集群的快速线性规划公式。但不支持job的自适应，只在job提交者指定的小批量大小和GPU数量的情况下优化分配的GPU的类型。</li><li>如果批量太小，这种方法可能会导致频繁更新、更强大的GPU利用率不足等问题</li><li>当集群里的job数量拥塞时，会频繁切换运行的job，使得将GPU时间浪费在checkpoint的恢复操作上<br>这是因为Gavel用的是填充有<code>(job_id, GPU_type)</code>对的吞吐量矩阵(throughput matrix)表示调度选项，如果简单地扩展条目为<code>(job_id, GPU_type, num_GPUs, minibatch_size)</code>，有个问题:</li><li>求解吞吐量矩阵的非零解，需要对每个job进行大量多余分析，优化程序会很大，无法迅速解决问题</li></ul><h3 id="自适应感知-Adaptivity-aware-的调度器"><a href="#自适应感知-Adaptivity-aware-的调度器" class="headerlink" title="自适应感知(Adaptivity-aware)的调度器"></a>自适应感知(Adaptivity-aware)的调度器</h3><p>考虑job在不同数量的GPU上执行(非刚性，调整批量大小batch_size)，但这些GPU都是同一类型的，不考虑异构性。<br>其中的代表是Pollux，使用每个Job的吞吐量模型为其设置GPU数量和批量大小，并根据更新的job的行为和job的队列信息在每个调度周期重新考虑所有的分配(弹性分配，避免未使用或过度使用GPU资源)。</p><ul><li>每个作业的goodput模型由两个组件组成<ul><li>一个是统计效率模型(基于梯度噪声标度Gradient Noise Scale的每个样本的训练进度，训练每个样本的速度)，批量大小的函数</li><li>一个是吞吐量模型(每秒处理的样本数量)，GPU数量和批量大小的函数。<ul><li>通过放大、测量每个尝试的GPU数量计数并为其他计数插值，可以了解每个job如何随着GPU数量缩放</li><li>每个job模型采用遗传算法搜索所有当前job的资源分配空间和相应的批量大小，通过公平性地加权以最大化集群范围内的总产出<ul><li>遗传算法对于同构的GPU集群的扩展都非常困难，何况对异构GPU集群的扩展</li><li>因为对于每个<code>(job, GPU_count)</code>对，要考虑将此作业放置在所有节点的可能性。而可能的解的数量和节点数量及节点内GPU的数量呈指数关系。(1000多个GPU的集群，遗传算法需要几十分钟才能完成)</li></ul></li><li>但是无预分析(no-pre-profiling)的吞吐量模型阻碍了GPU的异构性</li></ul></li></ul></li><li>在同构系统中，Pollux针对整个资源空间进行优化，你每个job的可选情况复杂度为$O(N^R)$<ul><li>假设一个job需要R个GPU，考虑全局情况，则每个GPU都有N个节点选择。所以一共为$O(N^R)$</li></ul></li></ul><h3 id="同构集群上刚性job的调度器"><a href="#同构集群上刚性job的调度器" class="headerlink" title="同构集群上刚性job的调度器"></a>同构集群上刚性job的调度器</h3><p>要求job的提交者为每个job指定GPU数量和相关配置，调度器不会根据当前负载或当前job的可扩展性/运行效率调整分配的GPU数量，也不会考虑GPU类型的差异(假设所有GPU都是相同种类)。效率较低。<br>现在有些能够调整GPU数量提高GPU利用率，但不会同时调整批量大小和GPU数量、类型。代表性的是Shockwave</p><h3 id="非集群调度器的并行优化器"><a href="#非集群调度器的并行优化器" class="headerlink" title="非集群调度器的并行优化器"></a>非集群调度器的并行优化器</h3><p>为单独的job孤立地考虑并选择配置，而非集群调度器，即只考虑个体job运行效率，而不考虑整体集群的效率</p><h2 id="Sia-Design-and-Implementation"><a href="#Sia-Design-and-Implementation" class="headerlink" title="Sia Design and Implementation"></a>Sia Design and Implementation</h2><p>Sia是一个抢占式的基于轮次的调度器，优化一组job的资源分配，以最大限度地提高集群范围内的goodput指标(该指标包含统计效率模型和吞吐量模型，具体见<a href="#自适应感知adaptivity-aware的调度器">自适应感知(Adaptivity-aware)的调度器</a>)。使用checkpoint-restore的抢占机制优化自适应的job</p><h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/Sia%20Process.png" alt="img"></p><ul><li>用户提交一个job给Sia的队列Queue，记为J</li><li>Sia的Profiler宣告本次执行的最大的批量大小(max_bsz)和最大的GPU数量(max_ngpus)。接着配置J使用小批量数据分别在各个类型的一块GPU运行</li><li>Sia的Goodput Estimator引导一个吞吐量模型评估J在各个类型的一块GPU上运行的性能(评估用的是statistical efficiency model和throughput model)<ul><li>Goodput Estimator为GPU内存容量，互连的速度和吞吐量进行建模</li></ul></li><li>J会一直停留在队列Queue中，直到Sia分配一些GPU给它</li><li>Sia的Policy根据来自Goodput Estimator的goodput评估值，在集群的job之间找到最佳的集群资源划分方案</li><li>Sia的Placer根据Policy给出的配置方案将对应的资源分配给对应的job，并尝试减少由于资源碎片化而导致的不必要的job的迁移<ul><li>将分配分为策略和实际两步进行，能够限制分配的放置空间，也就是减少碎片化。需要遵循三条规则：<ul><li>部分节点(这些节点被请求的GPU数量少于其拥有的最大GPU数量?)的分配不得在两个节点上拆分(即节点本身满足资源要求，就不能强行拆分到不同节点)</li><li>整个节点分配必须占用整个节点(也是不能随便拆分)</li><li>如果不存在满足以上两个规则的位置，则拿下一个job再重启继续尝试分配(这种情况非常罕见，通常不超过3次)</li></ul></li></ul></li><li>Sia的Adaptive Executors负责运行job，支持:<ul><li>透明的checkpoint-restore机制，用于低开销的job抢占和资源扩展</li><li>自适应的批量大小，以最大化统计效率。当统计效率要求比GPU内存有限的支持更大的批量大小时，会使用梯度累计算法(gradient accumulation)</li><li>频繁报告当前分配下模型的梯度和吞吐量统计数据(默认为30s一次)</li><li>每个job最开始是在一块GPU上运行的，然后通过每个调度轮扩大为在2倍数量的GPU上运行(当然有最低运行要求的job，会从要求的最小GPU数量开始运行扩大)。每个job也可能被缩小规模到最低运行要求以便在集群中job拥塞的时候容纳更多的joib</li></ul></li><li>J在Adaptive Executors开始执行后，Goodput Estimator会使用Adaptive Executors报告J的梯度和吞吐量统计数据更新J在当前资源配置下的goodput model</li><li>在下一轮调度前，Sia的Policy会根据Goodput Estimator反馈的更新后J在所有GPU类型上的goodput估计值继续寻找最佳的集群资源划分方案</li><li>不断循环，直到完成或终止</li></ul><h3 id="Bootstrapping-of-throughput-models"><a href="#Bootstrapping-of-throughput-models" class="headerlink" title="Bootstrapping of throughput models"></a>Bootstrapping of throughput models</h3><p>为每种类型的GPU构建每个job的吞吐量模型(作为GPU数量和批量大小的函数)的传统方法是运行每种GPU的多GPU分配方案，收集计算和通信时间。这种分析方法的开销随着GPU类型数量和每个节点拥有的GPU的数量都是线性增长的<br>Sia则以最少的分析信息开始，根据观察到的分配进行改进。对于每个job，Sia为每种GPU类型学习一个吞吐量模型和为job学习一个统计效率模型(批量大小的函数)。Sia首先分配各种类型的GPU给最低数量要求给job($\geq 1$)，从小的批量大小开始，不断提高批量大小直到达到GPU内存限制(通常是10倍)。每个类型的GPU在这个过程花费$\lt 20$ GPU seconds。就能获得：</p><ul><li>不同的GPU类型和批量大小组合的计算时间</li><li>比较不同GPU类型的计算时间<br>假设计算时间和GPU数量增长是独立的，无关系的，因为采用的是采用all-reduce的数据并行技术。所以整个集群的时间分为计算时间和通信时间，计算时间只和单个GPU的计算时间相关</li></ul><p>Sia会为某种GPU类型在2个GPU上运行，然后计算吞吐量模型结果。与单个GPU上运行的吞吐量模型结果的两倍的差值便能得到耗费在通信的时间<br>根据在该类型GPU上测得的通信时间可以给其他类型GPU利用，因为假设是通信时间与计算时间无关，即通信时间与GPU类型无关。也就是说，已知N个某类型GPU的表现，则可以根据不同类型的GPU在单个GPU上的表现的比值来估计N个其他类型GPU的表现(实验也佐证了可用性)，用一个公式表示：</p><ul><li>$est-xput_B(N) = \frac{xput_B(1)}{xput_A(1)}*xput_A(N)$<br>不过需要注意的是，这只是对没有运行的GPU类型的粗略的估计，如果已经运行了某类型的GPU，则应该使用在线配置(online profiling)的结果准确预测通信时间</li></ul><h3 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h3><p>配置由一系列的资源组成，如CPU, GPU, Network, etc，可以表示为<code>(n, r, t)</code>，其中<code>n</code>表示节点数量，<code>r</code>表示资源数量，<code>t</code>表示资源类型。</p><ul><li>例如<code>(2, 16, T4)</code>表示的是2个节点包含16个T4 GPU<br>为了减少资源争用，避免分布式作业共享节点。配置集合可以划分为两部分：</li><li>单节点分配集合: 不会跨节点分配资源，$\{(1, 2^0, X), (1, 2^1, X), …, (1, R, X)\}\bigcup\quad\leftarrow$ single-node<ul><li>该分配方式限制每次都分配一个节点内2的倍数的GPU，直到节点拥有的最多GPU数量$R$。如果$R$不是2的幂，则可以把$R$拆分成2的幂的和，即把一个物理节点拆分成多个虚拟节点</li></ul></li><li>多节点分配集合: 需要跨节点分配资源，$\{(2, 2R, X), …, (N, N\cdot R, X), n\in N\}\quad\leftarrow$ multi-node<ul><li>该分配方式要求每个节点的GPU都必须完全被利用，即遵守第二条规则(详见<a href="#process">process</a>)<br>假设集群是同构的，Sia在优化配置过程中考虑的复杂度是$O(N+log_2R)$</li></ul></li><li>因为不需要考虑所有的情况。</li><li>假设一个job需要R个GPU，每个节点都能满足该要求，且Sia限制job不能在节点资源未得到完全利用的情况下跨越其他节点，则只需选定在哪个节点运行——有$N$种情况。</li><li>Sia从1个GPU开始，每个迭代轮次将分配的GPU数量翻倍，即共有$log_2R$种配置</li></ul><h3 id="Scheduler-objective"><a href="#Scheduler-objective" class="headerlink" title="Scheduler objective"></a>Scheduler objective</h3><h4 id="Valid-Configurations"><a href="#Valid-Configurations" class="headerlink" title="Valid Configurations"></a>Valid Configurations</h4><p>即可用的配置，可用于分配给设备的配置。具体可见<a href="#Configuration">Configuration</a>。在一个调度轮次，一个job要么不被分配资源，要么按照列举的可用配置分配资源</p><h4 id="Goodput-Estimation"><a href="#Goodput-Estimation" class="headerlink" title="Goodput Estimation"></a>Goodput Estimation</h4><p>定义了一个大小为$|J|\times|C|$(每个job的每种配置)的goodput矩阵$G$，$G_{ij}$表示的是$J_i$使用$c_j$给定配置的资源估计得到的goodput</p><ul><li>对于这个矩阵的每一行，即表示每一个job，可以直接对比矩阵的值来确定哪种配置更加优秀</li><li>但对于这个矩阵的每一列，即表示每一种配置，不可以直接对比矩阵的值来说明该配置更适合哪个job去运行(需要简单的行归一化)</li></ul><h4 id="Normalized-goodput-matrix"><a href="#Normalized-goodput-matrix" class="headerlink" title="Normalized goodput matrix"></a>Normalized goodput matrix</h4><p>对于最低GPU数量要求为$N_i^{min}$的$J_i$，有公式: </p><ul><li>$G_{ij}\leftarrow N_i^{min}\cdot\frac{G_{ij}}{min_jG_{ij}}$<ul><li>$min_jG_{ij}$: $J_i$所有配置的goodput估计值中的最小值</li></ul></li><li>即用矩阵每行最小值进行归一化，有两个好处：<ul><li>可以把$G$解释为$J$的效用矩阵(utility-matrix)，每一个元素的值都说明某种配置对某个job的效用</li><li>对于每种配置，可以比较其对每种job的效用<br>每个新运行的job都会为矩阵$G$添加一个新的行，每个已完成的job都会在矩阵$G$删除对应行。矩阵$G$只记录活跃的job，且其中的值也会随着goodput的变化而变化</li></ul></li></ul><h4 id="Scheduler-objective-1"><a href="#Scheduler-objective-1" class="headerlink" title="Scheduler objective"></a>Scheduler objective</h4><p>定义了一个大小与$G$相同的二进制矩阵$A$，其中$A_{ij}==1$表明下一轮次J_i选择的配置为$c_j$<br>Sia调度的目标是为每个job选择配置，使得所有job的归一化goodput之和最大</p><ul><li>不是每个job都挑选归一化goodput值最大是因为可能出现多对一的情况，但实际是一对一的<br>于是对Sia的调度目标建模得到：</li><li>$\displaystyle\max_A\sum_{i=1}^{|J|}(\sum_{j=1}^{|C|}A_{ij}\cdot G_{ij} + \lambda(1-\lVert A_i\rVert_1))$<ul><li>$\lVert v\rVert_1$表示相邻$v$的L1范式，即求和后的绝对值</li><li>这是一个二进制整数线性规划问题，有以下限制:<ul><li>每个job最多选择一种配置: $\lVert A_i\rVert \leq 1$</li><li>被分配的GPU数量不能超过对应类型GPU的可用数量</li></ul></li><li>外层的求和是对所有job选择的配置的goodput估计值求和，max说明目标把该求和最大化</li><li>内层对包含两个部分<ul><li>前者为一整行求和，由于$A$矩阵每一行最多只有一个非零值，因此实际是说明$J_i$选择的配置的goodput。</li><li>后者为惩罚项，当$J_i$不选择任何一种配置的时候，该项为非0，否则为0。惩罚项也许是负数，即对不分配资源做出惩罚，是减少调度器队列中的job的一种激励，使得Sia为集群中的每个job至少分配一个GPU</li></ul></li></ul></li></ul><h4 id="Restart-Factor-重启参数"><a href="#Restart-Factor-重启参数" class="headerlink" title="Restart Factor(重启参数)"></a>Restart Factor(重启参数)</h4><p>频繁地重新分配资源会带来频繁地重启，而频繁重启对性能有害，代价昂贵。<br>因此设置了一个重启参数$r_i$，用来评估是否要重启job(重新分配资源)</p><ul><li>$r_i=\frac{T_i-N_i\cdot S_i}{T_i+S_i}$<ul><li>$J_i$: job</li><li>$T_i$: job已经运行的时间</li><li>$S_i$: 每次浪费在重启操作上的GPU seconds</li><li>$N_i$: 先前已经重启过的次数(已经重启过的次数越多，再次重启的可能性就越低)<br>最好只在如果不重启(重新分配资源)会使得调度目标的goodput最佳值大幅度下降时，才会重启</li></ul></li></ul><h4 id="Balancing-goodput-with-fairness"><a href="#Balancing-goodput-with-fairness" class="headerlink" title="Balancing goodput with fairness"></a>Balancing goodput with fairness</h4><p>用一个参数$p$来平衡job的goodput。让每个job都能获得相对公平的资源分配。则加上重启参数$r_i$公式变为</p><ul><li>$\displaystyle\max_A\sum_{i=1}^{|J|}(\sum_{j=1}^{|C|}A_{ij}\cdot (r_i\cdot G_{ij})^p + \lambda(1-\lVert A_i\rVert_1))\quad p\gt 0$</li><li>$\displaystyle\min_A\sum_{i=1}^{|J|}(\sum_{j=1}^{|C|}A_{ij}\cdot (r_i\cdot G_{ij})^p + \lambda(1-\lVert A_i\rVert_1))\quad p\lt 0$</li><li>实验发现$p$在$-1.0~1.0$之间能够在获得稳定的公平性的同时对性能指标最小的负面影响，其中$p=-0.5$效果最好</li></ul><h4 id="Support-for-limited-adaptivity-支持有限制的自适应性"><a href="#Support-for-limited-adaptivity-支持有限制的自适应性" class="headerlink" title="Support for limited adaptivity(支持有限制的自适应性)"></a>Support for limited adaptivity(支持有限制的自适应性)</h4><p>大的数据规模使模型有高的吞吐量和使GPU有高利用率，但也可能导致训练模型的泛化误差(generalization gap)。因此Sia页支持对不同类型的job使用不同程度的自适应性。</p><ul><li>strong-scaling(强扩展) jobs: 固定的批量大小，但允许优化GPU数量和类型<ul><li>优化的目标函数公式: $\displaystyle\max_A\sum_{i=1}^{|J|}(\sum_{j=1}^{|C|}A_{ij}\cdot (r_i\cdot T_{ij})^p + \lambda(1-\lVert A_i\rVert_1))\quad p\gt 0$(因为对于固定的批量大小，throughput和goodput成正比，不用考虑统计效率)</li></ul></li><li>rigid(刚性) job: 固定的批量大小和GPU数量，只允许优化GPU类型<ul><li>优化的目标函数公式: $\displaystyle\max_B\sum_{i=1}^{|J_R|}(\sum_{g=1}^{|N_g|}B_{ig}\cdot (r_i\cdot T_{ig})^p + \lambda(1-\lVert B_{ig}\rVert_1))\quad p\gt 0$(固定了GPU数量，配置的方案只需要考虑GPU的类型)</li></ul></li></ul><h4 id="Preemption-and-reservation-抢占和预订"><a href="#Preemption-and-reservation-抢占和预订" class="headerlink" title="Preemption and reservation(抢占和预订)"></a>Preemption and reservation(抢占和预订)</h4><p>Sia假设所有的job都是抢占式的，但也支持一小部分的job是非抢占式的(只要它们的总需求能够得到满足)。在分配资源的时候，保证这些非抢占式的job被先分配资源，每一轮调度都要保证非抢占(在公式中为每个非抢占式的job加入限制项，促进资源分配给它们)。</p><p>如果DL训练job的资源分配发生改变，Sia只能在当前小批量处理完成后才抢占该作业进行重新分配(不会有正在进行的通信导致结果丢失等问题)</p><p>Reservation预定策略可以为设置了该策略的队列中的job预先保留一部分计算资源，不给其他job使用。原理和对非抢占式的处理相似</p><h4 id="Checkpoint-restore"><a href="#Checkpoint-restore" class="headerlink" title="Checkpoint-restore"></a>Checkpoint-restore</h4><p>Sia会在每一轮调度结束后，都会为最新的模型参数、数据加载器(数据采样器和迭代器的状态)和优化器的状态(如Adam的梯度统计数据)建立checkpoint，并存储在磁盘中<br>如果DL训练job的分配发生变化，则:</p><ul><li>在该轮调度结束完成checkpoint保存</li><li>释放分配给job的所有资源</li><li>在新的要分配的资源上，为每个GPU启动一个Adaptive Executors</li><li>从磁盘上的checkpoint在新分配的资源上恢复训练状态，继续模型训练</li></ul><p>checkpoint-restore还可以用于将job从故障中恢复，即在每个轮次出了故障，从上次的checkpoint重新恢复</p><h4 id="Support-for-other-parallelization-techniques-其他的并行技术"><a href="#Support-for-other-parallelization-techniques-其他的并行技术" class="headerlink" title="Support for other parallelization techniques(其他的并行技术)"></a>Support for other parallelization techniques(其他的并行技术)</h4><p>扩展Sia的吞吐量模型，使其支持使用流水线和数据并行的job，允许Sia调度拥有几十亿参数模型的job</p><ul><li>流水线并行属于模型并行，策略是把一个大模型拆分到多个GPU上。</li><li>数据并行用来扩展训练规模，即同一时间可以训练更多的数据</li></ul><p>假设模型的每个部分可以映射到$P$个GPU上($P\geq 1$)，有$N$个数据并行副本的job实际需要$N\times P$个GPU。因为每个数据并行副本都要在同一时间单独完整地过一遍模型，也就需要$N$个模型，而模型被拆分到了P个GPU上，所以一共需要$N\times P$个GPU<br>N条流水线上的副本会使用梯度all-reduce算法进行同步，完成一次训练迭代</p><ul><li>all-reduce实际目的是要汇总所有计算出来的梯度做个平均再分发给各个并行部分去更新各自参数继续训练</li></ul><p>假设给定mini-batch_size为$M$，表示一次迭代要处理的总的数据数量。给定micro-batch为$m$，表示每P个GPU(模型每个部分)一次迭代能处理的数据数量。</p><ul><li>则模型每一部分P个GPU上的副本都要在本地用$\frac{M}{mN}$个micro-batch计算梯度。<ul><li>数据并行，所以一个模型处理$\frac{M}{N}$的数据</li><li>模型并行，所以模型的每个组件(拥有$P$个GPU)需要处理$\frac{\frac{M}{N}}{m}$的数据</li></ul></li></ul><p>现在的混合并行优化器(hybrid-parallel, 即混合了模型并行和数据并行)非常耗时，这属于未来可以研究的方向</p><h4 id="Scheduling-other-workload-types-调度其他类型的工作负载"><a href="#Scheduling-other-workload-types-调度其他类型的工作负载" class="headerlink" title="Scheduling other workload types(调度其他类型的工作负载)"></a>Scheduling other workload types(调度其他类型的工作负载)</h4><p>Sia可以不止用于调度深度学习训练的job，还可以用于其他类型的job(例如inference推理)</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>实现用的是开源的AdaptDL框架(基于PyTorch，提供对动态调整批量大小和GPU数量的原生支持)，只是用Sia替换了本身的调度器，且重构了框架的数据加载器(data-loader)<br>采用可配置的学习率缩放规则(configurable learning rate scaling rule)，根据批量大小缩放训练的学习率(批量大学习率大，批量小学习率小)</p><ul><li>使用AdamW优化器的模型，采用平方根学习率缩放规则(即缩放的步长为平方或平方根)</li><li>使用SGD优化器的模型，采用AdaScale自适应缩放规则(训练一个检测器来检测学习率的效果，从而决定如何缩放学习率)</li></ul><p>Sia的Policy像一个Kubernetes服务一样运行，在每轮调度开始的时候，根据最近的goodput模型的结果用公式优化资源分配(详细见<a href="#scheduler-objective">Scheduler objective</a>)。</p><ul><li>将公式看作是混合整数线性规划问题(Mixed-Integer Linear Program)，用来自CVXPY包的GLPK_MI求解器求解</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Sia-Heterogeneity-aware-goodput-optimized-ML-cluster-scheduling&quot;&gt;&lt;a href=&quot;#Sia-Heterogeneity-aware-goodput-optimized-ML-cluster-sche</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Ray conclusion</title>
    <link href="http://zjn-astonishe.github.io/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Ray%20conclusion/"/>
    <id>http://zjn-astonishe.github.io/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Ray%20conclusion/</id>
    <published>2024-10-27T01:06:43.000Z</published>
    <updated>2024-11-09T04:33:57.466Z</updated>
    
    <content type="html"><![CDATA[<h1 id="《Ray-A-Distributed-Framework-for-Emerging-AI-Applications》-Review"><a href="#《Ray-A-Distributed-Framework-for-Emerging-AI-Applications》-Review" class="headerlink" title="《Ray: A Distributed Framework for Emerging AI Applications》 Review"></a>《Ray: A Distributed Framework for Emerging AI Applications》 Review</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文提出了一个分布式强化学习框架Ray，以解决其他框架无法统一直接地完成强化学习计算的问题。</p><ul><li>功能上，Ray实现了一个统一的接口，由一个动态执行的引擎提供并行任务(task-parallel)和基于actor的计算(actor-based computation)的支持。</li><li>性能上，Ray采用分布式调度器和一个分布式的(distributed)且容许出错的(falut-tolerant)存储以管理系统的状态控制。</li><li>实验结果表明：Ray拥有每秒超过180万个任务的扩展性，以及在几个具有挑战性的强化学习应用中拥有比其他现有专门系统更好的性能。</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>新兴的人工智能应用越来越多地运行在动态的环境中。它们对环境的变化做出反应，并采取一系列行动来完成长期目标——不仅是利用收集到的数据，而且还要探索可能的行动空间。这些更广泛的要求很自然地被置于强化学习(RL)的范式中。</p><p>强化学习(Reinforcement Learning，RL)，即在一个不确定的环境中，基于延迟和有限的反馈，进行不断地学习。它的核心目标是学习一个策略(strategy)，实现从环境状态到行动选择的映射，以达到更好的效果。这就提出了它需要具备的三个主要能力：</p><ul><li>依赖于模拟(simulation)去评估策略，探索不同的行动序列，并了解这些选择的长期后果。</li><li>需要进行分布式训练，根据模拟或与物理世界的互动产生的数据改进策略。</li><li>策略的目的是提供解决方案以控制问题。因此有必要在closed-loop和open-loop的控制场景中为策略提供服务。</li></ul><h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>考虑RL系统的基本组成部分，如图1所示，在一个RL环境中，一个agent会反复与环境进行交互，它的目标是学习一个使奖励(reward)最大化的策略(policy)。策略则是一个从环境状态到行动选择的映射。其中的Environment、Agent、state、action、reward的精确定义是视应用而定的。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/16.png" style="zoom: 33%;" /></p><p>为了学习一个策略，Agent通常要采用两步程序：</p><ul><li>评估策略<ul><li>为了评估策略，Agent需要与Environment互动，产生由state和reward二元组构成的trajectory。</li></ul></li><li>改进策略<ul><li>根据得到的trajectory改进policy：按照使reward最大化的梯度方向更新policy。</li></ul></li></ul><p>图2显示了一个Agent学习策略的伪代码例子，通过调用<code>rollout(environment, policy)</code>生成trajectory评估策略，然后调用<code>train_policy()</code>通过<code>policy_update(trajectories)</code>改进当前策略。随后不断重复，直到收敛。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/1.png" style="zoom:45%;" /></p><p>由伪代码可以看出，RL应用的框架必须为Training、Serving和Simulation提供有效的支持：</p><ul><li>Training涉及运行随机梯度下降(SGD)以更新策略。</li><li>Serving使用训练过的策略，根据环境的当前状态采取一个行动。</li><li>Simulation进行策略评估。</li></ul><p>因此，RL应用的框架应该满足以下的需求：</p><ul><li><p>支持细粒度的计算：</p><ul><li>在真实世界交互在毫秒内呈现动作，并执行大量的计算。</li></ul></li><li><p>支持异构的计算</p><ul><li><p>时间上：计算的持续时间从毫秒到小时不等</p></li><li><p>硬件上：训练通常需要异构的硬件(CPU、GPU、TPU)</p></li></ul></li><li><p>灵活的计算模型：无状态计算和有状态计算</p><ul><li>无状态计算：在任何节点上都可运行，很适合细粒度的模拟和数据处理，如从图像或视频中提取特征。</li><li>有状态计算，非常适合实现参数服务器(parameter server)，对GPU支持的数据进行重复计算，或运行不暴露其状态的第三方模拟器。</li></ul></li><li><p>动态执行</p><ul><li>计算完成的顺序并不总是事先知道的。</li><li>计算的结果可以决定未来的计算。</li></ul></li><li><p>高吞吐量</p><ul><li>为了在大型集群中实现较高的利用率，框架必须每秒能处理数百万的任务。</li></ul></li><li><p>通用性</p><ul><li>能够与现有的模拟器和深度学习框架无缝集成。</li></ul></li></ul><h3 id="Deficiencies"><a href="#Deficiencies" class="headerlink" title="Deficiencies"></a>Deficiencies</h3><p>RL要求Training、Serving和Simulation必须紧密耦合在一个应用程序中，且三者有严格的延迟要求。但目前没有任何框架能支持。</p><ul><li>MapReduce、Apache Spark、Dryad等批量同步并行系统(Bulk-synchronous parallel systems)不支持细粒度模拟(fine-grained simulation)或策略服务(policy serving)。</li><li>CIEL、Dask等任务并行系统(Task-parallel systems)和Naiad、Storm等流媒体系统(streaming systems)几乎不支持分布式的训练和服务(distributing training and serving)</li><li>TensorFlow和MXNet等分布式深度学习框架不支持模拟(simulation)和服务(serving)。</li><li>TensorFlow Serving和Clipper等模型服务系统不支持训练(training)和模拟(simulation)。</li></ul><p>也许在理论上可以直接缝合多个专门的框架以提供整体功能，但在实践中，系统间由此产生的数据移动和延迟会令人望而却步。</p><p>如果为专门的RL应用程序构建一次性的系统(one-off systems)，会给分布式应用程序的开发带来巨大的系统工程负担，因为基本把系统层需要处理的问题推到了应用层。</p><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><h3 id="Programming-and-Computation-Model"><a href="#Programming-and-Computation-Model" class="headerlink" title="Programming and Computation Model"></a>Programming and Computation Model</h3><p>Ray实现了一个动态任务图计算模型(dynamic task graph computation model)，将应用程序建模为一个在执行过程中不断变化的依赖性任务图。在此基础上，提供了actor和task-parallel的编程抽象。</p><h4 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h4><p>在Ray中有两种编程模型：task和actor</p><ul><li><p>task：表示在无状态worker上执行一个remote函数。当调用一个remote函数的时候，会立即返回表示task结果的future(可以由<code>ray.get()</code>API获取)。也可以作为参数传递给其他remote函数，而无需等待结果。</p><ul><li>由于是无状态的，所以输出完全由输入决定。</li><li>相当于C/C++中的函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running a Task</span></span><br><span class="line"><span class="comment"># Define the square task.</span></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">square</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch four parallel square tasks.</span></span><br><span class="line">futures = [square.remote(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Retrieve results</span></span><br><span class="line"><span class="built_in">print</span>(ray.get(futures))</span><br><span class="line"><span class="comment"># -&gt;[0, 1, 4, 9]</span></span><br></pre></td></tr></table></figure></li><li><p>actor：表示有状态的计算模型，每个actor都公开了其可以被远程调用并连续执行的方法(这些方法类似于task，不同之处是这些方法在有状态worker上执行)。actor的句柄可以传递给其他的actor或者task，使它们也能够调用该actor上的方法。</p><ul><li>相当于C/C++中的类。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calling an Actor</span></span><br><span class="line"><span class="comment"># Define the Counter actor.</span></span><br><span class="line"><span class="meta">@ray.remote</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Counter</span>:</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">incr</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.i += value</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Counter actor.</span></span><br><span class="line">c = Counter.remote()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Submit calls to the actor. These calls run asynchronously but in</span></span><br><span class="line"><span class="comment"># submission order on the remote actor process.</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    c.incr.remote(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Retrieve final actor state.</span></span><br><span class="line"><span class="built_in">print</span>(ray.get(c.get.remote()))</span><br><span class="line"><span class="comment"># -&gt; 10</span></span><br></pre></td></tr></table></figure></li></ul><p>两者主要区别如表2所示：</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/17.png" style="zoom:50%;" /></p><p>为了满足异构性和灵活性的需求，用三种方式增强API：</p><ul><li>为了处理具有不同持续时间的并发任务，引入<code>ray.wait()</code>，等待前k个可用结果，而不是像<code>ray.get()</code>等待所有的结果。</li><li>为了处理资源异构任务，Ray使开发人员能够指定资源需求(在定义task和actor的时候指定)，以便Ray调度器可以高效管理资源。</li><li>为了提高灵活性，启用了嵌套的remote函数，意味着remote函数可以调用其他remote函数。对于实现高可扩展性也是至关重要的，因为它允许多个进程以分布式方式调用remote函数。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2.png" style="zoom:50%;" /></p><div style="page-break-after: always;"></div><h4 id="Computation-Model"><a href="#Computation-Model" class="headerlink" title="Computation Model"></a>Computation Model</h4><p>Ray采用了动态任务图计算模型。当remote函数与actor方法的输入可用时，系统会自动触发它们的执行。其构建方式可以从下面图3和图4实例得知：</p><p><center class = "half">  <img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/4.png" width="49%"/>  <img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3.png" width="50%"/></center></p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>如图5所示，Ray的架构设计分为两层：</p><ul><li>提供API的应用层。</li><li>提供高扩展性和容错的系统层。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/22.png" style="zoom: 40%;" /></p><h4 id="Application-Layer"><a href="#Application-Layer" class="headerlink" title="Application Layer"></a>Application Layer</h4><p>Ray的应用层采用传统的driver-worker模式进行组织：</p><ul><li>driver：运行用户程序的进程。</li><li>worker：一个无状态的进程，用于执行由driver或另一个worker调度的task(remote函数)。它会自动启动，并由系统曾分配任务。</li><li>actor：一个有状态的进程，只执行它公开的方法。相当于worker或driver的显式实例化。<ul><li>方法是串行执行的，且每个方法都依赖于前一个方法执行所产生的状态。</li></ul></li></ul><h4 id="System-Layer"><a href="#System-Layer" class="headerlink" title="System Layer"></a>System Layer</h4><p>系统层包括三个主要的组件：所有的组件都具备水平可扩展性和容错性。</p><ul><li><p>全局控制存储(global control store，GCS)</p><ul><li>负责维护系统的整个控制状态，核心是具有发布订阅(pub-sub)功能的KV(key-value)存储。<ul><li>每条存储的数据都会采用一条随机生成的数据作为ID，利用ID作为分片键将数据分散到多个GCS分片中。由于GCS也是分布式的，等同于一个分布式的redis集群加上一个统一入口，分片提供了扩展能力。并且每个分片都进行了链复制(chain-replicated)以保证容错。</li></ul></li><li>GCS显著简化了Ray的总体设计，使得系统中的每个节点(除了GCS节点)都是无状态的。<ul><li>实现了持久脉络存储(durable lineage storage)与其他系统组件的解耦。所有的节点在故障时只需重新启动并从GCS读取存储的脉络(lineage)即可恢复。</li><li>将对象元数据(object metadata)存储在GCS，而不是全局调度器中，实现任务调度和数据传输的解耦，最大限度减少任务调度的开销。</li></ul></li></ul></li><li><p>自下而上的分布式调度器(bottom-up distributed scheduler，DS)</p><ul><li><p>设计了一个由全局调度器和每个节点上的本地调度器组成的两级层次调度器。</p><ul><li>都使用事件驱动的单线程进程(event-driven singlet-hreaded process)实现。</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/19.png" style="zoom:50%;" /></p></li><li><p>本地调度器(local scheduler)</p><ul><li>为了避免全局调度器过载，在节点上创建的task首先提交到节点的本地调度器。自下而上的意思便是首先在本地尝试调度task。</li><li>如果节点过载(本地task队列超过了预定义的阈值)，或者不能满足task的需求(例如缺乏GPU)，本地调度器将决定不在本地调度task，而将其转发给全局调度器。</li></ul></li><li><p>全局调度器(global scheduler)</p><ul><li>考虑每个节点的负载和task的约束条件做出调度决定。更确切的说，全局调度器确定有足够的任务所要求类型的资源的节点集，并在这些节点中选择那个具有最低估计等待时间的节点。<ul><li>最低估计等待时间(lowest estimated waiting time)为以下两项之和：<ul><li>task在该节点排队的估计时间(task队列大小*平均task执行时间)。</li><li>task的远程输入的估计传输时间(远程输入的总大小除以平均带宽)。</li></ul></li></ul></li><li>全局调度器通过心跳(heartbeats)获取每个节点的队列大小和资源的可用性。</li><li>全局调度器使用简单的指数平均(simple exponential averaging)计算平均task执行时间和平均传输带宽。</li><li>全局调度器实际也是分布式的。如果全局调度器成为了瓶颈，可以通过GCS实例化更多的共享相同信息的调度器副本，使得调度器架构具有很高的可扩展性。</li></ul></li></ul></li><li><p>内存中的分布式对象存储(In-Memory distributed object store，DOS)</p><ul><li>为了最小化task的延迟，实现了一个基于内存的分布式存储系统，用于存储每个task的输入和输出，或者无状态的计算结果。<ul><li>Ray中的节点在启动时，默认会从机器的物理内存占据一部分作为对象存储。</li><li>对象存储直接采用Apache Arrow数据格式。</li></ul></li><li>每个节点通过共享内存进行对象存储：<ul><li>如果本地资源能够满足，则允许在同一节点上运行的不同task之间进行零拷贝的数据共享。</li><li>对象存储仅限于不可变的数据，无需更新对象。</li><li>如果task的输入不是来自本地，则在执行之前DOS会将输入复制到本地的对象存储中。task的输出也会写入本地的对象存储。<ul><li>复制消除了由热数据对象引起的潜在瓶颈(不需要多次通信传输相同对象)。内存不够时，使用LRU机制将部分数据换出内存，写入磁盘。</li><li>task只需要从本地内存读取数据或像本地内存写入数据(应用层)，因此最大限度地缩短了任务的执行时间，但也增加了计算绑定工作负载的吞吐量。</li></ul></li><li>当节点出现故障时，Ray通过重启节点运行re-execution进行恢复，在初始执行期间，存储在GCS中的脉络会追踪无状态的task和有状态的actor，并使用前者重构重启节点存储中的对象。</li><li>对象存储不支持分布式对象，使用者可以在应用层去实现。</li></ul></li><li>为了在不同的对象存储之间传输大型的对象，将对象分割到多个TCP连接中。</li></ul></li></ul><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Ray是一个活跃的开源项目(40k代码)，与Python环境完全集成，只需要执行以下指令即可轻松安装。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Ray</span><br></pre></td></tr></table></figure><div style="page-break-after: always;"></div><h3 id="Running"><a href="#Running" class="headerlink" title="Running"></a>Running</h3><p>下面展示Ray是如何运行端到端的工作的：以将两个对象(可以是标量或者矩阵)<code>a</code>和<code>b</code>相加，并返回结果<code>c</code>为例。</p><p>图a表示的是driver程序调用$add.remote(a, b)$的具体步骤，其中<code>a</code>和<code>b</code>分别存储在节点<code>N1</code>和<code>N2</code>。</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/20.png" style="zoom:50%;" /></p><ol><li>remote函数$add()$在初始化时自动注册到GCS，并分发给系统中的每个worker。</li><li>driver程序将$add(a,b)$提交给<code>N1</code>的本地调度器。</li><li><code>N1</code>的本地调度器在发现输入<code>b</code>不在本地，不满足运行条件，便将$add(a,b)$转发给全局调度器。</li><li>全局调度器在GCS中查找$add(a,b)$的参数位置。</li><li>全局调度器决定在存储参数<code>b</code>的<code>N2</code>上运行$add(a,b)$。</li><li><code>N2</code>的本地调度器检查本地的对象存储是否包含$add(a,b)$的参数。</li><li>由于<code>N2</code>的本地存储没有对象<code>a</code>，且全局调度器已经指定由其完成该task，于是它在GCS中查找对象<code>a</code>的位置。</li><li><code>N2</code>得知对象<code>a</code>存储在<code>N1</code>，<code>N2</code>的对象存储在本地复制<code>N1</code>中的对象<code>a</code>。</li><li>现在$add()$的所有参数都存储在<code>N2</code>本地，<code>N2</code>的本地调度器在本地worker处调用$add()$。</li><li>$add()$通过共享内存访问参数。</li></ol><p>图b表示的是在<code>N1</code>执行$ray.get()$获得在<code>N2</code>执行的$add()$所返回值的具体步骤：</p><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/21.png" style="zoom:50%;" /></p><ol><li><code>N1</code>调用$ray.get(id_c)$时，driver程序使用$add()$返回的<code>future</code>检查本地对象存储中的值<code>c</code>。</li><li>由于<code>N1</code>本地对象存储没有存储<code>c</code>，<code>N1</code>在GCS中查找<code>c</code>的位置。此时GCS中并没有<code>c</code>的条目，因为<code>c</code>还未被创建。因此<code>N1</code>的对象存储空间向GCS中的对象表注册一个回调，该回调在<code>c</code>的条目被创建时触发。</li><li>当<code>N2</code>中的worker执行完$add()$后，将结果<code>c</code>存储在<code>N2</code>的本地对象存储中。</li><li><code>N2</code>的本地存储将对象<code>c</code>的条目添加到GCS中。</li><li>GCS创建对象<code>c</code>的条目，并由此触发了<code>N1</code>的对象存储所注册的回调。</li><li><code>N1</code>的本地存储从<code>N2</code>的本地存储复制对象<code>c</code>。</li><li><code>N1</code>本地存储将对象<code>c</code>返回到$ray.get()$，完成了task。</li></ol><div style="page-break-after: always;"></div><h2 id="Advantages-and-Disadvantages"><a href="#Advantages-and-Disadvantages" class="headerlink" title="Advantages and Disadvantages"></a>Advantages and Disadvantages</h2><h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h3><ul><li>Ray设计和构建了第一个联合了训练、模拟和服务的分布式框架，能够满足新型人工智能应用对框架的各种需求。<ul><li>Ray在只考虑训练、模拟或服务的单独场景中拥有接近甚至超过专门系统的性能表现。</li><li>在Ray上实现的需要训练、模拟和服务紧密耦合的强化学习算法在性能上接近甚至超过专门为该算法设计的系统。</li></ul></li><li>Ray在单个动态任务图中统一了task parallel和actor programming models，采用了由全局控制存储GCS和自下而上的分布式调度器构成的架构，使得：<ul><li>Ray允许开发者用task对模拟actor产生的输出进行昂贵的后续处理，且Ray能够通过60个节点每秒完成100万个task，意味着可以实现Billion级别的大规模仿真。</li><li>Ray的GCS和分布式调度器能够水平扩展系统以支持高吞吐量的细粒度任务，同时保持容错性和低延迟任务调度。</li></ul></li><li>Ray非常的灵活，并没有绑定成某一种特定应用场景或计算模式的解决方案，是一个真正的原生分布式框架。<ul><li>在上层抽象出不同的计算模式，包括流处理、批处理、图计算、机器学习、深度学习、强化学习等。但Ray本身不提供具体计算的功能，可结合TensorFLow或者Pytorch计算框架使用。</li><li>在下层提供分布式服务，解决调度问题、容错问题、资源恢复问题等。通过简单的硬件资源参数设置，就能让计算运行在不同的异构硬件上。</li></ul></li><li>Ray的API简单高效，使得编程模式非常友好。能非常容易实现先进的RL算法。</li></ul><h3 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h3><ul><li>Ray的task是无副作用的，而对于GPU，开发者并不只想用来运行任务、获取数据和存储数据，希望能够把状态实际保留在GPU上，导致任务不能保持无副作用，令提供容错能力变得困难。</li><li>Ray的actor只适用于顺序性的单独线程，在使用多线程的情况下难以提供容错性。</li><li>Ray缺少系统隔离能力，一个有害的actor实现可以轻易独占当前的CPU资源。</li><li>由于Ray的分布式节点上会有不止一个线程，需要频繁进行切换，其调度和上下文切换的开销会比理论上的结果更大。表现在当任务较多时，容易卡死。</li><li>Ray要在不完全了解计算图的情况下做出调度决策、优化可能需要更复杂的运行时分析。</li><li>Ray使用对象(object)的作用域来管理对象的生命周期，这意味着没有作用域的对象将会有无限的生命周期。Ray缺乏垃圾回收策略，无法约束GCS中的存储成本。使得在数据密集时容易出现严重的数据膨胀。</li><li>深度学习框架像对待数据计算一样重视且全权管理数据搬运，不能把数据搬运委托给更底层的机制以至于数据搬运隐式地在背后发生，这会丧失宝贵的确定性和可预测性，深度学习要求必须把数据搬运像计算一样作为算子显式的调度管理。但Ray通过对象存储和RPC机制实现了一套“自动”的数据搬运机制，反而可能会帮倒忙。</li><li>Ray主要使用gRPC通信，手段较为单一，缺乏集群通信等的支持。通信功能有待增强。</li><li>Ray的API和功能不够丰富，需要更高级别的为调度决策提供信息的原语和库。</li><li>Ray是基于Python实现的，Python主要的应用场景在数据科学(数据分析/AI/科学计算)和运维两个领域。企业应用主要还是使用Java，所以Ray好像很少有企业应用的案例。因此从市场前景的角度，Ray还离通用框架有一定距离。</li></ul><div style="page-break-after: always;"></div><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>通过对本文的学习，主要有以下收获：</p><ul><li>对新一代的分布式强化学习框架Ray有了大致的了解。</li><li>分布式系统中如果丢失了一些数据，可以根据保留的最初创建该数据的任务链恢复数据。如果任务(task)是无副作用且确定性的，那么重新执行将获得同样的输出。依此可以实现容错性模型。</li><li>集中控制状态将是未来分布式系统的一个关键设计组件。它既能简化边缘端设备的程序设计，又方便对各个边缘设备进行监控调试，在边缘节点故障时能通过集中控制状态组件迅速恢复。</li><li>通过将调度器划分为全局和本地两层，并将全局调度器与数据存储解耦，能够最大限度减少任务调度的开销。</li><li>分布式系统中的组件也可以是分布式的，这使得系统具有非常强大的扩展性。</li><li>复杂的算法能通过简单的API实现。</li><li>对开源的第三方著名项目要保持继承发扬的态度，既不应抗拒，也不应依赖。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><p><a href="https://arxiv.org/pdf/1712.05889v2">Ray: A Distributed Framework for Emerging AI Applications</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5ODY2MTk3Nw==&amp;mid=2247486546&amp;idx=1&amp;sn=1f9578739d434f30afc2e90556685004&amp;chksm=fe418264c9360b72b75ceddcd1c413493c9dec3061fcaddab41e82db5363676580c7c39f5b05&amp;scene=21#wechat_redirect">Ion Stoica：做成Spark和Ray两个明星项目的秘笈</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/566982400">《Ray: A Distributed Framework for Emerging AI Applications》论文解读</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/446818362">Ray解读：从论文看架构</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/86658441">Ray: A Distributed Framework for Emerging AI App 笔记</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/681561996">Ray: A Distributed Framework for Emerging AI Applications 阅读笔记</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/507619560">Ray: A Distributed Framework for Emerging AI Applications 学习笔记</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/61818897">分布式框架Ray及RLlib简易理解</a></p></li><li><p><a href="https://www.zhihu.com/question/265485941">如何看UCBerkeley RISELab即将问世的Ray，replacement of Spark？</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/498342150">解读谷歌 Pathways 架构（二）：向前一步是 OneFlow</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;《Ray-A-Distributed-Framework-for-Emerging-AI-Applications》-Review&quot;&gt;&lt;a href=&quot;#《Ray-A-Distributed-Framework-for-Emerging-AI-Applicatio</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="http://zjn-astonishe.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>全局状态及快照算法</title>
    <link href="http://zjn-astonishe.github.io/2024/10/25/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-10-25-%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81%E5%8F%8A%E5%BF%AB%E7%85%A7%E7%AE%97%E6%B3%95/"/>
    <id>http://zjn-astonishe.github.io/2024/10/25/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-10-25-%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81%E5%8F%8A%E5%BF%AB%E7%85%A7%E7%AE%97%E6%B3%95/</id>
    <published>2024-10-25T13:25:26.000Z</published>
    <updated>2024-11-09T04:33:57.468Z</updated>
    
    <content type="html"><![CDATA[<h1 id="全局状态及快照算法"><a href="#全局状态及快照算法" class="headerlink" title="全局状态及快照算法"></a>全局状态及快照算法</h1><h2 id="System-model"><a href="#System-model" class="headerlink" title="System model"></a>System model</h2><ul><li>The system consists of a collection of $n$ processes $p_1, p_2, …, p_n$ that are connected by channels</li><li>There are no globally shared memory and physical global clock and processes communicate by passing messages through communication channels with unpredicatable message delays<ul><li>$C_{ij}$ denotes the channel from process $p_i$ to process $p_j$ and its state is denoted by $SC_{ij}$<ul><li>For a channel $C_{ij}$, the following set of messages can be defined based on the local states of the processes $p_i$ and $p_j$</li><li>Transit: $transit(LS_i, LS_j)=\{m_{ij}|send(m_{ij}\in LS_i \wedge rec(m_{ij})\notin LS_j\}$(发送端发送了，但接收端没接收，说明消息在信道)</li></ul></li></ul></li><li>The actions performed by a process are modeled as three types of events: <ul><li>Internal events, </li><li>the message send event</li><li>the message receive event</li></ul></li><li>For a message $m_{ij}$ that is sent by process $p_i$ to process $p_j$, let $send(m_{ij})$ and $rec(m_{ij})$ denote its send and receive events</li><li>At any distant, the state of process $p_i$, denoted by $LS_i$, is a result of the sequence of all the events executed by $p_i$ till that instant(进程的状态是之前所有执行过的事件)<ul><li>For an event $e$ and a process state $LS_i, e\in LS_i$ iff $e$ belongs to the sequence of events that have taken process $p_i$ to state $LS_i$</li><li>For an event $e$ and a process state $LS_i, e\notin LS_i$, iff $e$ does not belong to the sequence of events that have taken process $p_i$ to state $LS_i$</li></ul></li></ul><h3 id="Models-of-communication"><a href="#Models-of-communication" class="headerlink" title="Models of communication"></a>Models of communication</h3><ul><li>In FIFO model, each channel acts as a first-in first-out message queue and thus, message ordering is preserved by a channel(FIFO消息是有序到达)</li><li>In non-FIFO model, a channel acts like a set in which the sender process adds messages and the receiver process removes messages from it in a random order.(non-FIFO消息是无序到达)</li><li>A system that supports causal delivery of messages satisfies the following property: <ul><li>“For any two messages $m_{ij}$ and $m_{kj}$, if $send(m_{ij})\rightarrow send(m_{kj})$, then $rec(m_{ij}) −→ rec(m_{kj})$”</li><li>因果序，先发和后发有因果序，则先发的会先收，且和后收有因果序</li></ul></li></ul><h2 id="Consistent-global-state"><a href="#Consistent-global-state" class="headerlink" title="Consistent global state"></a>Consistent global state</h2><ul><li>The global state of a distributed system is a collection of the local states of the processes and the channels<ul><li>Notationally, global state $GS$ is defined as<ul><li>$GS=\{\bigcup_i LS_i, \bigcup_{i, j}SC_{ij}\}$(所有的进程状态和所有的信道状态)</li></ul></li></ul></li><li>A global state $GS$ is a consistent global state iff it satisfies the following two conditions:<ul><li>$C1: send(m_{ij})\in LS_i \Rightarrow m_{ij}\in SC_{ij} \bigoplus rec(m_{ij})\in LS_j$, ($\bigoplus$ is Ex-OR operator, 异或)(发出去的消息要么被接收要么在信道，不可能同时在接收方和信道，也不可能都不在)</li><li>$C2: send(m_{ij})\notin LS_i \Rightarrow m_{ij}\notin SC_{ij}\wedge rec(m_{ij})\notin LS_j$(没发的消息不可能出现在接收方和信道)</li></ul></li></ul><h3 id="Interpretation-in-terms-of-cuts"><a href="#Interpretation-in-terms-of-cuts" class="headerlink" title="Interpretation in terms of cuts"></a>Interpretation in terms of cuts</h3><ul><li>(从剪切的角度解释，看<a href="https://zjn-astonishe.github.io/2024/09/28/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-09-28-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/">分布式计算模型</a>)</li><li>A cut in a space-time diagram is a line joining an arbitrary point on each process line that slices the space-time diagram into a <code>PAST</code> and a <code>FUTURE</code></li><li>A consistent global state corresponds to a cut in which every message received in the <code>PAST</code> of the cut was sent in the <code>PAST</code> of that cut(在过去接收的只能是在过去发送的), and such a cut is known as a consistent cut</li></ul><h3 id="Issues-in-recording-a-global-state"><a href="#Issues-in-recording-a-global-state" class="headerlink" title="Issues in recording a global state"></a>Issues in recording a global state</h3><ul><li>How to distinguish between the messages to be recorded in the snapshot from those not to be recorded(区分记录在快照的消息和未记录的消息)<ul><li>Any message that is sent by a process before recording its snapshot, must be recorded in the global snapshot (from C1).(在快照前发送的消息必须在快照中记录，无论是信道还是接收方)</li><li>Any message that is sent by a process after recording its snapshot, must not be recorded in the global snapshot (from C2).(在快照后发送的消息不能出现在快照中)</li></ul></li><li>How to determine the instant when a process takes its snapshot(如何确定何时该进行快照)<ul><li>A process $p_j$ must record its snapshot before processing a message $m_{ij}$ that was sent by process $p_i$ after recording its snapshot.(在发送进程快照后才发送的消息，接收进程必须在接收后处理该消息之前进行快照)</li></ul></li></ul><h2 id="Snapshot-algorithms"><a href="#Snapshot-algorithms" class="headerlink" title="Snapshot algorithms"></a>Snapshot algorithms</h2><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Snapshot%20algorithms.png" alt="img"></p><h3 id="Chandy-Lamport-algorithm-For-FIFO-channels"><a href="#Chandy-Lamport-algorithm-For-FIFO-channels" class="headerlink" title="Chandy-Lamport algorithm(For FIFO channels)"></a>Chandy-Lamport algorithm(For FIFO channels)</h3><ul><li>uses a control message, called a <code>marker</code> whose role in a FIFO system is to separate messages in the channels(<code>marker</code>用于分离信道中的消息)</li><li>After a site has recorded its snapshot(记录快照后), it sends a <code>marker</code>, along all of its outgoing channels before sending out any more messages(发送更多消息前，沿着所有的传出信道发送<code>marker</code>)</li><li>A marker separates the messages in the channel into those to be included in the snapshot from those not to be recorded in the snapshot.(分为在快照中的消息和不在快照中的消息)</li><li>A process must record its snapshot no later than when it receives a marker on any of its incoming channels.(进程必须在其任何传入信道接收<code>marker</code>后处理<code>marker</code>前记录自己的快照)</li></ul><h4 id="process"><a href="#process" class="headerlink" title="process"></a>process</h4><ul><li>The algorithm can be initiated by any process by executing the “Marker Sending Rule” by which it records its local state and sends a <code>marker</code> on each outgoing channel(任何一个进程都能作为发起者)</li><li>A process executes the “Marker Receiving Rule” on receiving a marker. If the process has not yet recorded its local state, it records the state of the channel on which the marker is received as empty and executes the “Marker Sending Rule” to record its local state.(所有如果在处理<code>marker</code>的时候没有保存本地快照，则相当于没有收到此<code>marker</code>，然后自己执行Marker Sending Rule，此前传<code>marker</code>的进程也会收到该进程的<code>marker&#39;</code>，则会再发一遍)</li><li>The algorithm terminates after each process has received a marker on all of its incoming channels.(如果每个进程在所有的传入信道都接收到了<code>marker</code>则算法终止)<ul><li>All the local snapshots get disseminated to all other processes and all the processes can determine the global state.(所有本地快照都会传播到所有其他进程，因此所有进程都可以确定全局状态。)</li></ul></li></ul><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Marker Sending Rule for process i</span><br><span class="line">1- Process i records its state</span><br><span class="line">2- For each outgoing channel C on which a marker has not been sent, i sends a marker along C, before i sends further messages along C</span><br><span class="line"></span><br><span class="line">Marker Receiving Rule for process j</span><br><span class="line">On receiving a marker along channel C:</span><br><span class="line">    if j has not recorded its state then</span><br><span class="line">        Record the state of C as the empty set</span><br><span class="line">        Follow the &quot;Marker Sending Rule&quot;</span><br><span class="line">    else</span><br><span class="line">        Record the state of C as the set of messages</span><br><span class="line">        received along C after j&#x27;s state was recorded</span><br><span class="line">        and before j received the marker along C</span><br></pre></td></tr></table></figure><h4 id="Correctness"><a href="#Correctness" class="headerlink" title="Correctness"></a>Correctness</h4><ul><li>Due to FIFO property of channels, it follows that no message sent after the marker on that channel is recorded in the channel state. Thus, condition $C2$ is satisfied(FIFO的特性，使得marker后发送的消息不会被记录，满足$C2$规则)</li><li>When a process $p_j$ receives message $m_{ij}$ that precedes the marker on channel $C_{ij}$($p_j$从信道$C_{ij}$收到在marker之前的$m_{ij}$后), it acts as follows: <ul><li>If process $p_j$ has not taken its snapshot yet, then it includes $m_{ij}$ in its recorded snapshot. (如果$p_j$还没进行快照，那么在后面进行快照的时候，需要包括$m_{ij}$)</li><li>Otherwise, it records $m_{ij}$ in the state of the channel $C_{ij}$. Thus, condition C1 is satisfied(否则把$m_{ij}$记录在信道$C_{ij}$的状态中，满足$C1$规则)</li></ul></li><li>$C1$和$C2$都满足，因此该算法是正确的</li></ul><h4 id="Complexity"><a href="#Complexity" class="headerlink" title="Complexity"></a>Complexity</h4><ul><li>The recoding part of a single instance of the algorithm requires $O(e)$ messages(该算法整个过程需要$O(e)$条消息，因为每个进程需要向各个信道都发marker，$e$表示的是消息信道的数量) and $O(d)$ time($d$表示网络的直径，表明的是marker传输的最长距离), where $e$ is the number of edges in the network and $d$ is the diameter of the network</li></ul><h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h4><ul><li>The recorded global state may not correspond to any of the global states that occurred during the computation.(记录的全局状态可能不对应于计算期间发生的任何全局状态)</li><li>This happens because a process can change its state asynchronously before the markers it sent are received by other sites and the other sites record their states.(进程可以在发送的marker被其他站点接收之前异步更改其自己的状态，并且其他站点记录自身的状态。其实就是说记录的状态被发出去后，发送者会继续运行，其他站点记录状态后记录在全局快照中，也会继续运行。全局状态的快照是由不同的异步局部状态组成的)<ul><li>But the system could have passed through the recorded global states in some equivalent executions.(虽然全局状态快照的情景没有同时发生，但异步发生过)</li><li>The recorded global state is a valid state in an equivalent execution and if a stable property (i.e., a property that persists) holds in the system before the snapshot algorithm begins, it holds in the recorded global snapshot.(记录的全局状态是等效执行的有效状态，如果在快照算法开始之前系统中保持稳定的属性(持久属性)，则将保持在记录的全局快照中)</li><li>Therefore, a recorded global state is useful in detecting stable properties(记录的全局状态在检测平稳的特点是有用的)</li></ul></li></ul><h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ul><li>In a non-FIFO system, a marker cannot be used to delineate messages into those to be recorded in the global state from those not to be recorded in the global state.(消息到达不按顺序，则无法划分记录在全局状态中的消息和不需要记录在全局状态中的消息)<ul><li>In a non-FIFO system, either some degree of inhibition or piggybacking of control information on computation messages to capture out-of-sequence messages.(只能通过某种程度的抑制或在计算消息上附加控制消息来捕获乱序消息)</li></ul></li></ul><h3 id="Spezialetti-Kearns-algorithm-For-FIFO-channels"><a href="#Spezialetti-Kearns-algorithm-For-FIFO-channels" class="headerlink" title="Spezialetti-Kearns algorithm(For FIFO channels)"></a>Spezialetti-Kearns algorithm(For FIFO channels)</h3><ul><li>There are two phases in obtaining a global snapshot(两个阶段获取全局快照): <ul><li>locally recording the snapshot at every process(每个进程本地记录快照) </li><li>distributing the resultant global snapshot to all the initiators(每个进程将生成的全局快照分发)</li></ul></li></ul><h4 id="Efficient-snapshot-recording-高效记录快照"><a href="#Efficient-snapshot-recording-高效记录快照" class="headerlink" title="Efficient snapshot recording(高效记录快照)"></a>Efficient snapshot recording(高效记录快照)</h4><ul><li>In the Spezialetti-Kearns algorithm, a markers carries the identifier of the initiator of the algorithm. Each process has a variable master to keep track of the initiator of the algorithm.(marker携带算法发起者的标识符，每个进程都由一个变量主控器来跟踪算法的发起者)</li><li>A key notion used by the optimizations is that of a region in the system. A region encompasses all the processes whose master field contains the identifier of the same initiator.(优化使用的一个关键概念是系统中的区域，每个区域包含的所有进程，都有包含同一启动器标识符的主字段)<ul><li>When the initiator’s identifier in a marker received along a channel is different from the value in the master variable, the sender of the marker lies in a different region.(当随着信道接收到的marker中的启动器的标识符与主变量中的值不同时，标识符的发送者处于不同的区域)</li><li>The identifier of the concurrent initiator is recorded in a local variable id-border-set.(并发启动器的标识符记录在局部变量中的id-border-set)</li></ul></li><li>The state of the channel is recorded just as in the Chandy-Lamport algorithm(including those that cross a border between regions, 包括跨越区域边界的信道).</li><li>Snapshot recording at a process is complete after it has received a marker along each of its channels.(进程的快照记录在沿其每个信道接收到标识符后完成)</li><li>After every process has recorded its snapshot, the system is partitioned into as many regions as the number of concurrent initiations of the algorithm.(每个进程记录其快照后，系统被划分为算法并发启动次数一样多的区域)</li><li>Variable id-border-set at a process contains the identifiers of the neighboring regions.(进程中的变量id-border-set包含相邻区域的标识符)</li></ul><h4 id="Efficient-dissemination-of-the-recorded-snapshot-高效传播记录的快照"><a href="#Efficient-dissemination-of-the-recorded-snapshot-高效传播记录的快照" class="headerlink" title="Efficient dissemination of the recorded snapshot(高效传播记录的快照)"></a>Efficient dissemination of the recorded snapshot(高效传播记录的快照)</h4><ul><li>In the snapshot recording phase, a forest of spanning trees is implicitly created in the system. The initiator of the algorithm is the root of a spanning tree and all processes in its region belong to its spanning tree.(在快照的记录阶段，系统中会隐式地创建一个生成森林，该算法的发起者是生成树的根，其区域中的所有进程都属于其生成树)</li><li>If $p_i$ receives its first marker from $p_j$ then process $p_j$ is the parent of process $p_i$ in the spanning tree.(进程接收到的第一个marker对应的发送者进程是该进程在生成树的父母节点)</li><li>When an intermediate process in a spanning tree has received the recorded states from all its child processes and has recorded the states of all incoming channels, it forwards its locally recorded state and the locally recorded states of all its descendent processes to its parent.(当生成树中的中间进程接收到其所有子进程记录的状态并记录了所有传入信道的状态时，会将其本地记录的状态及其所有子进程的本地记录状态转发给其父进程)</li><li>When the initiator receives the locally recorded states of all its descendents from its children processes, it assembles the snapshot for all the processes in its region and the channels incident on these processes.(当启动器从其子进程接收到其所有子进程的本地记录状态时，会为其区域中的所有进程以及这些进程上的信道进行快照)</li><li>The initiator exchanges the snapshot of its region with the initiators in adjacent regions in rounds.(发起者在轮次中与相邻区域的发起者交换各自区域的快照)</li><li>The message complexity of snapshot recording is $O(e)$ irrespective of the number of concurrent initiations of the algorithm.(快照记录的消息复杂度为$O(e)$，与算法的并发启动次数无关。$e$是传递消息的次数) The message complexity of assembling and disseminating the snapshot is $O(rn^2)$ where $r$ is the number of concurrent initiations.(消息包装和分发的复杂度是$O(rn^2)$，$r$是并发启动的数量，$n$是每个并发启动的子区域包含的进程节点的数量，平方是因为互相交换)</li></ul><h3 id="Lai-Yang-algorithm-For-non-FIFO-channels"><a href="#Lai-Yang-algorithm-For-non-FIFO-channels" class="headerlink" title="Lai-Yang algorithm(For non-FIFO channels)"></a>Lai-Yang algorithm(For non-FIFO channels)</h3><ul><li>The Lai-Yang algorithm fulfills this role of a marker in a non-FIFO system by using a coloring scheme on computation messages(使用涂色方案实现non-FIFO的marker) that works as follows:<ul><li>Every process is initially white(进程是初始化为白色) and turns red while taking a snapshot.(快照后变红) The equivalent of the “Marker Sending Rule” is executed when a process turns red.(变红的时候需要执行Marker Sending Rule)</li><li>Every message sent by a white (red) process is colored white (red).(什么色的进程发送什么颜色的消息)</li><li>Thus, a white (red) message is a message that was sent before (after) the sender of that message recorded its local snapshot.(发送者记录本地快照之前发送的为白色消息，之后发送的为红色消息)</li><li>Every white process takes its snapshot at its convenience, but no later than the instant it receives a red message.(每个白色进程都会在该进程方便的时候拍摄快照，但不能迟于它受到其他进程的红色消息的那一刻，最迟可以在接收到红色消息之后，处理红色消息之前)</li><li>Every white process records a history of all white messages sent or received by it along each channel(每个白色进程都记录了它沿每个信道发送或接收的所有白色消息的历史记录)</li><li>When a process turns red, it sends these histories along with its snapshot to the initiator process that collects the global snapshot(当一个进程变为红色时，会将这些历史记录及其快照发送给收集全局快照的启动器进程initiator)</li><li>The initiator process evaluates $transit(LS_i, LS_j)$ to compute the state of a channel $C_{ij}$ as given below:<ul><li>$SC_{ij}=$ white messages sent by $p_i$ on $C_{ij}$ — white messages received by $p_j$ on $C_{ij}=\{send(m_{ij})|send(m_{ij})\in LS_i\} - \{rec(m_{ij})|rec(m_{ij})\in LS_j\}$(得到的便是在信道中的消息)</li></ul></li></ul></li></ul><h3 id="Mattern’s-algorithm-For-non-FIFO-channels"><a href="#Mattern’s-algorithm-For-non-FIFO-channels" class="headerlink" title="Mattern’s algorithm(For non-FIFO channels)"></a>Mattern’s algorithm(For non-FIFO channels)</h3><h4 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h4><ul><li>Mattern’s algorithm is based on vector clocks and assumes a single initiator process(该算法基于向量时钟和假设一个启动器进程) and works as follows:<ul><li>The initiator “ticks” its local clock and selects a future vector time $s$ at which it would like a global snapshot to be recorded(启动器根据本地时钟选定一个未来的向量时间$s$，该时间记录全局快照). It then broadcasts this time $s$ and freezes all activity until it receives all acknowledgements of the receipt of this broadcast.(该进程将整个未来向量时间$s$广播出去，然后阻塞该进程，直到收到广播出去的所有确认回复为止)</li><li>When a process receives the broadcast, it remembers the value $s$ and returns an acknowledgement to the initiator.(接收到广播的进程，会记住该广播来的值$s$并向发送者，即启动器返回确认)</li><li>After having received an acknowledgement from every process, the initiator increases its vector clock to $s$ and broadcasts a dummy message to all processes.(启动器收到所有的进程的确认回复后，启动器将自己的虚拟时钟修改为$s$，然后广播一个dummy消息)</li><li>The receipt of this dummy message forces each recipient to increase its clock to a value $\geq s$ if not already $\geq s$.(接收到dummy消息的进程也把自己的时钟修改为$s$)</li><li>Each process takes a local snapshot and sends it to the initiator when (just before) its clock increases from a value less than $s$ to a value $\geq s$.(每个进程在将少于$s$的时钟值修改为$s$前，拍摄一个本地快照，并将快照发送给启动器)</li><li>The state of $C_{ij}$ is all messages sent along $C_{ij}$, whose timestamp is smaller than $s$ and which are received by $p_j$ after recording $LS_j$.(信道的状态是时间戳小于s，且在接收进程拍摄快照后才接收的所有消息)</li></ul></li></ul><h4 id="Termination-detection-scheme-如何判断算法结束"><a href="#Termination-detection-scheme-如何判断算法结束" class="headerlink" title="Termination detection scheme(如何判断算法结束)"></a>Termination detection scheme(如何判断算法结束)</h4><ul><li><p>A termination detection scheme for non-FIFO channels is required to detect that no white messages are in transit(算法结束的条件是没有白消息在传输)</p></li><li><p>First Method</p><ul><li>Each process i keeps a counter $cntr_i$ that indicates the difference between the number of white messages it has sent and received before recording its snapshot.(进程都保持一个计数器$cntr_i$，用来表示在拍摄快照之前已经发送的白消息和已经接收到的白消息的差值)</li><li>It reports this value to the initiator process along with its snapshot and forwards all white messages, it receives henceforth, to the initiator.(将此值与其快照一起报告给启动器进程，并在此后将后续收到的所有白色消息转发给启动器)</li><li>Snapshot collection terminates when the initiator has received $\Sigma_i cntr_i$ number of forwarded white messages(当启动器收到的白消息的数量和计数器$cntr_i$的总和相等时，算法结束)</li></ul></li><li><p>Second Method</p><ul><li>Each red message sent by a process carries a piggybacked value of the number of white messages sent on that channel before the local state recording.(进程发送的每条红色消息都附带了在本地状态记录之前在该信道上发送的白色消息数量的值)</li><li>Each process keeps a counter for the number of white messages received on each channel.(每个进程保持了一个计数器，用来记录每个信道上接收到的白色消息的数量，因为是通常是互相的，所以接收的应该等于发出去的)</li><li>A process can detect termination of recording the states of incoming channels when it receives as many white messages on each channel as the value piggybacked on red messages received on that channel.(当一个进程在每个信道上接收到的白色消息与在该信道上接收的红色消息上附带的值一样多时，可以检测到记录传入信道状态的终止)</li></ul></li></ul><h3 id="Snapshots-in-a-causal-delivery-system"><a href="#Snapshots-in-a-causal-delivery-system" class="headerlink" title="Snapshots in a causal delivery system"></a>Snapshots in a causal delivery system</h3><ul><li>The causal message delivery property CO provides a built-in message synchronization to control and computation messages.(因果消息传递属性CO为控制和计算消息提供了内置的消息同步，即有因果序)<ul><li>assume that the underlying system supports causal message delivery</li></ul></li><li>In both these algorithms recording of process state is identical(记录过程状态的算法是相同的) and proceed as follows :<ul><li>An initiator process broadcasts a token, denoted as token, to every process including itself.(启动器进程向每个进程广播一个token)</li><li>Let the copy of the token received by process $p_i$ be denoted $token_i$.(进程接收到token后都拷贝一个token副本)</li><li>A process $p_i$ records its local snapshot $LS_i$ when it receives $token_i$ and sends the recorded snapshot to the initiator.(接收到$token_i$时拍摄其本地快照$LS_i$，并将该记录的快照发送给启动器initiator)</li><li>The algorithm terminates when the initiator receives the snapshot recorded by each process.(当启动器接收到每个进程记录的快照后算法终止)</li></ul></li></ul><h4 id="Correctness-1"><a href="#Correctness-1" class="headerlink" title="Correctness"></a>Correctness</h4><ul><li>For any two processes $p_i$ and $p_j$, the following property is satisfied:<ul><li>$send(m_{ij})\notin LS_i \Rightarrow rec(m_{ij})\notin LS_j$(消息$m_{ij}$不在进程$p_i$的快照$LS_i$中，能推断出接收消息$m_{ij}$也不在进程$p_j$的快照$LS_j$中)</li><li>This is due to the causal ordering property of the underlying system as explained next<ul><li>Let a message $m_{ij}$ be such that $rec(token_i)\rightarrow send(m_{ij})$($send(m_{ij})\notin LS_i$)</li><li>Then $send(token_j)\rightarrow send(m_{ij})$ and the underlying causal ordering property ensures that $rec(token_j)$, at which instant process $p_j$ records $LS_j$, happens before $rec(m_{ij})$(进程$p_i$先发送$token_j$给进程$p_j$，然后才发送$m_{ij}$，所以进程$p_j$先收到$token_j$拍摄快照$LS_j$，然后才接收$m_{ij}$?)</li><li>Thus, $m_{ij}$ whose send is not recorded in $LS_i$, is not recorded as received in $LS_j$</li></ul></li></ul></li></ul><h4 id="Acharya-Badrinath-algorithm"><a href="#Acharya-Badrinath-algorithm" class="headerlink" title="Acharya-Badrinath algorithm"></a>Acharya-Badrinath algorithm</h4><ul><li>Each process $p_i$ maintains arrays $SENT_i[1, …, N]$ and $RECD_i[1, …, N]$(进程$p_i$维护两个数组，一个发送数组，一个接收数组)<ul><li>$SENT_i[j]$ is the number of messages sent by process $p_i$ to process $p_j$</li><li>$RECD_i[j]$ is the number of messages received by process $p_i$ from process $p_j$</li></ul></li><li>Channel states are recorded as follows:<ul><li>When a process $p_i$ records its local snapshot $LS_i$ on the receipt of $token_i$, it includes arrays $RECD_i$ and $SENT_i$ in its local state before sending the snapshot to the initiator(发送给启动器的快照包括发送数组和接收数组)</li></ul></li><li>When the algorithm terminates, the initiator determines the state of channels as follows:(判断算法终止)<ul><li>The state of each channel from the initiator to each process is empty(从启动器到每个进程的每个信道都是空的，即没有消息在信道)<ul><li>The state of channel from process $p_i$ to process $p_j$ is the set of messages whose sequence numbers are given by $\{RECD_j[i]+1, …, SENT_i[j]\}$(在两个进程之间信道的消息是根据发送和接收数组确定的，发送数组里有而接收数组里没有的则在信道)</li></ul></li></ul></li><li>Complexity<ul><li>This algorithm requires $2n$ messages and 2 time units for recording and assembling the snapshot, where one time unit is required for the delivery of a message.(遍历所有的各个进程的传入传出信道)</li><li>If the contents of messages in channels state are required, the algorithm requires $2n$$ messages and 2 time units additionally.(就是还要遍历所有各个进程的传入传出信道)</li></ul></li></ul><h4 id="Alagar-Venkatesan-algorithm"><a href="#Alagar-Venkatesan-algorithm" class="headerlink" title="Alagar-Venkatesan algorithm"></a>Alagar-Venkatesan algorithm</h4><ul><li>A message is referred to as old if the send of the message causally precedes the send of the token, Otherwise, the message is referred to as new.(在发送token之前的消息为旧消息，否则为新消息)</li><li>When a process receives the token, it takes its snapshot(收到token后拍摄快照), initializes the state of all channels to empty(将所有的信道状态初始化为空), and returns <code>Done</code> message to the initiator(将Done消息返回给启动器). </li><li>Now onwards, a process includes a message received on a channel in the channel state only if it is an old message.(当消息是旧消息的时候，进程才会接收到信道上的消息，因为新消息是token发送之后的消息，而收到token后将信道都变为空，即新消息都丢了？)</li><li>After the initiator has received <code>Done</code> message from all processes, it broadcasts a Terminate message.(启动器接收到所有进程的Done消息后，广播一个终止消息)<ul><li>A process stops the snapshot algorithm after receiving a Terminate message.(非启动器进程则在收到终止消息后终止快照算法)</li></ul></li></ul><h3 id="Necessary-and-sufficient-conditions-for-consistent-global-snapshots"><a href="#Necessary-and-sufficient-conditions-for-consistent-global-snapshots" class="headerlink" title="Necessary and sufficient conditions for consistent global snapshots"></a>Necessary and sufficient conditions for consistent global snapshots</h3><h4 id="Consistent-global-snapshots"><a href="#Consistent-global-snapshots" class="headerlink" title="Consistent global snapshots"></a>Consistent global snapshots</h4><ul><li>许多应用要求在执行或post martem期间定期记录和分析本地进程状态</li><li>进程在执行过程中保存的中间状态称为进程的本地检查点(local checkpoint)</li><li>一致的快照由一组同时发生或可能同时发生的本地状态集合组成</li><li>进程异步保存checkpoint，假设每个进程在执行前保存一个初始checkpoint，在执行完后保存一个虚拟checkpoint(virtual)<ul><li>进程$p_p$第i个checkpoint表示为$C_{p, i}$</li><li>进程$p_p$的第i个checkpoint间隔(interval)包括它的第i-1个checkpoint和第i个checkpoint之间执行的所有计算(包括了第i-1个checkpoint期间的计算但不包括第i个checkpoint)</li></ul></li><li>即使两个本地checkpoint没有存在因果路径，也可能不属于一致的全局快照</li></ul><h4 id="zigzag-path"><a href="#zigzag-path" class="headerlink" title="zigzag path"></a>zigzag path</h4><ul><li><p>A zigzag path exists from a checkpoint $C_{x, i}$ to a checkpoint $C_{y, j}$ iff there exists messages $m_1, m_2, …, m_n(n\geq 1)$:</p><ul><li>$m_1$ is sent by process $p_x$ after $C_{x, i}$(第一条消息由zigzag路径起点快照发送)</li><li>If $m_k(1\leq k\leq n)$ is received by process $p_z$, then $m_{k+1}$ is sent by $p_z$ in the same or a later checkpoint interval(although $m_{k+1}$ may be sent before or after $m_k$ is received)(通常来讲是消息有顺序。但也允许进程在接收到因果路径中的前一个消息之前发送消息，只要在同一个checkpoint间隔。后发送的消息在后一个间隔则不需考虑上述情况)</li><li>$m_n$ is received by process $p_y$ before $C_{y, j}$(最后一条消息由zigzag路径终点快照接收)</li><li>A zigzag path between two checkpoints is a causal path(因果路径)<ul><li>A causal path exists from a checkpoint A to another checkpoint B iff there is chain of messages starting after A and ending before B such that each message is sent after the previous one in the chain is received(因果路径消息必须有序)</li><li>a causal path is always a zigzag path, but a zigzag path need not be a causal path.</li></ul></li><li>如下图的进程$p_1$的快照$C_{1,1}$和进程$p_2$的快照$C_{2,2}$存在因果路径，zigzag路径允许进程$p_2$在接收消息$m_3$之前发送消息$m_4$给进程$p_3$，由快照$C_{3,2}$记录。因此$C_{1,1}$和$C_{3,2}$之间存在zigzag path<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/zigzag-path.png" alt="img"></li></ul></li><li><p>特殊的zigzag path—— zigzag cycle</p><ul><li>A checkpoint $C$ is involved in a zigzag cycle iff there is a zigzag path from $C$ to itself(起点和终点都是自己的zigzag路径)</li><li>如下图的进程$p_2$的快照$C_{2, 1}$，起始消息$m_2$给$p_1$，$p_1$在同一个checkpoint间隙早些时候发送消息$m_1$给$p_2$，终点也是$C_{2, 1}$</li><li>a causal path never forms a cycle.(因果路径不存在环)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/zigzag-cycle.png" alt="img"></li></ul></li></ul><h4 id="Necessary-and-sufficient-conditions"><a href="#Necessary-and-sufficient-conditions" class="headerlink" title="Necessary and sufficient conditions"></a>Necessary and sufficient conditions</h4><ul><li>if no zigzag path (or cycle) exists between any two checkpoints from a set S of checkpoints, then a consistent snapshot can be formed that includes the set S of checkpoints and vice versa.(checkpoint集合中任意两个checkpoint之间都没有zigzag路径的话，则整个集合的checkpoint可以构成consistent global snapshot，反之亦然)<ul><li>The absence of a causal path between checkpoints in a snapshot corresponds to the necessary condition for a consistent snapshot(必要条件：如果一个snapshot是consistent的，则其中的检查点之间没有因果路径，zigzag路径是特殊的因果路径，没有因果路径自然也没有zigzag路径)</li><li>The absence of a zigzag path between checkpoints in a snapshot corresponds to the necessary and sufficient conditions for a consistent snapshot.(充要条件)</li><li>A set of checkpoints S can be extended to a consistent snapshot if and only if no checkpoint in S has a zigzag path to any other checkpoint in S.</li><li>A checkpoint can be a part of a consistent snapshot if and only if it is not invloved in a Z-cycle(要在consistent的snapshot中加入一个checkpoint，则当且仅当该checkpoint和snapshot中的任意一个checkpoint之间不存在zigzag路径，而其自身不存在zigzag回路)</li></ul></li></ul><h4 id="Finding-consistent-global-snapshots-in-a-distributed-computation"><a href="#Finding-consistent-global-snapshots-in-a-distributed-computation" class="headerlink" title="Finding consistent global snapshots in a distributed computation"></a>Finding consistent global snapshots in a distributed computation</h4><ul><li>Discuss how individual local checkpoints can be combined with those from other processes to form global snapshots that are consistent.</li><li>Let $A, B$ be individual checkpoints and $R, S$ be sets of checkpoints. Let $\leadsto$ be a relation defined over checkpoints and sets of checkpoints such that:<ul><li>$A\leadsto B$ iff a Z-path exists from $A$ to $B$</li><li>$A\leadsto S$ iff a Z-path exists from $A$ to some member of $S$</li><li>$S\leadsto A$ iff a Z-path exists from some member of $S$ to $A$</li><li>$R\leadsto S$ iff a Z-path exists from some member of $R$ to some member of $S$</li><li>$S\nrightarrow S$ defines that no Z-path(including Z-cycle) exists from any member of $S$ to any other member of $S$ and implies that checkpoints in $S$ are all from different processes</li></ul></li><li>A set of checkpoints $S$ can be extended to a consistent global snapshot iff $S\nrightarrow S$<ul><li>A checkpoint $C$ can be part of a consistent global snapshot iff it is not involved in a Z-cycle</li><li>A set of checkpoints $S$ is a consistent global snapshot iff $S\nrightarrow S$ and $|S|=N$, where $N$ is the number of processes</li></ul></li><li>Given a set $S$ of checkpoints such that $S\nrightarrow S$, what checkpoints from other processes can be combined with $S$ to build a consistent global snapshot?<ul><li>None of the checkpoints that have a Z-path to or from any of the checkpoints in $S$ can be used</li><li>Only those checkpoints that have no Z-paths to or from any of the checkpoints in $S$ are candidates for inclusion in the consistent snapshot<ul><li>The set of all such candidates is called as the Z-zone of $S$ and </li><li>All checkpoints that have no causal path to or from any checkpoint in $S$ is called as the C-cone of $S$ </li><li>the Z-cone of $S$ is a subset of the C-cone of $S$ for an arbitrary $S$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Z-cone%20and%20S-cone.png" alt="img"></li></ul></li><li>Although candidates for building a consistent snapshot from $S$ must lie in the Z-cone of $S$, not all checkpoints in the Z-cone can form a consistent snapshot with $S$. (在Z-cone里的检查点不一定能与$S$构成一致性的快照，但能与$S$构成一致性快照的检查点一定在Z-cone)<ul><li>If a checkpoint in the Z-cone is involved in a Z-cycle, then it cannot be part of a consistent snapshot.(Z-cone里可能存在zigzag环)</li><li>Let $S$ be a set of checkpoints such that $S\nrightarrow S$. Then, for each process $p_q$, the set $S_{useful}^q$ is defined as <ul><li>$S_{useful}^q=\{C_{q, i} | (S\nrightarrow C_{q, i}\bigwedge(C_{q, i}\nrightarrow S)\bigwedge(C_{q, i}\nrightarrow C_{q, i})\}$</li><li>$\displaystyle S_{useful}=\bigcup_q S_{useful}^q$(各个进程可与$S$构成一致性快照的检查点的并集)</li></ul></li><li>Let $S$ be a set of checkpoints such that $S\nrightarrow S$, let $C_{q, i}$ be any checkpoint of process $p_q$ such that $C_{q, i}\notin S$. Then $S\bigcup \{C_{q, i}\}$ can be extended to a consistent snapshot iff $C_{q, i}\in S_{useful}$($S_{useful}$是所有可以与$S$构成一致性快照的检查点)</li></ul></li><li>Although none of the checkpoints in $S_{useful}$ has a Z-path to or from any checkpoint in $S$, Z-paths may exist between members of $S_{useful}$($S_{useful}$里的检查点之间可能会存在Z-path)</li><li>One final constraint is placed on the set $T$ we choose from $S_{useful}$ to build a consistent snapshot from $S$: checkpoints in $T$ must have no Z-paths between them. Furthermore, since $S\nrightarrow S$, at least one such $T$ must exist(即使只有一个元素)<ul><li>Let $S$ be a set of checkpoints such that $S\nrightarrow S$ and let $T$ be any set of checkpoints such that $S\bigcap T=\empty$. Then $S\bigcup T$ is a consistent global snapshot iff <ul><li>$T\subseteq S_{useful}$</li><li>$T\nrightarrow T$</li><li>$|S\bigcup T|=N$</li></ul></li></ul></li></ul></li></ul><h4 id="Manivannan-Netzer-Singhal-algorithm-for-enumerating-consistent-snapshots-算法伪代码"><a href="#Manivannan-Netzer-Singhal-algorithm-for-enumerating-consistent-snapshots-算法伪代码" class="headerlink" title="Manivannan-Netzer-Singhal algorithm for enumerating consistent snapshots(算法伪代码)"></a>Manivannan-Netzer-Singhal algorithm for enumerating consistent snapshots(算法伪代码)</h4><ul><li>compute all consistent snapshots that include a given set a set of checkpoints $S$</li></ul><p>ComputeAllCgs($S$) {  // 计算一个完整的一致性快照<br>  $\quad$let $G=\empty$<br>  $\quad$if $S\nrightarrow S$ then<br>  $\quad<script type="math/tex">\quad$let `AllProcs` be the set of all processes not represented in $S$ // 在没有加入一致性快照的进程里选择  $\quad</script>\quad$<code>ComputeAllCgsForm</code>($S$, $AllProcs$)<br>  $\quad$return $G$<br>}<br>ComputeAllCgsForm($T$, $ProcSet$) {<br>  $\quad$if $(ProcSet=\empty)$ then   // 已经是完整的一致性快照了<br>  $\quad<script type="math/tex">\quad</script>G=G\bigcup\{T\}$<br>  $\quad$else //否则递归查找<br>  $\quad<script type="math/tex">\quad$let $p_q$ be any process in $ProcSet$  $\quad</script>\quad$for each checkpoint $C\in T_{useful}^q$ do  // 首先还是得保证检查点是可用的<br>  $\quad<script type="math/tex">\quad</script>\quad$<code>ComputeAllCgsForm</code>($T\bigcup\{C\}$, $ProcSet\setminus \{p_q\}$)<br>}</p><ul><li>Let $S$ be a set of checkpoints and $G$ be the set returned by <code>ComputeAllCgs</code>. If $S\nrightarrow S$, then $T\in G$ iff $T$ is a consistent snapshot containing $S$. That is, $G$ contains exactly the consistent snapshots that contain $S$</li></ul><h4 id="R-graph-to-determine-the-existence-of-Z-paths-between-checkpoints"><a href="#R-graph-to-determine-the-existence-of-Z-paths-between-checkpoints" class="headerlink" title="R-graph to determine the existence of Z-paths between checkpoints"></a>R-graph to determine the existence of Z-paths between checkpoints</h4><ul><li>Discuss a method for determining the existence of Z-paths between checkpoints in a distributed computation that has terminated or has stopped execution, using the rollback-dependency graph (R-graph).</li><li>The rollback-dependency graph of a distributed computation is a directed graph $G=(V, E)$, where the vertices $V$ are the checkpoint of the distributed computation and an edge $(C_{p, i}, C_{q, j})$ from checkpoint $C_{p, i}$ to checkpoint $C_{q, j}$ belongs to $E$ if<ul><li>$p=q$ and $j=i+1$</li><li>$p\neq q$ and a message $m$ sent from the $i^{th}$ checkpoint interval of $p_p$ is received by $p_q$ in its $j^{th}$ checkpoint interval($i, j\gt 0$)</li><li>there is a path from $C$ to $D$ in the R-graph by $C\rightsquigarrow^{rd} D$, only denotes the existence of a path, does not specify any particular path</li></ul></li><li>Let $G=(V, E)$ be the R-graph of a distributed computation. For any two checkpoints $C_p^i$ and $C_q^j$, $C_p^i\rightsquigarrow C_q^j$ iff:<ul><li>$p=q$ and $i\lt j$(因果路径)</li><li>$ C_p^{i+1}\rightsquigarrow^{rd} C_q^j$ in $G$(note that in this case $p$ could still be equal to $q$)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Rgraph.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;全局状态及快照算法&quot;&gt;&lt;a href=&quot;#全局状态及快照算法&quot; class=&quot;headerlink&quot; title=&quot;全局状态及快照算法&quot;&gt;&lt;/a&gt;全局状态及快照算法&lt;/h1&gt;&lt;h2 id=&quot;System-model&quot;&gt;&lt;a href=&quot;#System-model&quot;</summary>
      
    
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>高性能处理器的并行计算技术</title>
    <link href="http://zjn-astonishe.github.io/2024/10/23/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-23-%E9%AB%98%E6%80%A7%E8%83%BD%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF/"/>
    <id>http://zjn-astonishe.github.io/2024/10/23/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-23-%E9%AB%98%E6%80%A7%E8%83%BD%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF/</id>
    <published>2024-10-23T08:00:38.000Z</published>
    <updated>2025-04-14T03:05:11.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高性能处理器的并行计算技术"><a href="#高性能处理器的并行计算技术" class="headerlink" title="高性能处理器的并行计算技术"></a>高性能处理器的并行计算技术</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="现代处理器的特征"><a href="#现代处理器的特征" class="headerlink" title="现代处理器的特征"></a>现代处理器的特征</h3><ul><li>深度流水线<ul><li>十多个功能段的流水线</li></ul></li><li>多功能部件<ul><li>多个浮点乘法器、 加法器等</li></ul></li><li>一个时钟周期能够流出多条指令</li><li>发掘指令之间的并行性已成为现代处理器性能优化的重要方面</li></ul><h3 id="指令级并行的技术"><a href="#指令级并行的技术" class="headerlink" title="指令级并行的技术"></a>指令级并行的技术</h3><ul><li>软件技术: 循环展开、VLIW…</li><li>硬件技术: 分支预测、推测执行、动态调度</li></ul><h3 id="指令级并行存在的挑战"><a href="#指令级并行存在的挑战" class="headerlink" title="指令级并行存在的挑战"></a>指令级并行存在的挑战</h3><ul><li><p>指令之间存在一定的竞争或依赖关系</p><h4 id="结构冒险"><a href="#结构冒险" class="headerlink" title="结构冒险"></a>结构冒险</h4></li><li><p>多条指令争用同一个功能部件<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%BB%93%E6%9E%84%E5%86%92%E9%99%A9.png" alt="img"></p></li><li>如Cycle4中的访存和取指冲突<ul><li>因为数据和指令都存储在同一个内存中，而内存只有一个端口，不能被同时访问</li><li>可通过将L1缓存改造成数据Cache和指令Cache，即将指令和数据分开存储来解决内存争用</li><li>可通过分时解决争用问题，如上升沿读取指令，下降沿读取数据</li><li>也可通过延迟一个周期再执行指令避开争用情况<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%BB%93%E6%9E%84%E5%86%92%E9%99%A9%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt="img"></li></ul></li></ul><h4 id="数据冒险"><a href="#数据冒险" class="headerlink" title="数据冒险"></a>数据冒险</h4><ul><li>数据之间存在真假依赖</li><li>数据相关类型<ul><li>Read After Write(RAW, 写后读): 真相关，上一个指令的结果作为下一个指令的参数，会造成冒险</li><li>Write After Read(WAR, 读后写): 名字相关、反相关，上一个指令的参数位置是下一个指令所输出的位置，不会造成冒险</li><li>Write After Write(WAW, 写后写): 名字相关、输出相关，上一个指令输出的位置是下一个指令输出的位置，不会造成冒险(只要中间不会有其他操作)</li><li>Read After Read(RAR, 读后读)</li></ul></li><li>数据相关的特性<ul><li>传递性: i与j相关，j与h相关，则i与h相关</li><li>不一定导致数据冒险(Hazard)<ul><li>数据相关是程序的属性</li><li>但是否发生冒险还取决于处理器的体系结构和功能部件，只要<strong>合理安排功能部件</strong>就能避免冒险</li><li>两条存在相关的指令，在指令序列中如果距离较远，也不会造成冒险(所以<strong>调度指令序列</strong>也能避免冒险)</li></ul></li><li>给出了可发掘的指令级并行的上限<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%86%92%E9%99%A9.png" alt="img"></li></ul></li><li>数据相关的解决方法<ul><li>转发(Forward): 将计算出的结果尽早发送到等待该结果的部件<ul><li>例如ALU的运算结果先不访存和写回，直接传递给下一条指令作为运算参数<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%86%92%E9%99%A9%E5%AF%B9alu%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png" alt="img"></li><li>但不能解决所有问题，例如对于load来说，结果在访存阶段结束才能拿到，但下一条指令在阶段结束前就需要该结果作为参数，只能辅以延时等待解决<br><img src="https://github.com/zjn-astonishe/image/blob/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%86%92%E9%99%A9%E5%AF%B9load%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.png?raw=true" alt="img"></li></ul></li></ul></li></ul><h4 id="控制冒险"><a href="#控制冒险" class="headerlink" title="控制冒险"></a>控制冒险</h4><ul><li>分支语句执行具有不确定性，无法确定后续执行的指令(跳还是不跳)</li><li>如果一条指令是否执行依赖于一条分支指令的执行结果，则不能把这条指令提到分支指令之前(否则会不受控于分支结果)</li><li>如果一条指令与一个分支指令没有控制相关，则不能把这条指令放在分支指令之后(否则会使这条指令受分支结果控制)</li><li>控制冒险在很大程度上限制了指令级并行(决定上限)<ul><li>对于典型的MIPS程序，分支频率在15%~25%，平均3~6条指令就有一个分支出现<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%8E%A7%E5%88%B6%E5%86%92%E9%99%A9.png" alt="img"></li></ul></li></ul><h3 id="发掘指令级并行的目的"><a href="#发掘指令级并行的目的" class="headerlink" title="发掘指令级并行的目的"></a>发掘指令级并行的目的</h3><ul><li>降低程序执行的平均CPI<ul><li>$Pipeline\quad CPI = Ideal\quad pipeline\quad CPI + Structural\quad stalls + Data\quad hazard\quad stalls + Control\quad stalls$</li><li>最小化冒险发生的可能性</li></ul></li></ul><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><figure class="highlight mips"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">I1  FDIV.D  f6,  f6,  f4</span><br><span class="line">I2  FLD     f2,  <span class="number">45</span>(x3)</span><br><span class="line">I3  FMUL.D  f0,  f2,  f4</span><br><span class="line">I4  FDIV.D  f8,  f6,  f2</span><br><span class="line">I5  FSUB.D  f10, f0,  f6</span><br><span class="line">I6  FADD.D  f6,  f8,  f2</span><br></pre></td></tr></table></figure><h4 id="数据相关分析"><a href="#数据相关分析" class="headerlink" title="数据相关分析"></a>数据相关分析</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E5%AE%9E%E4%BE%8B.png" alt="img"></p><ul><li>红色代表真相关<ul><li>指令I1与指令I4、I5存在写后读(f6)真相关</li><li>指令I2与指令I3、I4、I6存在写后读(f2)真相关</li><li>指令I3与指令I5存在写后读(f0)真相关</li><li>指令I4与指令I6存在写后读(f8)真相关</li></ul></li><li>蓝色代表名字相关、反相关<ul><li>指令I4、I5与指令I6存在读后写(f6)反相关</li></ul></li><li>橙色代表输出相关<ul><li>指令I1与指令I6存在写后写(f6)输出相关</li></ul></li></ul><h4 id="执行顺序分析"><a href="#执行顺序分析" class="headerlink" title="执行顺序分析"></a>执行顺序分析</h4><ul><li><p>标准五段流水线的执行顺序</p><ul><li><p>顺序执行: </p><p>clk|1|2|3|4|5|6|7|8|9|10|11<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>in/out|I1|I2|stall|stall|stall|I3|I4|stall|stall|I5|I6</p></li><li><p>乱序执行: </p><p>clk|1|2|3|4|5|6|7|8|9|10|<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>in/out|I2|I1|stall|stall|I3|I4|stall|stall|I5|I6</p></li></ul></li><li><p>假定各种指令具有额外延迟</p><ul><li>相对标准的五段流水线需要额外时钟数，即如果下一条指令想要使用本指令的结果，必须延后几个周期发射</li></ul><p>Ins|I1|I2|I3|I4|I5|I6<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>Latency|4|1|3|4|1|1</p></li><li><p>按序发射/按序执行</p><p>clk|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>in/out|I1|I2|stall|stall|I1_c|I2_c|I3|I4|stall|I3_c|I5|I4_c|I6|I5_c|I6_c</p></li><li><p>按序发射/乱序执行</p><p>clk|1|2|3|4|5|6|7|8|9|10|11|12|<br>:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:<br>in/out|I1|I2|I2_c|I3|I1_c|I4|I3_c|I5|I5_c|I4_c|I6|I6_c</p></li></ul><h2 id="基于循环展开的指令调度"><a href="#基于循环展开的指令调度" class="headerlink" title="基于循环展开的指令调度"></a>基于循环展开的指令调度</h2><ul><li><p>基本假设</p><ul><li>五段流水线</li><li>分支延迟为1个额外时钟</li><li>定点计算的额外延迟为0</li><li>浮点单元足够多，能够充分流水</li></ul><p>产生结果指令|等待结果指令|延迟<br>:-:|:-:|:-:<br>FP ALU OP|Another FP ALU OP|3<br>FP ALU OP|Store double|2<br>Load double|FP ALU OP|1<br>Load double|Store double|0<br>FP ALU OP|Load double|1<br>Store double|ADR ALU OP|0<br>Load double|ADR ALU OP|0</p></li></ul><h3 id="循环展开的优势"><a href="#循环展开的优势" class="headerlink" title="循环展开的优势"></a>循环展开的优势</h3><ul><li>降低循环条件判断在代码中的比例，因为：<ul><li>分支指令本身会带来额外的延迟</li><li>分支指令会导致控制冒险</li></ul></li><li>提升循环体内可发掘的指令级并行性<ul><li>循环体内指令变多，能够充分占据功能部件</li><li>乱序执行调度的余地更大</li></ul></li></ul><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i=<span class="number">999</span>; i&gt;=<span class="number">0</span>; i=i<span class="number">-1</span>)</span><br><span class="line">    x[i] = x[i] + s;</span><br><span class="line"></span><br><span class="line">Loop:   fld     f0, <span class="number">0</span>(x1)       <span class="comment">// f0=array element</span></span><br><span class="line">        fadd.d  f4, f0, f2      <span class="comment">// add scalar in f2</span></span><br><span class="line">        fsd     f4, <span class="number">0</span>(x1)       <span class="comment">// store result</span></span><br><span class="line">        addi    x1, x1, <span class="number">-8</span>      <span class="comment">// decrement pointer 8B(per DW)</span></span><br><span class="line">        bne     x1, x2, Loop    <span class="comment">// branch x1 != x2</span></span><br></pre></td></tr></table></figure><h4 id="未调度的执行"><a href="#未调度的执行" class="headerlink" title="未调度的执行"></a>未调度的执行</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Clock</th><th style="text-align:center">Unscheduled</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">fld     f0, 0(x1) </td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Stall</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">fadd.d  f4, f0, f2    // Load后ALU，延迟为1</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Stall</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">Stall</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">fsd     f4, 0(x1)     // ALU后Store，延迟为2</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">addi    x1, x1, -8    // Store后ADR ALU，延迟为0</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">Stall</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">bne     x1, x2, Loop  // ALU后Load，延迟为1</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">Stall                // 分支延迟为1</td></tr></tbody></table></div><ul><li>每次循环需要10个时钟周期</li><li>有效负荷较少： 只有fld、 fadd、 fsd三条指令<ul><li>有效负荷占30%</li><li>50%的时钟停顿(优化方案是尽可能消除停顿)</li><li>20%的时钟用于循环控制</li></ul></li></ul><h4 id="调度执行"><a href="#调度执行" class="headerlink" title="调度执行"></a>调度执行</h4><div class="table-container"><table><thead><tr><th style="text-align:center">Clock</th><th style="text-align:center">Unscheduled</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">fld     f0, 0(x1) </td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">addi    x1, x1, -8    // Load后ADR ALU，延迟为0</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">fadd.d  f4, f0, f2    // Load后ALU，延迟为1</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Stall</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">bne     x1, x2, Loop  // 如果放在上一个Stall，下一次循环的第一条指令会和本次循环最后一条指令冲突？</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">fsd     f4, 8(x1)     // ALU后Store，延迟为2</td></tr></tbody></table></div><ul><li>每次循环需要6个时钟周期<ul><li>加速比： 10/6=1.67</li></ul></li><li>有效负荷仍然较少： 只有fld、 fadd、 fsd三条指令<ul><li>有效负荷占50%(优化方案是尽可能增加有效负荷——循环展开，增加一次循环的操作数)</li><li>一个时钟停顿， 占16.67%</li><li>两个时钟用于循环控制， 占33.33%(优化方案是减少循环控制开销——循环展开，减少循环的次数)</li></ul></li></ul><h4 id="未调度的循环展开"><a href="#未调度的循环展开" class="headerlink" title="未调度的循环展开"></a>未调度的循环展开</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Loop:   fld f0,0(x1)        // 1</span><br><span class="line">        fadd.d f4,f0,f2     // 2,3</span><br><span class="line">        fsd f4,0(x1)        // 4,5,6</span><br><span class="line">        fld f6,8(x1)        // 7</span><br><span class="line">        fadd.d f8,f6,f2     // 8,9</span><br><span class="line">        fsd f8,8(x1)        // 10,11,12</span><br><span class="line">        fld f10,16(x1)      // 13</span><br><span class="line">        fadd.d f12,f10,f2   // 14,15</span><br><span class="line">        fsd f12,16(x1)      // 16,17,18</span><br><span class="line">        fld f14,24(x1)      // 19</span><br><span class="line">        fadd.d f16,f14,f2   // 20,21</span><br><span class="line">        fsd f16,24(x1)      // 22,23,24</span><br><span class="line">        addi x1,x1,32       // 25</span><br><span class="line">        bne x1,x2,Loop      // 26,27</span><br><span class="line">        Stall               // 28</span><br></pre></td></tr></table></figure><ul><li>展开4次循环，每次循环需要28个时钟周期<ul><li>发射14条指令<ul><li>12条指令的有效负荷(42.9%)</li><li>2个时钟周期用于循环控制(7.1%)</li><li>14个时钟周期的停顿(50%)</li></ul></li><li>但需要更多的寄存器</li><li>平均每个原始循环7个时钟(28/4)</li></ul></li></ul><h4 id="调度的循环展开"><a href="#调度的循环展开" class="headerlink" title="调度的循环展开"></a>调度的循环展开</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Loop:   fld f0,0(x1)        // 1</span><br><span class="line">        fld f6,8(x1)        // 2</span><br><span class="line">        fld f10,16(x1)      // 3</span><br><span class="line">        fld f14,24(x1)      // 4</span><br><span class="line">        fadd.d f4,f0,f2     // 5</span><br><span class="line">        fadd.d f8,f6,f2     // 6</span><br><span class="line">        fadd.d f12,f0,f2    // 7</span><br><span class="line">        fadd.d f16,f14,f2   // 8</span><br><span class="line">        fsd f4,0(x1)        // 9</span><br><span class="line">        fsd f8,8(x1)        // 10</span><br><span class="line">        addi x1,x1,32       // 11</span><br><span class="line">        fsd f12,-16(x1)     // 12</span><br><span class="line">        bne x1,x2,Loop      // 13</span><br><span class="line">        fsd f16,-8(x1)      // 14        </span><br></pre></td></tr></table></figure><ul><li>每次循环需要14个时钟周期<ul><li>发射14条指令<ul><li>12条指令的有效负荷，占比85.7%</li><li>2个时钟周期用于循环控制(14.3%)</li><li>0个时钟周期的停顿</li></ul></li><li>平均每个原始循环3.5个时钟(14/4)</li></ul></li></ul><h4 id="Example-Conclusion"><a href="#Example-Conclusion" class="headerlink" title="Example Conclusion"></a>Example Conclusion</h4><ul><li>未调度未展开： 10个时钟周期</li><li>有调度未展开： 6个时钟周期</li><li>未调度有展开： 7个时钟周期</li><li>有调度有展开： 3.5个时钟周期</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul><li>确认循环迭代是不相关的， 从而能够展开</li><li>使用不同的寄存器， 避免名字相关</li><li>去除多余的分支与条件指令， 调整循环终止与迭代代码</li><li>分析是否存在关于存储地址的相关性</li><li>对代码进行调度</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>代码量显著增长，并由此可能引起频繁的指令缓存缺失(Miss)，因为要存更多的指令在缓存</li><li>寄存器消耗较多，并由此可能引起不必要的访存操作</li></ul><h2 id="基于计分板的指令调度"><a href="#基于计分板的指令调度" class="headerlink" title="基于计分板的指令调度"></a>基于计分板的指令调度</h2><h3 id="静态调度的缺陷"><a href="#静态调度的缺陷" class="headerlink" title="静态调度的缺陷"></a>静态调度的缺陷</h3><ul><li>以循环展开为代表的静态调度需要对每个流水段的执行时间有明确的预期</li><li>然而实际上，指令执行有很多的不确定性<ul><li>同一种指令集体系结构具有不同的组成和实现方式，从而具有不同的微体系结构<ul><li>如果流水线不同，针对一种流水线的静态调度难以在另一种流水线上高效执行</li><li>不可能要求所有的软件为所有的微体系结构编译出一个可执行文件的版本</li></ul></li><li>一些相关性在编译阶段难以发现<ul><li>存储地址带来的相关性</li><li>大量的分支指令</li></ul></li><li>一些不可控的额外延迟，例如缓存缺失(miss)</li></ul></li></ul><h3 id="基于计分板的动态调度-CDC6600"><a href="#基于计分板的动态调度-CDC6600" class="headerlink" title="基于计分板的动态调度(CDC6600)"></a>基于计分板的动态调度(CDC6600)</h3><h4 id="计分板-ScoreBoard-的结构"><a href="#计分板-ScoreBoard-的结构" class="headerlink" title="计分板(ScoreBoard)的结构"></a>计分板(ScoreBoard)的结构</h4><ul><li>记录已发射指令的状态</li><li>记录各个寄存器的使用和等待情况</li><li>记录各个功能单元的使用和等待情况<ul><li>2个乘法器，1个除法器，1个加法器，1个与数据加载器</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF%E7%BB%93%E6%9E%84.png" alt="img"></p><h4 id="动态调度机制"><a href="#动态调度机制" class="headerlink" title="动态调度机制"></a>动态调度机制</h4><ul><li>针对多个功能部件和流水线展开调度</li><li>针对RAW相关(写后读，真相关)<ul><li>设置数据就绪位，强制等待数据就绪</li></ul></li><li>针对WAR相关(读后写，反相关)<ul><li>在一个寄存器被读之前，不允许后续的指令写</li></ul></li><li>针对WAW相关(写后写，输出相关)<ul><li>延迟寄存器写回，避免覆盖</li></ul></li></ul><h4 id="定义流水线的各个阶段"><a href="#定义流水线的各个阶段" class="headerlink" title="定义流水线的各个阶段"></a>定义流水线的各个阶段</h4><ul><li>第一阶段: 发射阶段，负责译码、检查结构冒险(资源争用)<ul><li>按指令的原始顺序完成发射</li><li>有结构冒险的话，则不能发射</li><li>有WAW冒险(输出冒险)的话，也不能发射</li></ul></li><li>第二阶段: 读操作数，在没有冒险时读操作数<ul><li>在RAW消除后读操作数</li><li>计分板不考虑转发(Forward)，即上一条指令执行的结果不可能转发给下一条指令的操作数寄存器，必须遵循写回</li></ul></li><li>第三阶段: 执行阶段，对操作数进行运算(乱序执行)<ul><li>执行指令，并向计分板通知指令执行完成</li></ul></li><li>第四阶段: 写回阶段，写回结果到指定位置(乱序写回)<ul><li>在没有WAR冒险时写回结果</li></ul></li></ul><h4 id="运作机制"><a href="#运作机制" class="headerlink" title="运作机制"></a>运作机制</h4><ul><li>记录指令状态<ul><li>每条已发射指令处于流水线四个阶段的哪个</li></ul></li><li>记录寄存器状态<ul><li>记录每个寄存器是否会被某个功能单元写入，如果是，则还要记录将会被哪个功能单元写入</li></ul></li><li>记录功能单元状态  <ul><li>记录各个功能单元的使用和等待情况</li><li>9个字段的标记<ul><li>Busy: 当前单元是否空闲</li><li>Op: 当前单元执行的操作</li><li>Fi: 目的寄存器</li><li>Fj、Fk: 源寄存器的编号</li><li>Qj、Qk: 为源寄存器产生数据的功能单元</li><li>Rj、Rk: 源寄存器中数据是否就绪的标志位</li></ul></li></ul></li></ul><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example"></a>Example</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L.D     F6, 34(R2)</span><br><span class="line">L.D     F2, 45(R3)</span><br><span class="line">MUL.D   F0,  F2, F4</span><br><span class="line">SUB.D   F8,  F2, F4</span><br><span class="line">DIV.D   F10, F0, F6</span><br><span class="line">ADD.D   F6,  F8, F2</span><br></pre></td></tr></table></figure><ul><li><p>一些关于延迟的假设</p><ul><li>ADD指令: 读到操作数后需要2时钟计算出结果(有一个加法器)</li><li>MULTIPLY指令: 读到操作数后需要10时钟计算出结果(有两个乘法器)</li><li>DIVIDE指令: 读到操作数后需要40时钟计算出结果(有一个除法器)</li><li>Load指令: 只需要1个时钟</li></ul></li><li><p>首先要找出上述指令中的各种相关：</p><ul><li>RAW相关:<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF%E7%9A%84RAW%E7%9B%B8%E5%85%B3.png" alt="img"> </li><li>WAR相关:<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF%E7%9A%84WAR%E7%9B%B8%E5%85%B3.png" alt="img"></li></ul></li><li>初始化计分板，为三个表<ul><li>Instruction status: 记录每个时钟周期的指令状态(或者说每个时钟周期流水线的状态)</li><li>Functional unit status: 记录每个时钟周期的功能单元状态</li><li>Register result status: 记录每个时钟周期的寄存器状态<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%81.png" alt="img"></li></ul></li><li>第1个时钟周期<ul><li>Instruction status记录发射第1条指令</li><li>因为执行的是Load指令，所以Functional unit status记录Integer组件的使用情况，其中目的寄存器是F6，操作数的源寄存器是R2，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes，表示占用。另一个操作数是立即数，不需记录</li><li>Register result status记录F6寄存器需要被Integer功能组件写入<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF1.png" alt="img"></li></ul></li><li>第2个时钟周期<ul><li>Instruction status记录第1条指令读取操作数。不能发送第2条指令，因为有结构冒险，即第1条指令和第2条指令都是Load指令，都需要访问同一个功能组件(内存)</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF2.png" alt="img"></li></ul></li><li>第3个时钟周期<ul><li>Instruction status记录第1条指令处于执行完成阶段。依然不能发送第2条指令，也不能发送第3条指令，因为第3条指令的源操作数需要第2条指令指明，否则不知道该从哪里读取该操作数。</li><li>第1条指令操作数的源寄存器R2已不需再使用，因此设置Functional unit status中对应就绪标志位为No</li><li>Register result status不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF3.png" alt="img"></li></ul></li><li>第4个时钟周期<ul><li>Instruction status记录第1条指令处于写回完成阶段。不过下一周期才能发送后续指令</li><li>Functional unit status清空Integer功能组件的使用情况，因为第1条指令的Load已经完成，该时钟周期是要将结果写回指定位置(但是内存依然是被占用的，所以还不能发送第2条指令)</li><li>Register result status不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF4.png" alt="img"></li></ul></li><li>第5个时钟周期<ul><li>Instruction status记录发射第2条指令，因为第1条指令已经执行完毕，现在不存在结构冒险</li><li>因为执行的是Load指令，所以Functional unit status记录Integer组件的使用情况，其中目的寄存器是F2，操作数的源寄存器是R3，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes，表示占用。另一个操作数是立即数，不需记录</li><li>Register result status记录F2寄存器需要被Integer功能组件写入，将F6寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF5.png" alt="img"></li></ul></li><li>第6个时钟周期<ul><li>Instruction status记录第2条指令读取操作数，发射第3条指令</li><li>因为第3条执行的是Mult指令，所以Functional unit status记录Mult1组件的使用情况，其中目的寄存器是F0，一个操作数的源寄存器一个是F2，功能组件Integer尚未为此寄存器产生数据，因此设置就绪标志位为No。另一个操作数的源寄存器是F4，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes，表示占用</li><li>Register result status记录F0寄存器需要被Mult1功能组件写入<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF6.png" alt="img"></li></ul></li><li>第7个时钟周期<ul><li>Instruction status记录第2条指令处于执行完成阶段，第3条指令由于源寄存器F2的操作数需要在第2条指令完成写回后才能获得，因此不能进行读取操作数阶段。发射第4条指令</li><li>因为第4条执行的是Sub指令，所以Functional unit status记录Add组件的使用情况，其中目的寄存器是F8，一个操作数的源寄存器是F6，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes，表示占用。另一个操作数的源寄存器是F2，功能组件Integer尚未为此寄存器产生数据，因此设置就绪标志位为No。而第2条指令已经读取完了操作数，R3寄存器不需再使用，因此设置就绪标志位为No</li><li>Register result status记录F8寄存器需要被Add功能组件写入<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF7.png" alt="img"></li></ul></li><li>第8个时钟周期<ul><li>Instruction status记录第2条指令处于写回完成阶段，第3条指令和第4条指令由于源寄存器F2的操作数需要在第2条指令完成写回后才能获得，因此不能进行读取操作数阶段。发射第5条指令</li><li>因为第5条指令执行的是Div指令，所以Functional unit status记录Divide组件的使用情况，其中目的寄存器为F10，一个操作数的源寄存器是F0，功能组件Mult1尚未为此寄存器产生数据，因此设置就绪标志位为No。另一个操作数的源寄存器是F6，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes。清空Integer功能组件的使用情况，因为第2条指令的Load已经完成。而Integer已经为F2产生了数据，因此需修改第3条指令和第4条指令对应操作数源寄存器F2的就绪状态为Yes</li><li>Register result status记录F10寄存器需要被Divide功能组件写入，将F2寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF8_1.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF8_2.png" alt="img"></li></ul></li><li>第9个时钟周期<ul><li>Instruction status记录第3和第4条指令读取操作数(因为操作数源寄存器都已就绪)，第5条指令由于源寄存器F0的操作数需要在第3条指令完成写回后才能获得，因此不能进行读取操作数阶段。第6条指令与第4条指令都需要用到Add功能组件，存在结构冒险而不能发射</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF9.png" alt="img"></li></ul></li><li>第10个时钟周期<ul><li>由于第3和第4条指令都已经读取完了操作数，F2、F4、F6寄存器不需再使用，因此Functional unit status设置对应就绪标志位为No</li><li>其余两个表不变，因为第3和第4条指令执行需要不止1个时钟周期<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF10.png" alt="img"></li></ul></li><li>第11个时钟周期<ul><li>Instruction status记录第4条指令处于执行完成阶段</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF11.png" alt="img"></li></ul></li><li>第12个时钟周期<ul><li>Instruction status记录第4条指令处于写回完成阶段(有结构冒险的必须在写回完成后的下一周期才能发射指令)</li><li>Functional unit status清空Add功能组件的使用情况，因为第4条指令已经完成。</li><li>Register result status将F8寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF12.png" alt="img"></li></ul></li><li>第13个时钟周期<ul><li>Instruction status记录发射第6条指令</li><li>因为第5条指令执行的是ADD指令，所以Functional unit status记录Add组件的使用情况，其中目的寄存器为F6，一个操作数的源寄存器是F8，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes。另一个操作数的源寄存器是F2，流水线上看不需要依赖任何功能组件，因此设置就绪标志位为Yes。</li><li>Register result status记录F6寄存器需要被Add功能组件写入<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF13.png" alt="img"></li></ul></li><li>第14个时钟周期<ul><li>Instruction status记录第6条指令读取操作数</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF14.png" alt="img"></li></ul></li><li>第15个时钟周期<ul><li>由于第6条指令已经读取完了操作数，F8、F2寄存器不需再使用，因此Functional unit status设置对应就绪标志位为No</li><li>其余两个表不变，因为第3和第6条指令执行需要不止1个时钟周期<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF15.png" alt="img"></li></ul></li><li>第16个时钟周期<ul><li>Instruction status记录第6条指令处于执行完成阶段</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF16.png" alt="img"></li></ul></li><li>第17个时钟周期<ul><li>所有表都不变，因为第5条指令的源寄存器F6处于就绪状态，与第6条指令的目的寄存器现在存在WAR相关，因此在第5条指令读完操作数之前，第6条指令还不能进行写回<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF17.png" alt="img"></li></ul></li><li>第18个时钟周期<ul><li>所有表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF18.png" alt="img"></li></ul></li><li>第19个时钟周期<ul><li>Instruction status记录第3条指令处于执行完成阶段</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF19.png" alt="img"></li></ul></li><li>第20个时钟周期<ul><li>Instruction status记录第3条指令处于写回完成阶段</li><li>Functional unit status清空Mult1功能组件的使用情况，因为第3条指令已经完成。且Mult1功能组件已经为第5条指令的操作数源寄存器F0产生了数据，因此修改对就绪标志位为Yes</li><li>Register result status将F0寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF20.png" alt="img"></li></ul></li><li>第21个时钟周期<ul><li>Instruction status记录第5条指令读取操作数(第6条指令可以进行写回了)</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF21.png" alt="img"></li></ul></li><li>第22个时钟周期<ul><li>Instruction status记录第6条指令处于写回完成阶段</li><li>Functional unit status清空Add功能组件的使用情况，因为第6条指令已经完成。由于第5条指令已经读取完了操作数，F0、F6寄存器不需再使用，因此设置对应就绪标志位为No</li><li>Register result status将F6寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF22.png" alt="img"></li></ul></li><li>第61个时钟周期<ul><li>Instruction status记录第5条指令处于执行完成阶段</li><li>其余两个表不变<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF61.png" alt="img"></li></ul></li><li>第62个时钟周期<ul><li>Instruction status记录第5条指令处于写回完成阶段</li><li>Functional unit status清空Divide功能组件的使用情况，因为第5条指令已经完成。</li><li>Register result status将F10寄存器状态置空<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%AE%A1%E5%88%86%E6%9D%BF62.png" alt="img"></li></ul></li></ul><h4 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h4><ul><li>没有转发(Forward)</li><li>没有考虑分支指令，只能处理基本块内的调度，窗口较小</li><li>发生结构冒险时，选择不发射指令，但实际是可以发射的，因为有流水线，只需小小延迟</li><li>没有对名字相关采取重命名<ul><li>等待WAR冒险(改名即可避免)</li><li>阻止WAW冒险(改名即可避免)</li></ul></li><li>写寄存器与读该寄存器的数据不能同时进行，需要一个额外时钟(出现写回和读操作数需要两个周期)</li></ul><h2 id="基于Tomasulo算法的指令调度"><a href="#基于Tomasulo算法的指令调度" class="headerlink" title="基于Tomasulo算法的指令调度"></a>基于Tomasulo算法的指令调度</h2><h3 id="数据相关分析-1"><a href="#数据相关分析-1" class="headerlink" title="数据相关分析"></a>数据相关分析</h3><h4 id="真数据相关-True-Data-Dependence"><a href="#真数据相关-True-Data-Dependence" class="headerlink" title="真数据相关(True Data Dependence)"></a>真数据相关(True Data Dependence)</h4><ul><li>只有当指令的操作数可用时，才能执行指令，从而避免RAW冒险</li><li>计分板已经提供了解决方法</li></ul><h4 id="名字相关-Name-Dependence"><a href="#名字相关-Name-Dependence" class="headerlink" title="名字相关(Name Dependence)"></a>名字相关(Name Dependence)</h4><ul><li>WAR和WAW冒险实际源于名字相关<ul><li>WAR: 读后写的反相关</li><li>WAW: 写后写的输出相关</li></ul></li><li>计分板并没有完美解决，选择了不发送指令延时的方法</li><li>可以通过寄存器重命名(Register Renaming)消除</li></ul><h3 id="Tomasulo算法"><a href="#Tomasulo算法" class="headerlink" title="Tomasulo算法"></a>Tomasulo算法</h3><ul><li>为IBM 360/91设计的，在CDC 6600(上节的<a href="#基于计分板的动态调度cdc6600">基于计分板的动态调度(CDC6600)</a>)三年之后(1966)<ul><li>IBM的每条指令有两个寄存器描述符(register specifiers)，CDC 6600有三个(前者没有结果寄存器？)</li><li>IBM有四个浮点寄存器，而CDC 6600有八个</li></ul></li></ul><h4 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h4><ul><li>跟踪操作数何时可用</li><li>控制&amp;缓冲器分布于功能部件(FU)与集中于计分板<ul><li>功能部件缓冲器称为”保留站(RS)”</li><li>用于存放未决的操作数</li></ul></li><li>在硬件中引入寄存器重命名</li><li>指令中的寄存器被数值或者指向保留站的指针替代，即寄存器重命名(Register Renaming)<ul><li>消除WAR、WAW冒险</li><li>保留站比实际寄存器多，因为可以完成优化编译器所不能完成的一些工作结果</li><li>从RS直接到FU，无需通过寄存器，而是通过公共数据总线(Common Data Bus)把结果广播到所有FU(如上图)<ul><li>Normal data bus(正常数据总线): <ul><li>data + destination(“go to” bus)</li></ul></li><li>Common data bus(通用数据总线): <ul><li>data + source(“come from” bus)</li><li>64 bits of data + 4 bits of Functional Unit source address(源地址)</li><li>Write if matches expected Functional Unit(produces result)(与预期功能单元匹配就写入)</li><li>Does the broadcast(广播)</li></ul></li></ul></li><li>载入(Load)和存储(Store)也像其他功能部件一样使用保留站</li></ul></li></ul><h4 id="Reservation-stations-RS-保留站"><a href="#Reservation-stations-RS-保留站" class="headerlink" title="Reservation stations(RS, 保留站)"></a>Reservation stations(RS, 保留站)</h4><ul><li>Register renaming is provided by reservation stations, contains:</li><li>Components<ul><li>The instruction(指令)</li><li>Buffered operand values(when available)(缓冲的操作数数值)</li><li>Reservation station number of instruction providing the operand values(提供操作数数值的指令的保留站编号)</li></ul></li><li>RS fetches and buffers an operand as soon as it becomes available(RS在操作数可用的时候立即获取并缓存该操作数)<ul><li>not necessarily involving register file(不一定涉及寄存器文件)</li></ul></li><li>Pending instructions designate the RS to which they will send their output(待定指令指定了它们将要将输出发送到哪个RS)<ul><li>Result values broadcast on a result bus, called Common Data Bus(CDB, 结果在结果总线上广播，该总线叫做通用数据总线)</li></ul></li><li>Only the last output updates the register file(只有最后一个输出会更新寄存器文件)</li><li>As instructions are issued, the register specifiers are renamed with the reservation station(指令发送后，寄存器说明符将用保留站进行重命名)<ul><li>May be more reservation stations than registers(也许保留站甚至比寄存器还要多，因为寄存器名称冲突了才重命名的)</li></ul></li><li><p>Load and store buffers(加载和保存缓存，也就是保留站的工作)</p><ul><li>Contain data and addresses, act like reservation stations(包括了数据和地址)</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo%20Reservation%20Station.png" alt="img"></p></li></ul><h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><ul><li>Issue(Sometimes called dispatch调度 in a dynamically scheduled processor)<ul><li>Get next instruction from FIFO queue(FIFO队列中获取下一条指令)</li><li>If available RS(no structural hazard, 没有结构冒险), issue the instruction to the RS with operand values if avaliable(renames registers)(没有结构冒险就发射指令到RS，如此便能对寄存器进行重命名。如果操作数值可用，则带着操作数)</li><li>If operand values not available, stall the instruction(如果操作数值暂时不可用，则暂停该指令)</li></ul></li><li>Execute(执行)<ul><li>When operand becomes available, store it in any reservation stations waiting for it(操作数可用时)</li><li>When both operands ready, then execute(所有的操作数都就绪，执行它)</li><li>If not ready, watch Common Data Bus for result(不就绪，监听通用数据总线等待结果)</li></ul></li><li>Write result(写回)<ul><li>Write result on CDB, and from there into registers, reservation stations and store buffers waiting for this result(将结果写道通用数据总线，并从通用数据总线写入目标寄存器、保留站和等待此结果的存储缓冲区)</li><li>Stores must wait until store address and value to be stored are available(存储必须等到要存储的数据和存储目标地址可用才能存储)</li><li>Mark reservation station available(标记保留站可用)</li></ul></li></ul><h4 id="Example-3"><a href="#Example-3" class="headerlink" title="Example"></a>Example</h4><ul><li>保留站中记录的信息<ul><li>$Op$: 操作类型 </li><li>$Busy$: 是否占用</li><li>$V_j$, $V_k$: 源操作数的值(已经就绪的源操作数才能保留在此)</li><li>$Q_j$, $Q_k$: 如果源操作数的值尚未就绪，该部分指向操作数的值依赖保留站中的哪一项<ul><li>当值为0的时候，说明操作数已经就绪了</li><li>对于一个源操作数，V和Q只有一个有效</li></ul></li><li>$A$: Load、Store指令对应的地址<ul><li>如果地址还未算出来，则保留立即数</li><li>如果地址已经算出来，则保留访存目标地址的值</li></ul></li></ul></li><li>寄存器组保留的信息<ul><li>$Q_i$: 第i号寄存器的值由哪个保留站计算出来的</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L.D     F6, 34(R2)</span><br><span class="line">L.D     F2, 45(R3)</span><br><span class="line">MUL.D   F0,  F2, F4</span><br><span class="line">SUB.D   F8,  F2, F4</span><br><span class="line">DIV.D   F10, F0, F6</span><br><span class="line">ADD.D   F6,  F8, F2</span><br></pre></td></tr></table></figure><ul><li>初始化<ul><li>Instruction status: 记录指令所处状态</li><li>Load/Buffers: 缓存指令的操作数</li><li>Reservation Station: 记录操作数状态</li><li>Register result status: 记录寄存器状态<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo%E5%88%9D%E5%A7%8B%E7%8A%B6%E6%80%81.png" alt="img"></li></ul></li><li>第1个时钟周期<ul><li>Instruction status: 发射第1条指令</li><li>Load/Buffers: 缓存第1条指令的操作数，所占位置设置为Busy</li><li>Reservation Station: 暂时为空</li><li>Register result status: 记录寄存器F6需要等待来自Buffer中Load1的结果<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo1.png" alt="img"></li></ul></li><li>第2个时钟周期<ul><li>Instruction status: 由于有缓冲区的存在，可以不管结构冒险发射第2条指令，即使和第一条指令可能会争用内存。(即可以发送多条Load指令)</li><li>Load/Buffers: 缓存第2条指令的操作数，所占位置Busy设置为Yes</li><li>Reservation Station: 暂时为空</li><li>Register result status: 记录寄存器F2需要等待来自Buffer中Load2的结果<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo2.png" alt="img"></li></ul></li><li>第3个时钟周期<ul><li>Instruction status: 发射第3条指令，第1条指令执行完毕(可以在CDB广播该结果)</li><li>Load/Buffers: 为空，不需要缓存操作数</li><li>Reservation Station: 记录第3条指令占用乘法器Mult1进行乘法操作，第1个操作数来自寄存器F2的值还未就绪，需要来自第2条指令的写回结果，因此在$Q_j$记录该操作数在Buffer的来源，第2个操作数则已就绪写在$V_k$(操作数都保留在此，寄存器F4和F2不需要使用)</li><li>Register result status: 记录寄存器F0需要等待来自第3条指令的乘法器Mult1的计算结果<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo3.png" alt="img"></li></ul></li><li>第4个时钟周期<ul><li>Instruction status: 发射第4条指令，第1条指令写回完成，第2条指令执行完成(可以在CDB广播该结果)</li><li>Load/Buffers: 第1条指令占据的位置可以空出来了，Busy设置为No</li><li>Reservation Station: 记录第4条指令占用加法器Add1进行减法操作，因为第1个操作数来自第1条指令的结果，而第1条指令已经写回，则在$V_j$表示就绪(其中A1为第1条指令里的地址)。第2个操作数来自寄存器F2的值还未就绪，需要来自第2条指令的写回结果，因此在$Q_k$记录该操作数在Buffer的来源</li><li>Register result status: 记录寄存器F8需要等待来自第4条指令的加法器Add1的计算结果，记录F6寄存器已取得写回结果(A1为第1条指令里的地址)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo4.png" alt="img"></li></ul></li><li>第5个时钟周期<ul><li>Instruction status: 发射第5条指令，第2条指令写回完成，第3、4条指令此时操作数才能就绪，尚未执行完成</li><li>Load/Buffers: 第2条指令占据的位置可以空出来了，Busy设置为No</li><li>Reservation Station: 记录第5条指令占用乘法器Mult2进行除法操作，因为第1个操作数来自寄存器F0还未就绪，需要乘法器Mult1写回计算结果，因此在$Q_j$记录该操作数在来源。第2个操作数来自第1条指令的结果，而第1条指令早已经写回，则在$V_k$表示就绪(其中A1为第1条指令里的地址)。由于第2条指令已经完成写回，则需要第2条指令写回结果作为源操作数的第3、4条指令的RS项可以将未就绪改为就绪，并注明来源自写回结果(A2为第2条指令里的地址)，然后两者现在可同时执行，由于没有用到相同的源寄存器，操作数都存在不同RS中，就相当于重命名了。</li><li>Register result status: 记录F2寄存器已取得写回结果(A2为第2条指令里的地址)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo5.png" alt="img"></li></ul></li><li>第6个时钟周期<ul><li>Instruction status: 发射第6条指令，由上节知道加减需要2个周期完成执行，乘法需要10个周期完成执行，所以第3、4条指令尚未执行完成</li><li>Load/Buffers: 无需改动</li><li>Reservation Station: 记录第6条指令占用加法器Add2进行加法操作，因为第1个操作数来自寄存器F8还未就绪，需要加法器Add1写回计算结果，因此在$Q_j$记录该操作数在来源。第2个操作数来自第2条指令的结果，而第2条指令早已经写回，则在$V_k$表示就绪(其中A2为第2条指令里的地址)</li><li>Register result status: 记录F6寄存器需要等待来自第6条指令的加法器Add2的计算结果<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo6.png" alt="img"></li></ul></li><li>第7个时钟周期<ul><li>Instruction status: 第4条指令执行完成</li><li>Load/Buffers: 无需改动</li><li>Reservation Station: 无需改动</li><li>Register result status: 无需改动<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo7.png" alt="img"></li></ul></li><li>第8个时钟周期<ul><li>Instruction status: 第4条指令写回完成</li><li>Load/Buffers: 无需改动</li><li>Reservation Station: 第4条指令已完成，所以需清空占用的加法器Add1的记录，且因此第6条指令第1个操作数就绪，清空$Q_j$，设置$V_j$(从CDB获得？)</li><li>Register result status: 记录F8寄存器从CDB获得的值<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo8.png" alt="img"></li></ul></li><li>第9个时钟周期<ul><li>Instruction status: 第3、6条指令在执行，第5条指令在等待，所以无需修改</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo9.png" alt="img"></li></ul></li><li>第10个时钟周期<ul><li>Instruction status: 第6条指令执行完成</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo10.png" alt="img"></li></ul></li><li>第11个时钟周期<ul><li>Instruction status: 第6条指令写回完成</li><li>Load/Buffers: 无需改动</li><li>Reservation Station: 第4条指令已完成，所以需清空占用的加法器Add2的记录</li><li>Register result status: 记录F6寄存器从CDB获得的值(因为其他操作数都不在寄存器，所以不存在RAW冒险)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo11.png" alt="img"></li></ul></li><li>第12个时钟周期<ul><li>Instruction status: 无需修改</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo12.png" alt="img"></li></ul></li><li>第13个时钟周期<ul><li>Instruction status: 无需修改</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo13.png" alt="img"></li></ul></li><li>第14个时钟周期<ul><li>Instruction status: 无需修改</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo14.png" alt="img"></li></ul></li><li>第15个时钟周期<ul><li>Instruction status: 第3条指令执行完成</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo15.png" alt="img"></li></ul></li><li>第16个时钟周期<ul><li>Instruction status: 第3条指令写回完成</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 第3条指令已完成，所以需清空占用的乘法器Mult1的记录，且因此第5条指令的第1个操作数就绪，清空$Q_j$，设置$V_j$(从CDB获得？)</li><li>Register result status: 记录F0寄存器从CDB获得的值<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo16.png" alt="img"></li></ul></li><li>第17-55个时钟周期<ul><li>Instruction status: 由上节知道除法指令要花费48个时钟周期，无需修改</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo55.png" alt="img"></li></ul></li><li>第56个时钟周期<ul><li>Instruction status: 第5条指令执行完成</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 无需修改</li><li>Register result status: 无需修改<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo56.png" alt="img"></li></ul></li><li>第57个时钟周期<ul><li>Instruction status: 第5条指令写回完成</li><li>Load/Buffers: 无需修改</li><li>Reservation Station: 第5条指令已完成，所以需清空占用的乘法器Mult2的记录</li><li>Register result status: 记录F10寄存器从CDB获得的值<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo57.png" alt="img"></li></ul></li></ul><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul><li>按序发射</li><li>乱序执行</li><li>乱序完成</li></ul><h4 id="Tomasulo算法的优势"><a href="#Tomasulo算法的优势" class="headerlink" title="Tomasulo算法的优势"></a>Tomasulo算法的优势</h4><ul><li>分布式的冒险检测<ul><li>采用分布式的保留站: 每个保留站可以自主判断冒险</li><li>CDB的使用: 一个结果被多个功能单元等待，这些功能单元能够同时得到该结果的广播消息，同时记录(如果在寄存器，则只能一个功能单元访问，而总线能够被多个功能单元访问)</li></ul></li><li>能够消除WAW、WAR冒险<ul><li>利用保留站可实现寄存器的重命名</li><li>WAR: 一旦数据从寄存器读取到保留站，就不再依赖寄存器</li><li>WAW: 两条指令的目标寄存器相同，但结果由不同的功能单元等待，则不会出现WAW</li></ul></li></ul><h4 id="Tomasulo算法的劣势"><a href="#Tomasulo算法的劣势" class="headerlink" title="Tomasulo算法的劣势"></a>Tomasulo算法的劣势</h4><ul><li>主要缺点是复杂度太高<ul><li>要求大量的硬件</li><li>保留站必须维护一个高速运行和拥且复杂控制逻辑的关联缓冲器</li><li>单CDB总线可能也会由性能瓶颈，当太多功能部件共同访问的时候</li></ul></li></ul><h4 id="不精确的意外情况"><a href="#不精确的意外情况" class="headerlink" title="不精确的意外情况"></a>不精确的意外情况</h4><ul><li>动态地调度过程可能产生不精确的意外情况<ul><li>流水线可能已经完成了比异常产生时的指令在程序的顺序上更晚的指令(即先执行后面的指令去了，但前面的指令又异常了)</li><li>流水线可能还没完成异常产生时的指令在程序的顺序上之前的指令(即前面的指令还没完成，先执行后面指令，但又异常了)</li></ul></li></ul><h4 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h4><ul><li>寄存器较少的体系结构</li><li>代码难以调度的场景</li></ul><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul><li>基于保留站的动态调度<ul><li>允许发射很多指令，只要保留站够用</li><li>允许将寄存器的旧值保存在保留站，避免WAR冒险</li></ul></li><li>基于保留站的寄存器重命名<ul><li>解决寄存器不够用的问题</li></ul></li><li>解决访存时的地址冲突问题<ul><li>发射Load、Store指令的时候，仍需要检查是否有关于访存的WAW、WAR</li></ul></li><li>Tomasulo算法在代码执行过程中建立数据流依赖关系图</li></ul><h2 id="分支预测"><a href="#分支预测" class="headerlink" title="分支预测"></a>分支预测</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li>典型的MIPS程序中，每3-6条指令会出现一条分支指令，使得动态调度的窗口变小</li><li>上面几节分享的调度算法都没有涉及分支</li><li>指令并行发掘得越多，分支带来的性能损失就越大</li></ul><h3 id="Branch-Hazards-分支冒险"><a href="#Branch-Hazards-分支冒险" class="headerlink" title="Branch Hazards(分支冒险)"></a>Branch Hazards(分支冒险)</h3><ul><li>造成比数据冒险更糟糕的性能损失<ul><li>大概10-30%的性能衰减</li><li>cycle stall in the pipeline(流水线中的循环等待？)<ul><li>在分支后的指令在分支被确认之前就进入了流水线，但分支预测到的实际结果并不需要执行该指令</li><li>而且一个分支出错，也会影响下一个分支</li></ul></li></ul></li><li>需要减少流水线的分支惩罚</li></ul><h3 id="过去的解决方案"><a href="#过去的解决方案" class="headerlink" title="过去的解决方案"></a>过去的解决方案</h3><h4 id="方案1-freeze-flush-pipeline-冻结或冲洗流水线"><a href="#方案1-freeze-flush-pipeline-冻结或冲洗流水线" class="headerlink" title="方案1: freeze/flush pipeline(冻结或冲洗流水线)"></a>方案1: freeze/flush pipeline(冻结或冲洗流水线)</h4><ul><li>保留或删除分支后的任何指令，直到得知分支的目的地(早期方法)</li><li>该方案的分支惩罚是固定的，而且不能被软件继续优化减少</li></ul><h4 id="方案2-预测每个分支都不发生跳转-not-taken-属于静态分支预测"><a href="#方案2-预测每个分支都不发生跳转-not-taken-属于静态分支预测" class="headerlink" title="方案2: 预测每个分支都不发生跳转(not taken, 属于静态分支预测)"></a>方案2: 预测每个分支都不发生跳转(not taken, 属于静态分支预测)</h4><ul><li>性能较好但稍微复杂<ul><li>只是允许硬件继续运行</li></ul></li><li>需要知道指令什么时候可能会变化状态，以及如何退出(back out)这种改变(也就是要知道什么情况下可能会预测不跳转出现错误，也就是实际要跳转。还要知道如何应对预测错误)<ul><li>当分支确实不需要跳转的时候，说明预测成功，继续执行就可以了</li><li>当分支是需要跳转的时候，说明预测失败，则需要先重新读取跳转后的指令到PC才能继续执行，也就是说后续指令会慢一个周期(因为目前的下一条指令是不进行跳转的，不能对它进行解码，所以也就延迟了)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/untaken_branch.png" alt="img"></p><h4 id="方案3-预测每个分支都要发生跳转-taken-属于静态分支预测"><a href="#方案3-预测每个分支都要发生跳转-taken-属于静态分支预测" class="headerlink" title="方案3: 预测每个分支都要发生跳转(taken, 属于静态分支预测)"></a>方案3: 预测每个分支都要发生跳转(taken, 属于静态分支预测)</h4><ul><li>由于是否发生跳转需要在流水线第三阶段的执行阶段才能知道，提前预测需要跳转，则能提前对跳转部分进行解码，也就是有一个时钟周期的提升</li></ul><h4 id="方案4-延迟分支-Delayed-Branch"><a href="#方案4-延迟分支-Delayed-Branch" class="headerlink" title="方案4: 延迟分支(Delayed Branch)"></a>方案4: 延迟分支(Delayed Branch)</h4><ul><li>在早期RISC架构的处理器中使用</li><li>无论分支是否需要跳转，都要在分支指令后插入一条指令(但不允许该指令也是个分支指令，否则会含义不清晰，可能是嵌套分支)作为延迟指令，然后才在再下一个指令周期读取分支要执行的指令</li><li>缺点<ul><li>需要重新定义架构</li><li>可能会导致轻微的代码扩展</li><li>中断处理变得更困难<ul><li>由延迟槽中的指令引起的中断请求必须与”正常”指令引起的中断请求做不同的处理</li></ul></li><li>需要额外的硬件</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/delay_branch.png" alt="img"></p><h3 id="现在的解决方案-动态分支预测"><a href="#现在的解决方案-动态分支预测" class="headerlink" title="现在的解决方案: 动态分支预测"></a>现在的解决方案: 动态分支预测</h3><ul><li>基于程序的行为对分支做出预测</li><li>动态分支预测的改进将随着流水线长度的增加而增加，但因此分支延迟也会增加</li><li>更好的预测器将产生更好的性能</li><li>现代的高性能处理器都有大概15个时钟周期的分支预测失败延迟<ul><li>所以预测的准确度是很重要的</li></ul></li></ul><h4 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h4><ul><li>随着流水线变得更深，分支的潜在惩罚提高，过去的方案是不够的</li><li>需要更激进的方案来预测分支</li></ul><h4 id="静态分支预测"><a href="#静态分支预测" class="headerlink" title="静态分支预测"></a>静态分支预测</h4><ul><li>低成本的静态方案，依赖于编译时获得的可用信息</li><li>服从二项分布(要么跳转，要么不跳转)</li></ul><h4 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a>基本组件</h4><ul><li>分支历史记录表(BHT, Branch History Table)<ul><li>存放在指令cache或由专门硬件实现</li></ul></li><li>分支预测缓冲器(BPB, Branch Prediction Buffer): 预测分支是否发生跳转<ul><li>根据同一分支以前是否跳转预测当前执行是否跳转</li><li>根据最近的几个分支是否跳转预测当前执行是否跳转</li><li>预测的准确度取决于程序的特性和预测缓存的大小(是否有足够多的参考)</li></ul></li><li><p>分支目标缓冲器(BTB, Branch Target Buffer)</p><ul><li>目的: 在取指IF阶段(译码ID之前)就能知道本指令是否为分支指令，如果是分支指令则知道是否需要转移，如果需要转移则知道转移目标是哪里</li><li><p>BTB是一个高速缓存，只用来保存需要跳转的分支指令或者无条件跳转的指令，必要字段包括:</p><ul><li>分支指令的PC值</li><li>要跳转的分支指令的转移地址</li><li><p>工作原理</p><ul><li>在取指阶段，将PC与BTB中的条目比较，看BTB中是否出现当前PC</li><li>如果当前PC出现在BTB中， 则返回所保存的转移目标地址<ul><li>返回的转移目标地址用于下一条指令取指</li><li>如果该指令实际是不需要跳转的，则要在取消转移目标地址的指令执行，重启不跳转指令执行的同时，删除BTB中关于该指令的记录</li></ul></li><li>如果当前PC未出现在BTB中，则根据<code>PC+4</code>取指<ul><li>如果该指令实际是要跳转的，则要把转移目标地址和该指令的PC添加到BTB中</li></ul></li><li>于是在分支指令的取指阶段就可以知道转移目标</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/BTB.png" alt="img"></p></li><li><p>失败的代价:<br>Instruction in BTB|Prediction|Actual branch|Penalty cycle<br>:-:|:-:|:-:|:-:<br>yes|Taken|Taken|0<br>yes|Taken|Not Taken|2<br>yes||Taken|2<br>yes||Not Taken|0</p></li><li><p>改进:</p><ul><li>将目标指令也保存在BTB中，于是可在取目标PC的同时，将目标指令也取出来，减少一次访存</li><li>记录返回地址，将Call指令的返回地址(下一条指令)记录在一个小的栈中<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/BTB%E4%BC%98%E5%8C%96.png" alt="img"></li></ul></li></ul></li></ul></li></ul><h4 id="动态分支预测的基本原理"><a href="#动态分支预测的基本原理" class="headerlink" title="动态分支预测的基本原理"></a>动态分支预测的基本原理</h4><ul><li>利用最近的转移发生情况，预测下一次可能发生的转移(也就是看以前这条指令是跳还是不跳)</li><li>预测后，在实际发生时验证预测是否成功并调整再下一次的预测</li><li>转移发生的历史情况记录在BHT(Branch History Table)</li><li>每个表项由分支指令地址的低位作为索引，在IF阶段就可以获取预测位<ul><li>也就是不需要在ID进行解码才能获得</li><li>低位地址相同的分支指令共享一个表项，由此可能会有冲突</li><li>由于仅用于预测，所以不会影响实际的执行结果</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%8A%A8%E6%80%81%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86.png" alt="img"></p><h4 id="简单的1位预测器"><a href="#简单的1位预测器" class="headerlink" title="简单的1位预测器"></a>简单的1位预测器</h4><ul><li>分支预测缓冲器只有一个预测位: 用于记录单个分支指令最近的一次历史<ul><li>只要预测出错就在下一次做相反预测</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/1%E4%BD%8D%E9%A2%84%E6%B5%8B%E5%99%A8.png" alt="img"></p><h4 id="简单的2位预测器"><a href="#简单的2位预测器" class="headerlink" title="简单的2位预测器"></a>简单的2位预测器</h4><ul><li>采用2位记录历史: 与n位预测器效果差不多(也就是说还有3位4位等，但实际效果也没提升多少)<ul><li>采用有限状态机记录单个分支是否成功的历史情况</li><li>根据状态机的状态做出预测</li><li>根据真实分支情况修正预测器<ul><li>只有连续失败两次才改变下一次预测的结果</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2%E4%BD%8D%E9%A2%84%E6%B5%8B%E5%99%A8.png" alt="img"></p><h4 id="相关预测器-两级预测器-correlating-predictor"><a href="#相关预测器-两级预测器-correlating-predictor" class="headerlink" title="相关预测器(两级预测器, correlating predictor)"></a>相关预测器(两级预测器, correlating predictor)</h4><ul><li>用移位寄存器记录最近m个分支指令的转移情况<ul><li>转移成功置1，不成功置0</li><li>可以寻址$2^m$个预测器，而每个预测器有n位<ul><li>如下图的16个预测器，每个预测器2位<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%9B%B8%E5%85%B3%E9%A2%84%E6%B5%8B%E5%99%A81.png" alt="img"></li></ul></li></ul></li><li>预测器根据分支指令地址低位+寄存器m位(最近全局发生的m次分支)得到的地址查询BPB(Branch Prediction Buffer)从$2^m$个n位预测器中选出一个<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%9B%B8%E5%85%B3%E9%A2%84%E6%B5%8B%E5%99%A82.png" alt="img"><br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%9B%B8%E5%85%B3%E9%A2%84%E6%B5%8B%E5%99%A83.png" alt="img"></li><li>相关预测其的表现比单个分支预测的表现好很多，即使单个分支预测能记录所有该分支的跳转历史(局部性原理?)</li></ul><h4 id="局部预测器-local-predictor"><a href="#局部预测器-local-predictor" class="headerlink" title="局部预测器(local predictor)"></a>局部预测器(local predictor)</h4><ul><li>为每个分支设置$2^m$个n位的预测器</li><li>根据最近本身分支发生的m次分支从$2^m$个n位预测器中选出一个</li></ul><h4 id="竞赛预测器-Tournament-predictor"><a href="#竞赛预测器-Tournament-predictor" class="headerlink" title="竞赛预测器(Tournament predictor)"></a>竞赛预测器(Tournament predictor)</h4><ul><li>结合了相关预测器和本地预测器<ul><li>全局预测器(相关预测)使用最近的分支跳转历史作为索引</li><li>本地预测器使用分支地址作为索引(选择哪个预测器)</li></ul></li><li>其实就是单分支预测器和相关预测器分别进行各自的预测，然后用一个二路复用器选择用哪个预测结果作为最终预测结果<ul><li>二路复用器相当于一个2位预测器，如果当前选择的结果连续两次出错，就更改下次预测给出预测结果的预测器(所以也是用分支地址作为索引看怎么决定预测器的选择)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E7%AB%9E%E4%BA%89%E9%A2%84%E6%B5%8B%E5%99%A8.png" alt="img"></p><h4 id="标记混合预测器-Tagged-Hybrid-Predictors-or-TAGE——Tagged-Geometic-Predictors"><a href="#标记混合预测器-Tagged-Hybrid-Predictors-or-TAGE——Tagged-Geometic-Predictors" class="headerlink" title="标记混合预测器(Tagged Hybrid Predictors, or TAGE——Tagged Geometic Predictors)"></a>标记混合预测器(Tagged Hybrid Predictors, or TAGE——Tagged Geometic Predictors)</h4><ul><li>需要为每个分支提供预测器和记录每个分支的跳转历史<ul><li>这会导致巨大的记录表，查询会有麻烦</li><li>用hash表(hash值由分支地址和分支历史计算出来)解决该问题</li><li>不过较长的历史记录可能会导致哈希冲突的机会增加，因此使用多个表(也就是分多个表去查历史)X</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E6%B7%B7%E5%90%88%E6%A0%87%E8%AE%B0%E9%A2%84%E6%B5%8B%E5%99%A8.png" alt="img"></p><h3 id="Intel-Core-i7的分支预测方法"><a href="#Intel-Core-i7的分支预测方法" class="headerlink" title="Intel Core i7的分支预测方法"></a>Intel Core i7的分支预测方法</h3><ul><li>综合了三种预测器<ul><li>简单的2位预测器</li><li>基于全局历史的相关预测器</li><li>循环跳出(Loop Exit)预测器<ul><li>当一个分支被判断为一个循环时，用一个计数器记录循环次数，也就知道什么时候可能不需要跳转或者需要跳转了</li></ul></li></ul></li></ul><h2 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h2><h3 id="基本理念"><a href="#基本理念" class="headerlink" title="基本理念"></a>基本理念</h3><ul><li>推测执行是分支预测的目的所在</li><li>假定分支预测永远正确，则按预测结果发射指令</li><li>对发射的指令进行动态调度</li><li>设计一定的机制容忍预测出现错误</li></ul><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><h4 id="分支预测错误"><a href="#分支预测错误" class="headerlink" title="分支预测错误"></a>分支预测错误</h4><ul><li>可以按照分支预测执行，但是必须保证分支结果确定之后再提交(也就是不管预测执行，如果预测失败就不提交)</li></ul><h4 id="精确中断"><a href="#精确中断" class="headerlink" title="精确中断"></a>精确中断</h4><ul><li>一条指令发生中断/异常时，其后的指令不能已经提交</li></ul><h4 id="计分板和Tomasulo算法都不是按序提交"><a href="#计分板和Tomasulo算法都不是按序提交" class="headerlink" title="计分板和Tomasulo算法都不是按序提交"></a>计分板和Tomasulo算法都不是按序提交</h4><ul><li>都是按序发射，乱序执行，乱序提交</li></ul><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><ul><li>按序发射</li><li>乱序执行</li><li>按序提交</li><li>提交前阻止一切不可逆转的事件<ul><li>中断</li><li>异常</li></ul></li></ul><h3 id="硬件推测执行所需组件"><a href="#硬件推测执行所需组件" class="headerlink" title="硬件推测执行所需组件"></a>硬件推测执行所需组件</h3><ul><li>动态分支预测</li><li>动态指令调度<ul><li>跨基本块的调度</li></ul></li><li>指令推测执行</li><li>结果UNDO模块(不提交)</li></ul><h3 id="按序提交的实现——引入重排序缓存-ROB"><a href="#按序提交的实现——引入重排序缓存-ROB" class="headerlink" title="按序提交的实现——引入重排序缓存(ROB)"></a>按序提交的实现——引入重排序缓存(ROB)</h3><h4 id="回顾Tomasulo算法"><a href="#回顾Tomasulo算法" class="headerlink" title="回顾Tomasulo算法"></a>回顾Tomasulo算法</h4><ul><li>指令发射到保留站</li><li>保留站动态调度功能单元执行指令</li><li>功能单元发送结果到CDB(Common Data Bus)</li><li>CDB广播结果</li><li>寄存器组乱序提交结果<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Tomasulo.png" alt="img"></li></ul><h4 id="重排序缓存-Recorder-Buffer-ROB-的各个字段"><a href="#重排序缓存-Recorder-Buffer-ROB-的各个字段" class="headerlink" title="重排序缓存(Recorder Buffer, ROB)的各个字段"></a>重排序缓存(Recorder Buffer, ROB)的各个字段</h4><ul><li>指令类型<ul><li>分支指令: 无需提交结果</li><li>Store指令: 需要写内存</li><li>写寄存器指令: 需要写寄存器</li></ul></li><li>目标域<ul><li>寄存器编号</li><li>内存地址</li></ul></li><li>Value: 值</li><li>Ready: 指令是否已经执行完成，随时准备提交</li></ul><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><ul><li>指令发射阶段<ul><li>将发射的指令按序保存在ROB中</li><li>记录指令的目的寄存器、PC值<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROB%E6%8C%87%E4%BB%A4%E5%8F%91%E5%B0%84.png" alt="img"></li></ul></li><li>指令执行阶段<ul><li>将指令执行结果保存在ROB中<ul><li>暂时不提交</li><li>但可以广播到保留站各个等待该结果的单元中</li></ul></li><li>记录可能发生的中断和异常<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROB%E6%8C%87%E4%BB%A4%E6%89%A7%E8%A1%8C.png" alt="img"></li></ul></li><li>结果写回阶段<ul><li>将ROB头部的指令结果提交<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROB%E7%BB%93%E6%9E%9C%E6%8F%90%E4%BA%A4.png" alt="img"><ul><li>写寄存器</li><li>写存储器</li></ul></li><li>处理发生的中断和异常<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROB%E7%BB%93%E6%9E%9C%E5%86%99%E5%9B%9E.png" alt="img"></li></ul></li></ul><h4 id="可行优化——重排序缓存-Forwarding-推测执行"><a href="#可行优化——重排序缓存-Forwarding-推测执行" class="headerlink" title="可行优化——重排序缓存+Forwarding+推测执行"></a>可行优化——重排序缓存+Forwarding+推测执行</h4><ul><li>将已经确定提交的结果直接Forward到需要该结果的指令<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROBForwarding.png" alt="img"></li><li>发射分支指令到ROB<ul><li>但须标记这是预测执行的指令</li></ul></li><li>正常执行分支指令<ul><li>谨慎提交</li></ul></li><li>分支确定之后决定是否提交<ul><li>预测正确<ul><li>后续指令都可以提交</li></ul></li><li>预测错误<ul><li>清除ROB中的后续指令<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ROB%E4%BC%98%E5%8C%96.png" alt="img"></li></ul></li></ul></li></ul><h4 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h4><ul><li>发射</li><li>执行</li><li>写结果</li><li>提交</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;高性能处理器的并行计算技术&quot;&gt;&lt;a href=&quot;#高性能处理器的并行计算技术&quot; class=&quot;headerlink&quot; title=&quot;高性能处理器的并行计算技术&quot;&gt;&lt;/a&gt;高性能处理器的并行计算技术&lt;/h1&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>高性能计算机的体系结构</title>
    <link href="http://zjn-astonishe.github.io/2024/10/16/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-16-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    <id>http://zjn-astonishe.github.io/2024/10/16/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-16-%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</id>
    <published>2024-10-16T04:47:28.000Z</published>
    <updated>2024-11-09T04:33:57.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高性能计算机的体系结构"><a href="#高性能计算机的体系结构" class="headerlink" title="高性能计算机的体系结构"></a>高性能计算机的体系结构</h1><h2 id="超级计算机硬件"><a href="#超级计算机硬件" class="headerlink" title="超级计算机硬件"></a>超级计算机硬件</h2><ul><li>从硬件组成上看，超级计算机系统系统的硬件由五个子系统组成<ul><li>计算阵列</li><li>存储阵列</li><li>服务阵列</li><li>互连通信子系统</li><li>监控诊断子系统<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E8%B6%85%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84.png" alt="img"></li></ul></li></ul><h3 id="计算阵列"><a href="#计算阵列" class="headerlink" title="计算阵列"></a>计算阵列</h3><ul><li>由计算节点组成，每个计算节点配置不同数量的中央处理器(CPU)和协处理器(GPU)<ul><li>协处理器通常是加速器，被用于增加计算的吞吐量，以降低可编程性的微小代价获得<ul><li>因为加速器使用的控制逻辑通常与现有的处理器指令集架构(ISA)不兼容</li></ul></li></ul></li></ul><h4 id="存储阵列"><a href="#存储阵列" class="headerlink" title="存储阵列"></a>存储阵列</h4><ul><li>采用层次式混合共享存储架构，实现大容量、高带宽、低延迟的共享存储功能</li></ul><h4 id="服务阵列"><a href="#服务阵列" class="headerlink" title="服务阵列"></a>服务阵列</h4><ul><li>采用商用服务器，属于大容量的胖节点</li></ul><h4 id="互连通信子系统"><a href="#互连通信子系统" class="headerlink" title="互连通信子系统"></a>互连通信子系统</h4><ul><li>互连网络是大规模并行处理系统的核心<ul><li>天河系列的超算使用的拓扑结构是胖树拓扑结构，可高效进行均衡扩展</li><li>支持基于硬件实现的自动消息交换机制的集体操作(广播、多播)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E4%BA%92%E8%BF%9E%E9%80%9A%E4%BF%A1%E5%AD%90%E7%B3%BB%E7%BB%9F.png" alt="img"></li></ul></li></ul><h4 id="监控诊断子系统"><a href="#监控诊断子系统" class="headerlink" title="监控诊断子系统"></a>监控诊断子系统</h4><ul><li>实现了整体系统实时安全监测和诊断调试功能<ul><li>实时监控、控制、诊断和调试整个超级计算机系统的健康状态、功耗和温度信息</li></ul></li></ul><h2 id="高性能计算机体系结构分类"><a href="#高性能计算机体系结构分类" class="headerlink" title="高性能计算机体系结构分类"></a>高性能计算机体系结构分类</h2><h3 id="传统分类"><a href="#传统分类" class="headerlink" title="传统分类"></a>传统分类</h3><h4 id="尺寸"><a href="#尺寸" class="headerlink" title="尺寸"></a>尺寸</h4><ul><li>微型机</li><li>小型机<ul><li>VAX 11</li></ul></li><li>中型机<ul><li>IBM System/3</li></ul></li><li>大型机<ul><li>IBM Z9</li></ul></li><li>巨型机</li></ul><h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><ul><li>模拟计算机</li><li>数字计算机</li><li>混合计算机</li></ul><h4 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h4><ul><li>专用计算机</li><li>通用计算机</li></ul><h4 id="常见分类"><a href="#常见分类" class="headerlink" title="常见分类"></a>常见分类</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/classes_of_computers.png" alt="img"></p><ul><li>Internet of Things(IoT)/Embedded Computers<ul><li>connected to the Internet, typically wirelessly</li><li>Embedded Computers have the widest spread of processing power and cost(能耗和成本的分布很广，都是有低有高)<ul><li>Include 8-bit to 32-bit processors that may cost one peny, and high-end 64-bit processors for cars and network switches that cost $100(中控屏和交换机)</li><li>However, its primary goal is to meet the performance need at a minimum price</li></ul></li></ul></li><li>Personal Mobile Device(PMD)<ul><li>a collection of wireless devices with multimedia user interfaces, such as cell phones and tablet computers.</li><li>Application on PMDs are often web-based and media-oriented(应用通常是基于网络或面向多媒体)</li><li>Use the ability to run third-party software as dividing line between non-embedded and embedded computers<ul><li>能跑第三方软件的小型设备就是PMD，那这样树莓派也是PMD，只有裸机才是嵌入式设备？</li></ul></li><li>real-time, energy efficiency, memory usage minimization</li></ul></li><li>Desktop Computing(销量正在下降，不如手机和平板了)<ul><li>spans from low-end netbooks to high-end heavily configured workstation</li><li>Emphasis: optimize price-performance(强调性价比)<ul><li>performance is measured primarily in terms of computer performance and graphics performance(分为计算机本身的计算性能和图像处理性能)</li></ul></li><li>The newest, highest-performance microprocessors and cost-reduced microprocessors often appear first in desktop systems</li></ul></li><li>Servers<ul><li>to provide larger-scale and more reliable file and computig services(提高更大规模和更可靠的文件和计算服务)</li><li>Availability</li><li>Scalability</li><li>Throughout(吞吐量大)</li></ul></li><li>Clusters/Warehouse Scale Computers(largest of the clusters, tens of thousands of servers can act as one)<ul><li>collections of desktop computers or servers connected by local area networks to act as a single larger computer(多台计算机或服务器通过本地局域网连接在一起，表现为一台整体的计算机)<ul><li>each node runs its own operating system, and nodes communicate using a networking protocol</li></ul></li><li>price-performance and power are cirtical<ul><li>Supercomputers emphasize floating-point performance</li><li>Clusters emphasize interactive applications(交互式应用程序), large-scale storage(大规模存储), dependability(可靠性), and high Internet bandwidth(高带宽)</li></ul></li></ul></li></ul><h4 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h4><ul><li>由于计算机硬件技术的发展，划分标准需要跟随时代的变化<ul><li>有些上世纪的大型巨型计算机不如本世纪的中小型计算机</li></ul></li><li>不能反映机器的系统结构特征</li></ul><h3 id="Flynn分类法"><a href="#Flynn分类法" class="headerlink" title="Flynn分类法"></a>Flynn分类法</h3><ul><li>基于数据流和指令流的并行性关系<ul><li>指令流(Instruction): 机器执行的指令序列，即一系列将数据送入数据处理单元进行修改的命令</li><li>数据流(Data): 由指令流调用的数据序列，包括输入数据和中间结果，但不包括输出数据</li><li>多倍性(Multiple): 系统性能瓶颈部件上处于同一执行阶段的指令或数据的最大可能个数</li></ul></li><li>Michael J. Flynn提出，IEEE计算机体系结构技术委员会和ACM SIGARCH都他创立的<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Flynn.png" alt="img"></li></ul><h4 id="SISD"><a href="#SISD" class="headerlink" title="SISD"></a>SISD</h4><ul><li>单指令流和单数据流</li><li>传统冯诺依曼计算机(串行计算机)<ul><li>硬件上不支持任何并行化计算，所有指令串行执行</li><li>在一个时钟周期内只能执行一条数据流</li><li>早期的 IBM PC 机、Intel 8086/8088 微处理器、早期的巨型机<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/SISD.png" alt="img"></li></ul></li></ul><h4 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h4><ul><li>单指令流和多数据流</li><li>可实现数据级并行，对多个不同的数据流并行执行相同的数据处理操作<ul><li>主要适用于解决向量和矩阵等复杂的科学计算和大规模工程计算问题</li><li>如果没有和数据项一样多的ALU，需要将任务进行分割后分批并行处理</li><li>大多应用于数字信号处理、图像处理等领域<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/SIMD.png" alt="img"></li></ul></li><li><p>阵列计算机</p><ul><li>用一个单一的控制单元提供信号驱动多个处理单元同时运行<ul><li>每个处理器单元都由功能增强版的计算单元和本地内存组成</li><li>每个处理单元可以选择执行或者不执行控制器发出的指令流</li><li>处理单元之间通过互连网络连接</li></ul></li><li>发展前景不好</li><li>ILLIAC IV<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ILLIAC%20IV.png" alt="img"><ul><li>64个处理单元、64个处理单元存储器和存储器逻辑部件组成<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ILLIAC%20IV1.png" alt="img"><ul><li>64个处理部件$PE_0～PE_{63}$排列成8×8的方阵，任何一个$PE_i$只与其上、下、左、右4个近邻$(PE_i-8)\mod64$、$(PE_i+8)\mod64$、$(PE_i-1)\mod64$和$(PE_i+1)\mod64$直接相连</li><li>循此规则，南北方向上同一列的PE两端相连成一个环，东西方向上每一行的东端PE与下一行的西端PE相连，最下面一行的东端PE则与最上面一行的西端PE相连，从而构成一个闭合的螺线形状，所以称其为闭合螺线阵列<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/ILLIAC%20IV2.png" alt="img"></li></ul></li></ul></li></ul></li><li><p>向量计算机</p><ul><li>专门对向量进行处理的计算机</li><li>主要以流水线结构为主，以向量作为基本操作单元，操作数和结果都以向量的形式存在</li><li>向量的处理方法有多种不同的，主要包括横向，纵向和纵横处理方法</li><li>主要部件<ul><li>向量寄存器<ul><li>能存储由多个操作数组成的向量</li><li>长度：4-128 个 64 位元素不等</li></ul></li><li>向量化和流水化的功能部件<ul><li>对向量中的每个元素执行同样的操作</li></ul></li><li>向量指令<ul><li>对向量进行操作的指令，而非标量</li></ul></li><li>交叉存储器(interleaved memory)<ul><li>内存系统由多个内存体组成</li><li>每个内存体能够独立同时访问</li></ul></li><li>步长式存储器访问(strided memory access)<ul><li>能够访问向量中固定间隔(步长)的元素</li></ul></li><li>硬件散射/聚集(hardware scatter/gather, 与上面步长式互补)<ul><li>指对无固定间隔的数据进行读(gather)和写(scatter)</li></ul></li></ul></li><li>优点：<ul><li>速度快</li><li>容易使用</li><li>向量编译器擅长识别向量化的代码<ul><li>能识别不能向量化的循环，并提供循环不能向量化的原因</li><li>用户能对是否重写代码来支持向量化做出明智决定</li></ul></li><li>很高的内存带宽</li><li>充分利用高速缓存行中的每个元素</li></ul></li><li>缺点<ul><li>不能处理不规则的数据结构和其他并行结构</li><li>可扩展性受到限制<ul><li>能够处理更大问题的能力，新一代系统通过增加向量处理器的数目，而非增加向量长度来进行扩展</li></ul></li><li>因此对长向量的支持需要定制，非常昂贵</li></ul></li><li>Cray-1<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Cray_1.png" alt="img"><ul><li>向量寄存器组V<ul><li>由512个64位的寄存器组成，分成8块($V_0~V_7$)</li><li>每一个块称为一组向量寄存器(64个)，可存放长度(即元素个数)不超过64的向量</li><li>每个向量寄存器可以每拍向功能部件提供一个数据元素，或者每拍接收一个从功能部件发来的结果元素</li></ul></li><li>标量寄存器S<ul><li>8个: $S_0~S_7$，每个64位</li></ul></li><li>标量保存寄存器(后援寄存器, Scalar-save Register): 用于在标量寄存器和存储器之间提供缓存($T_0~T_{63}$)</li></ul></li><li>我国自主研发的第一台大型向量计算机“757”机</li></ul></li><li><p>阵列 vs 向量</p><ul><li>数据并行性<ul><li>阵列计算机以数据并行为主，将任务分配给多个处理器，每个处理器独立执行相同操作的不同数据元素。每个处理器通过独立处理多个数据元素来实现并行计算</li><li>向量计算机则通过执行相同的操作来同时处理大型连续数据向量，具有更高的数据并行性(感觉是不用大规模的处理器间通信)</li></ul></li><li>硬件架构<ul><li>阵列计算机通常是通过大量的相同处理器节点组成的，每个节点都有自己的本地存储器，可以独立访问和处理数据</li><li>向量计算机则具有特殊的硬件向量寄存器，可以在单个指令周期内同时处理整个向量。它们通常具有更高的数据带宽和更强大的浮点性能</li></ul></li><li>编程模型<ul><li>阵列计算机通常使用数据并行语言或库来编写程序，例如MATLAB和CUDA等。程序员需要显式地指定数据元素之间的并行性和通信操作(用的还是普通指令集？)</li><li>向量计算机有专门的向量指令集和编程模型</li></ul></li></ul></li></ul><h4 id="SIMT"><a href="#SIMT" class="headerlink" title="SIMT"></a>SIMT</h4><ul><li>单指令流和多线程流</li><li>SIMT是一种并行计算中使用的模型，主要是将SIMD与多线程结合在一起，广泛应用于GPU(图形处理器)上的计算单元中<ul><li>GPU拥有实时图形应用编程接口<ul><li>使用点、线、三角形来表示物体的表面</li><li>使用图形处理流水线将物体表面的内部表示转换为一个像素的数组(该像素数组可以在屏幕上显示)</li><li>流水线的许多阶段是可编程的，通过着色函数(shader)来说明</li></ul></li><li>GPU使用SIMD并行来优化性能<ul><li>对邻近元素使用相同的着色函数，等同于使用相同的控制流</li></ul></li><li>GPU严重依赖硬件多线程<ul><li>避免内存访问的延迟</li></ul></li><li>GPU不是纯粹的SIMD系统<ul><li>在一个给定的核上ALU使用了SIMD并行</li><li>GPU有很多个核，每个核都能独立执行指令流(多线程)</li></ul></li></ul></li><li>与SIMD相比<ul><li>更灵活<ul><li>SIMT允许一条指令的多数据分开寻址，而 SIMD 是必须连续在一起的片段</li><li>SIMT 可以支持编写线程级的并行代码，而 SIMD 不支持编写线程级的代码</li></ul></li><li>SIMD中的向量中元素相互之间可以自由通信(因为是取自相同的地址空间，元素是连续的？)</li><li>SIMT中的每个线程的寄存器都是私有的，线程之间只能通过共享内存和同步机制进行通信<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/SIMT.png" alt="img"></li></ul></li></ul><h4 id="MISD"><a href="#MISD" class="headerlink" title="MISD"></a>MISD</h4><ul><li>多指令流和单数据流</li><li>有人认为这种类型的计算机至今都未出现也有其他人认为有一些类似的例子，例如共享内存的多处理器系统和计算机中的流水线结构<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/MISD.png" alt="img"></li></ul><h4 id="MIMD"><a href="#MIMD" class="headerlink" title="MIMD"></a>MIMD</h4><ul><li>多指令流和多数据流: 每个处理器有自己的指令流，也可和其他处理器共享指令流，对自己的数据进行处理</li><li>多处理器(多核)，多计算机系统</li><li>应用最广泛的并行体系结构，现代流行的并行处理结构都可以划分为这一类<ul><li>超算、计算机集群、分布式系统、多处理器计算机和多核计算机<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/MIMD.png" alt="img"></li></ul></li></ul><h4 id="缺陷-1"><a href="#缺陷-1" class="headerlink" title="缺陷"></a>缺陷</h4><ul><li>分类的对象主要是控制驱动方式下的串行处理和并行处理计算机，对于非控制驱动方式的计算机不适合</li><li>把两个不同等级的功能并列对待导致MISD计算机不存在(存疑，有争议)</li><li>分类太粗，对流水线处理机的划分不明确，如标量流水线为SISD，向量流水线为SIMD</li></ul><h3 id="Flynn分类法扩展"><a href="#Flynn分类法扩展" class="headerlink" title="Flynn分类法扩展"></a>Flynn分类法扩展</h3><ul><li>根据不同的CPU是如何组织和共享内存的，将MIMD机器继续分类<ul><li>共享式内存</li><li>分布式内存</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/Flynn1.png" alt="img"></p><h4 id="共享式内存-Shared-Memory"><a href="#共享式内存-Shared-Memory" class="headerlink" title="共享式内存(Shared Memory)"></a>共享式内存(Shared Memory)</h4><ul><li>处理器之间共享内存，通过共享内存通信(所有处理器都通过软件或硬件的方式连接到一个全局可用的存储器)</li><li>所谓共享内存就是将所有的存储器抽象成一个整体地址<ul><li>即使物理上是分布的，但通过互联网或总线连接，可以抽象成逻辑上的全局存储器(主要针对分布式共享内存)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%85%B1%E4%BA%AB%E5%BC%8F%E5%86%85%E5%AD%98.png" alt="img"></li></ul></li><li>集中共享内存系统(Centralized Shared Memory, CSM)<ul><li>多处理器系统中处理器数目较少，所以处理器之间能够共享一个集中式的存储器</li><li>又称对称多处理器系统(Symmetric Multiprocessors, SMP)<ul><li>所有的存储器能够<strong>平等地互相访问</strong>，这就是对称一词的由来</li></ul></li><li>一致存储访问系统(Uniform Memory Access, UMA)<ul><li>因为每个处理器都能平等地访问存储器，所以它们<strong>访问存储器的延迟都是相同的</strong>，因此又被叫做一致存储访问系统</li></ul></li><li>每个处理器可以拥有<strong>私有内存或高速缓存</strong></li><li>多核芯片的集中共享内存系统<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/CSM1.png" alt="img"></li><li>多处理器的集中共享内存系统<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/CSM.png" alt="img"></li></ul></li><li>分布式共享内存系统(DSM, 非一致存储器访问系统(NUMA))<ul><li>每个处理器都拥有自己的存储器，也可以访问其他节点的存储器<ul><li>所有的处理器都能访问一个单一的地址空间</li><li>使用<code>LOAD</code>和<code>STORE</code>指令访问远程内存</li></ul></li><li>非一致存储访问系统(Non-Uniform Memory Access)<ul><li>每个结点访问本地内存和访问其它结点的远程内存的延迟是不同的</li><li>访问远程内存比访问本地内存延迟要高</li></ul></li><li>每个处理器可以使用高速缓存</li><li>NC-NUMA(不带缓存)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/NC_NUMA.png" alt="img"></li><li>CC-NUMA(带缓存)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/CC_NUMA.png" alt="img"></li><li>利用分布式共享内存技术，可以把几十个甚至上百个 CPU 集中在一台计算机中</li><li>目前几乎所有的多核心多处理器系统采用这种方式<ul><li>华为鲲鹏处理器</li><li>多路服务器</li></ul></li><li>但这种技术需要在处理器之间的数据传送和同步上花费更多的资源，因此设计的协议规则也更加复杂，需要在软件层面进行专门设计以充分提升分布式共享内存的带宽</li></ul></li></ul><h4 id="分布式内存-Distributed-Memory-也称为基于消息驱动Message-passing的计算机"><a href="#分布式内存-Distributed-Memory-也称为基于消息驱动Message-passing的计算机" class="headerlink" title="分布式内存(Distributed Memory, 也称为基于消息驱动Message-passing的计算机)"></a>分布式内存(Distributed Memory, 也称为基于消息驱动Message-passing的计算机)</h4><ul><li>处理器之间不共享内存，通过消息驱动通信<ul><li>对于要共享的数据，必须作为消息从一个处理器传递到另一个处理器</li><li>如果两个处理器之间没有运行软件协议加以辅助，那么无法互相访问数据</li></ul></li><li>在这种计算机体系结构中，每台计算机使用消息机制（如以太网）连接起来</li><li>每台计算机都有自己的处理器，每个处理器都有自己的私有内存，私有内存只提供自己的处理器进行访问。其他的计算机不能直接访问，每个计算机都有自己独立的物理地址空间<ul><li>与分布式共享内存不同，分布式共享内存处理器可以互相访问内存<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E5%86%85%E5%AD%98.png" alt="img"></li></ul></li><li>大规模并行处理器系统(Massively Parallel Processors, MPP)<ul><li>MPP系统是由成百上千台计算机组成的大规模高性能计算机系统<ul><li>MPP中一般每个节点可以认为是一个<strong>没有硬盘</strong>的计算机</li><li>MPP节点一般只驻留操作系统内核，由一条I/O总线连接到同一个硬盘上面</li><li>MPP使用的网络一般情况不是我们使用的高速以太网，它一般使用制造商专有的定制高速通信网络<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/MPP.png" alt="img"></li></ul></li><li>主要应用于科学技术、工程模拟等以计算为主的科研工作中</li><li>该系统一般开发困难，价格高，市场有限，是国家和公司综合实力的象征</li></ul></li><li>工作站机群系统(Cluster Of Workstations, COW)<ul><li>仓库级计算机 (Workstations Cluster)</li><li>由大量的家用计算机或者工作站通过商用网络连接在一起而构成的多计算机系统<ul><li>COW中每个节点都可以认为是一台独立的计算机，它们<strong>有自己的硬盘</strong>、CPU、存储器等，在商用网络的协作下组成一个工作站机群系统<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/COW.png" alt="img"></li></ul></li><li>COW 通常由多个服务器阵列排列而成<ul><li>服务器阵列是由多个机架排列而成，</li><li>机架是容纳服务器、交换机的外壳框架，一个机架上往往有多个服务器，服务器之间通过机架交换机(阵列交换机)进行通信<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/COW1.png" alt="img"></li></ul></li><li>随着云计算的蓬勃发展，COW正在变得越来越重要: 为全世界提供信息技术的基础</li><li>但COW的成本极高，它包含了机房、配电与制冷基础设施，服务器和联网设备，一般情况下一个COW能够容纳上万台服务器<ul><li>很多大公司的数据中心就是一个典型的例子</li></ul></li></ul></li><li>根据2018年6月全球超级计算机排行榜Top500名单中，有437台超算采用的是COW架构，另外63台采用的是MPP架构<ul><li>前十名中，有6台使用的是MPP架构，中国的神威·太湖之光使用的是MPP架构</li></ul></li><li>部分HPC采用的是COW的架构，但是一般不采用商业互联网和商业芯片，<strong>HPC一般使用定制的芯片和通信网络</strong><ul><li>HPC强调<strong>线程级并行或数据级并行</strong>，而COW则强调<strong>请求级并行</strong>，即可能有多个网络请求同时访问一台机器</li><li>HPC常常<strong>满负载</strong>持续数周完成大规模运行作业，而COW是面向并发请求的，通常<strong>不会满负载</strong></li></ul></li></ul><h3 id="按最大并行度的冯氏分类法"><a href="#按最大并行度的冯氏分类法" class="headerlink" title="按最大并行度的冯氏分类法"></a>按最大并行度的冯氏分类法</h3><h3 id="按并行度和流水线分类"><a href="#按并行度和流水线分类" class="headerlink" title="按并行度和流水线分类"></a>按并行度和流水线分类</h3><h3 id="按控制流和执行流分类"><a href="#按控制流和执行流分类" class="headerlink" title="按控制流和执行流分类"></a>按控制流和执行流分类</h3><h2 id="非冯·诺伊曼体系结构"><a href="#非冯·诺伊曼体系结构" class="headerlink" title="非冯·诺伊曼体系结构"></a>非冯·诺伊曼体系结构</h2><h3 id="冯·诺依曼体系结构"><a href="#冯·诺依曼体系结构" class="headerlink" title="冯·诺依曼体系结构"></a>冯·诺依曼体系结构</h3><ul><li>单处理机结构，机器以运算器为中心<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC.png" alt="img"></li><li>采用程序存储思想 <ul><li>指令和数据无差别的存储在存储器内 <ul><li>指令由操作码和操作数组成，都是顺序执行的 </li><li>数据以二进制形式表示<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC1.png" alt="img"></li></ul></li></ul></li><li>软件和硬件完全分离 </li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>从本质上讲是<strong>采取串行顺序处理</strong>的工作机制<ul><li>即使有关数据已经准备好，也必须逐条执行指令序列</li></ul></li><li>指令与数据在<strong>同一存储器</strong><ul><li>在高速运行时，<strong>不能达到同时取指令和取操作数</strong>，从而形成了<strong>传输过程的瓶颈</strong></li></ul></li></ul><h3 id="非冯·诺伊曼结构的改造思路"><a href="#非冯·诺伊曼结构的改造思路" class="headerlink" title="非冯·诺伊曼结构的改造思路"></a>非冯·诺伊曼结构的改造思路</h3><ul><li>对传统冯·诺依曼机进行改造，如采用多个处理部件形成流水处理</li><li>用多个冯·诺依曼机组成多机系统，支持并行算法结构</li><li>从根本上改变冯·诺依曼机的控制流驱动方式，如采用数据流</li></ul><h3 id="非冯·诺伊曼结构计算机典例"><a href="#非冯·诺伊曼结构计算机典例" class="headerlink" title="非冯·诺伊曼结构计算机典例"></a>非冯·诺伊曼结构计算机典例</h3><ul><li>归约机和数据流计算机是两种传统的非冯·诺伊曼结构的计算机，而量子计算机和光子计算机是近年来出现的新型计算机，他们从计算机的原理方面有很大的改变，几乎完全不同于冯·诺伊曼体系结构</li></ul><h4 id="归约机-Reduction-Machine"><a href="#归约机-Reduction-Machine" class="headerlink" title="归约机(Reduction Machine)"></a>归约机(Reduction Machine)</h4><ul><li>一种基于函数式语言编程的计算机，可根据表达式中的运算信息处理相应的数据</li></ul><h4 id="数据流计算机-Data-Flow-Computer"><a href="#数据流计算机-Data-Flow-Computer" class="headerlink" title="数据流计算机(Data Flow Computer)"></a>数据流计算机(Data Flow Computer)</h4><ul><li>一种基于数据流的计算机，每条指令的执行都是由数据来驱动的(来了什么数据用什么指令？不是根据指令取数据？)</li><li>数据流计算思想回答了 “<strong>一个运算操作能够被执行的充分条件是什么</strong>” 这个科学问题<ul><li>即<strong>数据就绪开始计算</strong>，同时提出了用数据流图描述计算任务的具体方法</li><li>创新之处就在于打破了传统串行执行指令的思维禁锢 </li></ul></li><li>数据流计算思想给云计算与大数据分析系统领域的编程模型带来了重大变化 <ul><li>指令是根据数据的可用性而不是按照严格的控制流顺序来执行的</li></ul></li></ul><h4 id="量子计算机-Quantum-Computer"><a href="#量子计算机-Quantum-Computer" class="headerlink" title="量子计算机(Quantum Computer)"></a>量子计算机(Quantum Computer)</h4><ul><li>一种基于量子逻辑的计算设备，利用不同量子态来记录状态，并且使用量子算法来操作数据</li></ul><h4 id="光子计算机-Optical-Computer"><a href="#光子计算机-Optical-Computer" class="headerlink" title="光子计算机(Optical Computer)"></a>光子计算机(Optical Computer)</h4><ul><li>以光子替代电子的先进计算机，将光子设备集成到当前的电子计算机中，形成光电混合的系统</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;高性能计算机的体系结构&quot;&gt;&lt;a href=&quot;#高性能计算机的体系结构&quot; class=&quot;headerlink&quot; title=&quot;高性能计算机的体系结构&quot;&gt;&lt;/a&gt;高性能计算机的体系结构&lt;/h1&gt;&lt;h2 id=&quot;超级计算机硬件&quot;&gt;&lt;a href=&quot;#超级计算机硬件&quot; c</summary>
      
    
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Security</title>
    <link href="http://zjn-astonishe.github.io/2024/10/10/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/2024-10-10-Security/"/>
    <id>http://zjn-astonishe.github.io/2024/10/10/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/2024-10-10-Security/</id>
    <published>2024-10-10T04:48:57.000Z</published>
    <updated>2024-11-09T04:33:57.470Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="What-does-network-security-include"><a href="#What-does-network-security-include" class="headerlink" title="What does network security include"></a>What does network security include</h3><h4 id="confidentiality-保密性，对消息加密"><a href="#confidentiality-保密性，对消息加密" class="headerlink" title="confidentiality(保密性，对消息加密)"></a>confidentiality(保密性，对消息加密)</h4><ul><li>research object: sender</li><li>but intended receiver should “understand” message contents<ul><li>sender encrypts message</li><li>receiver decrypts message</li></ul></li></ul><h4 id="authentication-身份认证"><a href="#authentication-身份认证" class="headerlink" title="authentication(身份认证)"></a>authentication(身份认证)</h4><ul><li>research object: sender, receiver<ul><li>want to confirm identity of each other</li></ul></li></ul><h4 id="message-integrity-消息完整"><a href="#message-integrity-消息完整" class="headerlink" title="message integrity(消息完整)"></a>message integrity(消息完整)</h4><ul><li>research object: sender, receiver<ul><li>want to ensure message not altered(in transit, or afterwards) without detection<ul><li>attacker may intercept, delete, add message<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E5%9C%BA%E6%99%AF%E6%A8%A1%E5%9E%8B.png" alt="img"></li></ul></li></ul></li></ul><h4 id="access-and-availability-可访问且可用"><a href="#access-and-availability-可访问且可用" class="headerlink" title="access and availability(可访问且可用)"></a>access and availability(可访问且可用)</h4><ul><li>services must be accessible and available to users</li></ul><h3 id="What-can-a-“bad-guy”-do"><a href="#What-can-a-“bad-guy”-do" class="headerlink" title="What can a “bad guy” do?"></a>What can a “bad guy” do?</h3><ul><li>eavesdrop(窃听): intercept message</li><li>alter(篡改): actively insert message into connection</li><li>impersonation(伪装): can fake(spoof) source address in packet(or any field in packet)</li><li>hijacking(劫持): “take over” ongoing connection by removing sender or receiver, inserting himself in place</li><li>denial of service(拒绝服务): prevent service from being used by others(like overloading resources, TCP flooding attack)</li></ul><h3 id="How-is-network-security-different-与普通安全相比"><a href="#How-is-network-security-different-与普通安全相比" class="headerlink" title="How is network security different?(与普通安全相比)"></a>How is network security different?(与普通安全相比)</h3><ul><li>From software and crypto(不仅和软件设计相关，还和密码学相关)</li><li>“Shared” resource(要进行资源共享)</li><li>Often “default open”(网路默认是对所有人开放的)</li><li>No built-in security in the beginning(起初就没考虑过安全问题), so it is vulnerable at every layer<ul><li>Network-layer(网络层)<ul><li>IP-level attack<ul><li>If you use IP Address for authentication, it will occur that ‘A’ receives responses from ‘S’, which are actually spoofed packets. But ‘A’ doesn’t know, so it will respond with a TCP Reset(RST) packet which closes the connection</li><li>Maybe you can solve it by overflowing the queues of ‘A’ with connection requests, because it is likely that ‘A’ drops the replies from ‘S’.</li><li>However, it also causes another attack, such as DDos attack.</li></ul></li><li>Routing attack<ul><li>篡改路由转发表</li></ul></li></ul></li><li>Transport-layer(传输层)<ul><li>针对三次握手协议，曾经是序号是从0开始的，所以会被预测。现在已经得到解决，使用随机序号</li></ul></li><li>Application-layer(应用层)<ul><li>DNS cache poisoning(利用DNS缓存机制下毒): 在向DNS服务器注册自己的DNS信息的时候附带进去对其他信息的错误重定向，使得用户在访问相关域名的时候被DNS服务器误导到攻击者指定的网站<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/DNS%20cache%20poisoning.png" alt="img"></li></ul></li></ul></li></ul><h2 id="Principles-of-cryptography"><a href="#Principles-of-cryptography" class="headerlink" title="Principles of cryptography"></a>Principles of cryptography</h2><h3 id="The-language-of-cryptography"><a href="#The-language-of-cryptography" class="headerlink" title="The language of cryptography"></a>The language of cryptography</h3><ul><li>假设有明文$m$，发送方有密钥$K_A$，接收方有密钥$K_B$。则发送方在发送时，用$K_A(m)$对明文进行加密，接收方接收到消息后，用$K_B(K_A(m))$将明文再解密出来</li></ul><h3 id="How-to-break-an-encryption-scheme"><a href="#How-to-break-an-encryption-scheme" class="headerlink" title="How to break an encryption scheme"></a>How to break an encryption scheme</h3><ul><li>cipher-text only attack(只知道密文)<ul><li>brute force: search through all keys(暴力破解，通常是穷举法)</li><li>statistical analysis(统计分析，例如福尔摩斯小说的跳舞小人)</li></ul></li><li>known-plaintext attack(已知明文攻击)<ul><li>根据明文和密文的对应关系(最简单的是解决凯撒加密法)</li></ul></li><li>chosen-plaintext attack(知道部分明文的密文)</li></ul><h3 id="Simple-encryption-scheme-substitution-cipher"><a href="#Simple-encryption-scheme-substitution-cipher" class="headerlink" title="Simple encryption scheme: substitution cipher"></a>Simple encryption scheme: substitution cipher</h3><ul><li>substituting one or more things for another(凯撒加密法)<ul><li>encryption key: mapping from set of plaintext’s letters to set of cipher-text’s letters, some even have unique design patterns </li></ul></li></ul><h3 id="Symmetric-key-cryptography-对称加密法，对称指的是密钥相不相同"><a href="#Symmetric-key-cryptography-对称加密法，对称指的是密钥相不相同" class="headerlink" title="Symmetric key cryptography(对称加密法，对称指的是密钥相不相同)"></a>Symmetric key cryptography(对称加密法，对称指的是密钥相不相同)</h3><ul><li>share same(symmetric) key: $K_S$</li><li>It is hard to agree on key in first place and without eavesdrop<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Symmetric%20key%20cryptography.png" alt="img"></li></ul><h4 id="DES-Data-Encryption-Standard"><a href="#DES-Data-Encryption-Standard" class="headerlink" title="DES: Data Encryption Standard"></a>DES: Data Encryption Standard</h4><ul><li>processes data in 64-bit blocks(一次处理64bit明文)</li><li>56-bit symmetric key(too simple, just need less than a day be decrypted through brute force)<ul><li>3DES: encrypt 3 times with 3 different keys</li></ul></li><li>block cipher with cipher block chaining(分块链式加密)<ul><li>initial permutation(交换中轴线分成的左右两边)</li><li>16 identical “rounds” of function Application, each using different 48 bits of key<ul><li>右侧数据直接复制作为下一层的左侧数据，本层所有数据和56-bit密钥中抽出形成的48-bit新密钥加密运算得到下一轮的右侧数据</li></ul></li><li>final permutation(交换中轴线分成的左右两边)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/DES.png" alt="img"></li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/715713979">DES具体算法</a></li></ul><h4 id="AES"><a href="#AES" class="headerlink" title="AES"></a>AES</h4><ul><li>processes data in 128 bit blocks(一次处理128bit明文)</li><li>128, 192, or 256 bit keys</li><li><a href="https://zhuanlan.zhihu.com/p/716048861">AES具体算法</a></li></ul><h3 id="Public-key-cryptography"><a href="#Public-key-cryptography" class="headerlink" title="Public key cryptography"></a>Public key cryptography</h3><ul><li>sender, receiver do not share secret key<ul><li>public encryption key known to all</li><li>private decryption key known only to receiver</li></ul></li><li>algorithms<ul><li>need $K_B^+(\cdot)$ and $K_B^-(\cdot)$: $K_B^-(K_B^+(m)) = m$</li><li>given public key $K_B^+$ and should be impossible to compute private key $K_B^-$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Public%20Key.png" alt="img"></li></ul></li></ul><h4 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h4><ul><li>Prerequisite: modular arithmetic<ul><li>$x\mod n$: remainder of x when divede by n(求余数)</li><li>facts:<ul><li>$[(a\mod n)+(b\mod n)]\mod n=(a+b)\mod n$</li><li>$[(a\mod n)-(b\mod n)]\mod n=(a-b)\mod n$</li><li>$[(a\mod n)<em>(b\mod n)]\mod n=(a</em>b)\mod n$<ul><li>$(a\mod n)^d\mod n=a^d\mod n$</li></ul></li><li>$x^y\mod n=x^{y \mod z}\mod n$<ul><li>$z = (p-1)(q-1)$ and $n = pq$<ul><li>$p$ and $q$ are two large prime numbers</li><li>欧拉函数特殊情况: $\phi(n)=n-1$, if n is a prime number</li><li><a href="https://zhuanlan.zhihu.com/p/151756874">欧拉函数和欧拉定理</a></li><li><a href="https://pages.cs.wisc.edu/~agorenst/rsa.pdf">证明的参考论文</a></li></ul></li></ul></li></ul></li></ul></li><li>Converting message into a number<ul><li>message is just a bit pattern, and it can be uniquely represented by an integer number, thus, encrypting a message is equivalent to encrypting a number<ul><li>to encrypt $m$, we encrypt the corresponding number, which gives a new number(the ciphertext)</li><li>$m$ can be converted into binary, and then into decimal</li></ul></li></ul></li><li>Creating public/private key pair(找一对足够大的互质数)<ul><li>choose two large prime numbers $p$, $q$(1024 bits each)</li><li>compute $n=pq$, $z=(p-1)(q-1)$</li><li>choose $e(e&lt;n)$ that has no common factors with $z$<ul><li>$e$, $z$ are relatively prime(相对质数)</li></ul></li><li>choose $d$ such that $ed-1$ is exactly divisible by $z$<ul><li>$ed\mod z = 1$</li></ul></li><li>public key is $K_B^+=(n, e)$</li><li>private key is $K_B^-=(n, d)$</li></ul></li><li>Encryption: <ul><li>$c = m^e\mod n$</li></ul></li><li>Decryption: <ul><li>$m = c^d\mod n = (m^e\mod n)^d\mod n = m^{ed}\mod n = m^{(ed\mod z)}\mod n = m\mod n = m$</li></ul></li><li>The result of using public key first, followed by private key is the same as the result of using private key first, followed by public key<ul><li>$K_B^-(K_B^+(m)) = m = K_B^+(K_B^-(m))$</li></ul></li><li>RSA is secure<ul><li>attacker just know total number $n=pq$ and a number $e$, but they don’t know $p$, $q$, $d$<ul><li>factoring a big number is hard</li></ul></li></ul></li><li>exponentiation in RSA is computationally intensive<ul><li>In practice, usinig public key crypto to establish secure connection, then establish second key – symmetric session key $K_S$ – for encrypting data</li></ul></li></ul><h2 id="Message-integrity-authentication"><a href="#Message-integrity-authentication" class="headerlink" title="Message integrity, authentication"></a>Message integrity, authentication</h2><ul><li>打过来显示的呼叫号码可以作假，但是自己手机打过去的号码是没问题的</li></ul><h3 id="Attempt-at-authentication-method"><a href="#Attempt-at-authentication-method" class="headerlink" title="Attempt at authentication method"></a>Attempt at authentication method</h3><ul><li>无论对消息中的标识加不加密发送，都容易遭受playback attack</li><li>采用nonce验证码的形式:<ul><li>nonce: number(R) used only once-in-a-lifetime</li><li>接收请求方向发送方发送一个验证码，<ul><li>发送方根据收到的验证码用两人都知道的密钥加密再发给接收方，由接收方比对<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ap4_0.png" alt="img"></li><li>发送方根据收到的验证码用自己的公钥加密发给接收方，接收方向发送方索要公钥后解密比对<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ap5_0.png" alt="img"></li></ul></li><li>由于所有信息都走的互联网(用手机发验证码也许不会？)，中间的攻击者也能收到这些信息，便可假扮双方骗取加密的密钥<ul><li>下图表示，攻击者收到Bob发送的$R$，用自己的私钥$K_T$加密$R$后发给Bob，但Bob不知道这个结果不适用正确用户Alice的密钥加密的，于是向攻击者索要公钥$K_T$，解密后当然没问题，Bob就把攻击者当成了Alice，用攻击者的密钥加密了信息，攻击者便窃取了信息。攻击者还可以把接收到的$R$发给Alice，Alice将用自己私钥加密后的$R$发给攻击者，接着攻击者便可向Alice索要公钥$K_A^+$，然后攻击者便可用公钥$K_A^+$加密篡改的消息$m’$假扮Bob发给Alice<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ap5_0_1.png" alt="img"></li></ul></li></ul></li></ul><h3 id="Public-key-certification"><a href="#Public-key-certification" class="headerlink" title="Public-key certification"></a>Public-key certification</h3><ul><li>上节的问题会出现是因为Bob并不知道攻击者发送的公钥$K_T$并不是真正用户Alice的公钥$K_A$</li></ul><h3 id="Certification-authorities-CA"><a href="#Certification-authorities-CA" class="headerlink" title="Certification authorities(CA)"></a>Certification authorities(CA)</h3><ul><li>CA: binds public key to particular entity, E.</li><li>E(person, router) registers its public key with CA<ul><li>E provides “proof of identity” to CA. </li><li>CA creates certificate binding E to its public key.</li><li>certificate containing E’s public key digitally signed by CA – CA says “this is E’s public key”<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CA.png" alt="img"></li><li>when want public key $K_B^+$<ul><li>get certificate</li><li>apply CA’s public key $K_{CA}^+$ to certificate, get public key $K_B^+ = K_{CA}^+(K_{CA}^-(K_B^+))$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CA1.png" alt="img"></li></ul></li></ul></li></ul><h3 id="Message-integrity-信息摘要，数字签名-digitally-signs"><a href="#Message-integrity-信息摘要，数字签名-digitally-signs" class="headerlink" title="Message integrity: 信息摘要，数字签名(digitally signs)"></a>Message integrity: 信息摘要，数字签名(digitally signs)</h3><ul><li>MD5(王晓云已破解，用另一串文本可以生成一样的MD5码)</li><li>SHA</li></ul><h2 id="Securing-e-mail"><a href="#Securing-e-mail" class="headerlink" title="Securing e-mail"></a>Securing e-mail</h2><h3 id="只加密邮件消息-send-confidential-e-mail"><a href="#只加密邮件消息-send-confidential-e-mail" class="headerlink" title="只加密邮件消息(send confidential e-mail)"></a>只加密邮件消息(send confidential e-mail)</h3><ul><li>sender <ul><li>generates random symmetric private key $K_S$</li><li>encrypts message $m$ with $K_S$(for efficiency)</li><li>encrypts $K_S$ with receiver’s public key $K_B^+$</li><li>sends both $K_S(m)$ and $K_B^+(K_S)$ to receiver</li></ul></li><li>receiver <ul><li>uses its own private key $K_B^-$ to decrypt $K_S$, and then uses $K_S$ to decrypt $m$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Secure%20email1.png" alt="img"></li></ul></li></ul><h3 id="只加密数字签名-send-authentication-message-integrity"><a href="#只加密数字签名-send-authentication-message-integrity" class="headerlink" title="只加密数字签名(send authentication message integrity)"></a>只加密数字签名(send authentication message integrity)</h3><ul><li>sender<ul><li>digitally signs message $m$ as $H(m)$</li><li>encrypts $H(m)$ with its own private key $K_A^-$</li><li>sends both message $m$(in the clear) and digital signature $K_A^-(H(m))$</li></ul></li><li>receiver<ul><li>uses sender’s public key $K_A^+$ to decrypt $K_A^-(H(m))$</li><li>digitally signs received message $m’$ as $H(m’)$</li><li>compares whether $H(m) = H(m’)$, if so, then $m=m’$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Secure%20email2.png" alt="img"></li></ul></li></ul><h3 id="两者都需加密-provide-secrecy-sender-authentication-message-integrity"><a href="#两者都需加密-provide-secrecy-sender-authentication-message-integrity" class="headerlink" title="两者都需加密(provide secrecy sender authentication, message integrity)"></a>两者都需加密(provide secrecy sender authentication, message integrity)</h3><ul><li>sender<ul><li>digitally signs message $m$ as $H(m)$</li><li>encrypts $H(m)$ with its own private key $K_A^-$</li><li>combine digital signature $K_A^-(H(m))$ and message $m$ together, then encrypts them with a symmetric private key $K_S$ which generates randomly</li><li>encrypts $K_S$ with receiver’s public key $K_B^+$</li><li>sends both $K_S(K_A^-(H(m))+m)$ and $K_B^+(K_S)$ to receiver</li></ul></li><li>receiver<ul><li>uses its own private key $K_B^-$ to decrypt $K_S$, and then uses $K_S$ to decrypt $m’$ and $K_A^-(H(m))$</li><li>uses sender’s public key $K_A^+$ to decrypt $K_A^-(H(m))$</li><li>digitally signs received message $m’$ as $H(m’)$</li><li>compares whether $H(m) = H(m’)$, if so, then $m’=m$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Secure%20email3.png" alt="img"></li></ul></li></ul><h2 id="Securing-TCP-connections-TLS-SSL"><a href="#Securing-TCP-connections-TLS-SSL" class="headerlink" title="Securing TCP connections: TLS/SSL"></a>Securing TCP connections: TLS/SSL</h2><ul><li>provides<ul><li>confidentiality(保密性)</li><li>integrity(完整性)</li><li>authentication(认证)</li></ul></li><li>application domain<ul><li>Web e-commerce transactions(电子商务)<ul><li>minimum hassle in doing business with new merchant</li></ul></li><li>encryption(especially credit-card numbers)</li><li>Web-server or optional client authentication</li><li>optional client authentication</li></ul></li><li>original goal<ul><li>send byte stream &amp; interactive data</li><li>set of secret keys for entire connection(该协议用来交换整个连接过程要用到的密钥)</li><li>certificate exchange as part of protocol: handshake phase(连接建立握手阶段的互相认证)</li><li>available to all TCP applications<ul><li>provides application programming interface(API) to applications<ul><li>C and Java SSL libraries/classes readily available</li></ul></li><li>secure socket interface(TCP的接口)</li></ul></li></ul></li><li>SSL适用于TCP协议和HTTP1.0</li><li>TLS适用于TCP协议和HTTP2.0，TLS1.0即SSL3.0</li><li>QUIC适用于UDP协议和HTTP2.0以上如HTTP3.0，包含了TLS<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/SSLTLSQUIC.png" alt="img"></li></ul><h3 id="SSL-Secure-Sockets-Layer-已经基本淘汰"><a href="#SSL-Secure-Sockets-Layer-已经基本淘汰" class="headerlink" title="SSL: Secure Sockets Layer(已经基本淘汰)"></a>SSL: Secure Sockets Layer(已经基本淘汰)</h3><h4 id="Toy-SSL-a-simple-secure-channel-简单模型"><a href="#Toy-SSL-a-simple-secure-channel-简单模型" class="headerlink" title="Toy SSL: a simple secure channel(简单模型)"></a>Toy SSL: a simple secure channel(简单模型)</h4><ul><li>handshake: users use their certificates, private keys to authenticate each other and exchange shared secret(互相认证并共享后面要用的某些加密密钥)</li><li>key derivation: users use shared secret to derive set of keys(用共享密钥和一些附加信息生成各自的密钥集), through key derivation function(KDF)<ul><li>use different keys for message authentication code(MAC) and encryption(前者是消息完整性验证码，后者是加密数据)</li><li>$K_c$: encryption key for data sent from client to server </li><li>$M_c$: MAC key for data sent from client to server</li><li>$K_s$: encryption key for data sent from server to client</li><li>$M_s$: MAC key for data sent from server to client</li></ul></li><li>data transfer: data to be transferred is broken up into series of records(将数据分片传输)<ul><li>with instant messaging, we cannot do integrity check over all bytes sent before displaying, thus break stream in series of records<ul><li>each record carries a MAC</li><li>receiver can act on each record as it arrives</li></ul></li><li>use variable-length records to distinguish MAC from data(用长度位确认MAC和数据的分界)</li><li>put sequence number into MAC to prevent attacker capturing and re-ordering records<ul><li>$MAC = MAC(M_x, sequence || data)$(用密钥$M_x$计算加了编号后的数据的完整性验证码，但要注意传输的数据里不包括序号)</li></ul></li><li>use nonce to prevent attacker capturing and replaying record(一次性验证码) </li></ul></li><li>connection closure: special messages to securely close connection(用特殊的消息来确保安全地关闭连接)<ul><li>use record types, with one type for closure to prevent attacker forging TCP connection close segment(truncation attack，用约定好的加密的类型来表示结束)<ul><li>$MAC=MAC(M_x, sequence||type||data)$<br>length|type|data|MAC<br>:-:|:-:|:-:|:-:<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ToySSL.png" alt="img"></li></ul></li></ul></li><li>allow client and server to support different encryption algorithms and to choose together specific algorithm before data transfer(允许自行协商选择加密算法)</li></ul><h4 id="SSL-cipher-suite-密码套件"><a href="#SSL-cipher-suite-密码套件" class="headerlink" title="SSL cipher suite(密码套件)"></a>SSL cipher suite(密码套件)</h4><ul><li>cipher suite: algorithms that can be used for key generation, encryption, MAC, digital signature<ul><li>public-key algorithm<ul><li>RSA</li></ul></li><li>symmetric encryption algorithm<ul><li>DES: block</li><li>3DES: block</li><li>Rivest Cipher 2: block</li><li>Rivest Cipher 4: stream</li></ul></li><li>MAC algorithm</li></ul></li><li>negotiation between client and server<ul><li>client offers choice of cipher suite</li><li>server picks one </li></ul></li></ul><h4 id="Real-SSL"><a href="#Real-SSL" class="headerlink" title="Real SSL"></a>Real SSL</h4><ul><li>The purpose of handshake<ul><li>server authentication(服务器认证客户端，例如确定客户端的权限)</li><li>negotiation: agree on crypto algorithms(协商加密用到的算法)</li><li>establish keys(根据选定的算法生成密钥)</li><li>client authentication(optional, 可选客户端认证服务器)</li></ul></li><li>The step of handshake <ul><li>client sends list of algorithms it supports, along with client nonce(用一次性验证码是避免被攻击者发动重放攻击)<ul><li>client typically offers range of algorithms, some strong, some weak</li></ul></li><li>server chooses algorithms from list; sends back: choice + certificate + server nonce</li><li>client verifies certificate, extracts server’s public key, generates pre_master_secret, encrypts with server’s public key, sends to server</li><li>client and server independently compute encryption and MAC keys from pre_master_secret and nonces(前面是握手时的协商)</li><li>client sends a MAC of all the handshake messages to protect handshake from tampering(最后用MAC验证握手消息的完整性，就是用接收到的消息计算MAC，然后发回给服务器，与服务器正确的MAC的比对消息完整性)</li><li>server sends a MAC of all the handshake messages to protect handshake from tampering<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Real%20SSL%20connection.png" alt="img"></li></ul></li><li>SSL record protocol<ul><li>record header<ul><li>content type</li><li>version</li><li>length</li></ul></li><li>data fragment<ul><li>each SSL data fragment has $2^14(16K)$bytes </li></ul></li><li>MAC<ul><li>calculated by sequence number, MAC key $M_x$, and data fragment<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/SSL%20record%20protocol.png" alt="img"></li></ul></li></ul></li><li>SSL record format<ul><li>data and MAC use symmetric algorithm to be encrypted</li><li>record header don’t need to be encrypted<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/SSL%20record%20format.png" alt="img"></li></ul></li><li>key derivation<ul><li>client nonce, server nonce, and pre-master secret input into pseudo random-number generator to produce master secret(共享密钥的生成)</li><li>master secret and new nonces input into another random-number generator: “key block”(工作密钥集合)</li><li>key block sliced and diced(将工作密钥集按照顺序和长度划分各个要用到的密钥)<ul><li>client MAC key</li><li>server MAC key</li><li>client encryption key</li><li>server encryption key</li><li>client initialization vector (IV)</li><li>server initialization vector (IV)</li></ul></li></ul></li></ul><h3 id="TLS-Transport-layer-security"><a href="#TLS-Transport-layer-security" class="headerlink" title="TLS: Transport-layer security"></a>TLS: Transport-layer security</h3><ul><li>is a widely deployed security protocol above the transport layer<ul><li>supported by almost all browsers, web servers: https(port 443)</li></ul></li><li>provides<ul><li>confidentiality: via symmetric encryption</li><li>integrity: via cryptographic hashing</li><li>authentication: via public key cryptography</li></ul></li></ul><h4 id="Toy-TLS"><a href="#Toy-TLS" class="headerlink" title="Toy TLS"></a>Toy TLS</h4><ul><li>handshake<ul><li>B establishes TCP connection with A</li><li>B verifies that A is really A</li><li>B sends A a master secret key(MS), used to generate all other keys for TLS session</li><li>potential issues: 3 RTT before client can start receiving data(including TCP handshake)(时延太长)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/t-tls%20handshake.png" alt="img"></li></ul></li><li>cryptographic keys<ul><li>like <a href="#toy-ssl-a-simple-secure-channel简单模型">SSL</a></li></ul></li><li>encrypting data<ul><li>like <a href="#toy-ssl-a-simple-secure-channel简单模型">SSL</a></li><li>but record encrypted using symmetric key, $Kc$, passed to TCP:$K_c(length||type||data||MAC)$</li></ul></li><li>connection close<ul><li>like <a href="#toy-ssl-a-simple-secure-channel简单模型">SSL</a></li><li>For example: type 0 for data, type 1 for close</li></ul></li></ul><h4 id="TLS-1-3-cipher-suite"><a href="#TLS-1-3-cipher-suite" class="headerlink" title="TLS:1.3 cipher suite"></a>TLS:1.3 cipher suite</h4><ul><li>TLS: 1.3 (2018): more limited cipher suite choice than TLS 1.2 (2008)<ul><li>only 5 choices, rather than 37 choices(5种加密算法选择？)</li><li>requires Diffie-Hellman (DH) for key exchange, rather than DH or RSA(密钥交换的方法)</li><li>combined encryption and authentication algorithm (“authenticated encryption”) for data rather than serial encryption, authentication<ul><li>4 based on AES</li></ul></li><li>HMAC uses SHA (256 or 284) cryptographic hash function(MAC使用的是SHA)</li><li><a href="https://www.rfc-editor.org/rfc/rfc8446">rfc8446</a></li></ul></li><li>handshake<ul><li>1 RTT<ul><li>client TLS hello msg: <ul><li>guesses key agreement protocol, parameters</li><li>indicates cipher suites it supports</li></ul></li><li>server TLS hello msg chooses <ul><li>key agreement protocol, parameters</li><li>cipher suite</li><li>server-signed certificate</li></ul></li><li>client:<ul><li>checks server certificate</li><li>generates key</li><li>can now make application request (e.g.., HTTPS GET)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/1RTT.png" alt="img"></li></ul></li></ul></li><li>0 RTT<ul><li>initial hello message contains encrypted application data!<ul><li>“resuming” earlier connection between client and server (恢复早期连接)</li><li>application data encrypted using “resumption master secret” from earlier connection</li></ul></li><li>vulnerable to replay attacks!(对于重放攻击来说很脆弱)<ul><li>maybe OK for get HTTP GET or client requests not modifying server state<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/0RTT.png" alt="img"></li></ul></li></ul></li></ul></li></ul><h2 id="Network-layer-security-IPSec-Routing"><a href="#Network-layer-security-IPSec-Routing" class="headerlink" title="Network layer security: IPSec, Routing"></a>Network layer security: IPSec, Routing</h2><h3 id="Network-layer-security"><a href="#Network-layer-security" class="headerlink" title="Network layer security"></a>Network layer security</h3><ul><li>between two network entities:<ul><li>sending entity encrypts datagram payload(发送实体加密的数据负载), payload could be:<ul><li>TCP or UDP segment, ICMP message, OSPF message</li></ul></li><li>all data sent from one entity to other would be hidden<ul><li>web pages, e-mail, P2P file transfer, TCP SYN packets</li></ul></li></ul></li><li>blanket coverage</li></ul><h3 id="Virtual-Private-Networks-VPNs"><a href="#Virtual-Private-Networks-VPNs" class="headerlink" title="Virtual Private Networks(VPNs)"></a>Virtual Private Networks(VPNs)</h3><ul><li>motivation:<ul><li>insinstitutions often want private networks for security(机构需求拥有安全的内网)<ul><li>costly: separate routers, links, DNS infrastructure.(如果使用单独的专有设备去实现会很昂贵，因此采用软件实现的虚拟的私密内网)</li></ul></li><li>institution’s inter-office traffic is sent over public Internet instead(机构间的通信需要经过公网)</li></ul></li><li>implementation: <ul><li>encrypted before entering public Internet(在进入公网前进行加密)</li><li>logically separate from other traffic(逻辑上与其他流分开，方便后续解密)</li></ul></li><li>其实就是在终端或者网关上安装一个软件，以实现不在内网时依然可以通过公网访问内网</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN.png" alt="img"></p><h3 id="IPSec-services"><a href="#IPSec-services" class="headerlink" title="IPSec services"></a>IPSec services</h3><ul><li>including<ul><li>data integrity(数据完整性)</li><li>origin authentication(认证)</li><li>replay attack prevention(防止重放攻击)</li><li>confidentiality(保密性)</li></ul></li><li>transport mode<ul><li>IPsec datagram emitted and received by end-system(端到端发送和接收IPsec数据报)</li><li>protects upper level protocols(保护网络层及上层的协议)</li></ul></li><li>IPsec protocols<ul><li>Authentication Header (AH) Protocol: provides source authentication and data integrity but not confidentiality</li><li>Encapsulation Security Protocol(ESP): provides source authentication, data integrity, and confidentiality(more widely used than AH, 因为AH不支持保密性)</li></ul></li><li>tunneling mode(隧道模式)<ul><li>edge routers IPsec-aware(在边缘路由器上实现IPsec)<ul><li>tunnel mode with ESP(most common and most important)</li><li>tunnel mode with AH</li></ul></li><li>hosts IPsec-aware(在终端主机实现IPsec)<ul><li>host mode with ESP</li><li>tunnel mode with AH</li></ul></li></ul></li><li>IPsec peers can be <ul><li>two end systems, </li><li>two routers/firewalls, </li><li>a router/firewall and an end system</li></ul></li></ul><h4 id="Security-associations-SAs"><a href="#Security-associations-SAs" class="headerlink" title="Security associations(SAs)"></a>Security associations(SAs)</h4><ul><li>before sending data, “security association(SA)” established from sending to receiving entity(在发送数据之前要先在发送方和接收方之间建立SA)<ul><li>SAs are simplex: for only one direction(单向的，只有从发送者到接收者)</li></ul></li><li>ending, receiving entitles maintain state information about SA<ul><li>recall: TCP endpoints also maintaain state info</li><li>IP is connectionless while IPsec is connection-oriented(IP不是面向连接的，IPsec则是面向连接的)</li></ul></li><li>Example SA from R1 to R2(R1 stores for SA)<ul><li>32-bit SA identifier: Security Parameter Index(SPI, 32位的SA标识)</li><li>origin SA interface(200.168.1.100, 发送方的边缘路由接口)</li><li>destination SA interface(193.68.2.23, 接收方的边缘路由接口)</li><li>type of encryption used(like 3DES with CBC, 加密算法)</li><li>encryption key(加密密钥)</li><li>type of integrity check used(like HMAC with MD5, 数据完整性检验方法)</li><li>authentication key(完整性?认证密钥)</li><li>with n salespersons, there are $2+2n$ SAs in R1’s SAD<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/SA%20from%20R1%20to%20R2.png" alt="img"></li></ul></li></ul><h4 id="Security-Association-Database-SAD"><a href="#Security-Association-Database-SAD" class="headerlink" title="Security Association Database(SAD)"></a>Security Association Database(SAD)</h4><ul><li>endpoint holds SA state in security association database(SAD, 用于保存终端的SA状态), where it can locate them during processing</li><li>when sending IPsec datagram, The sending router accesses SAD to determine how to process datagram(发送IPsec数据报的时候，路由器访问SAD确认如何处理数据报，说明是考虑的tunnel mode with ESP)</li><li>when IPsec datagram arrives to the receiving router, router examines SPI in IPsec datagram, indexes SAD with SPI, and processes datagram accordingly(从IPsec数据报中找到SPI，即Security Parameter Index，然后根据Index查找SAD，最后根据查到的信息处理数据报)</li></ul><h4 id="IPsec-datagram-focus-on-tunnel-mode-with-ESP-for-now"><a href="#IPsec-datagram-focus-on-tunnel-mode-with-ESP-for-now" class="headerlink" title="IPsec datagram(focus on tunnel mode with ESP for now)"></a>IPsec datagram(focus on tunnel mode with ESP for now)</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IPsec%20datagram.png" alt="img"></p><ul><li>ESP trl(trailer): Padding for block ciphers(尾部填充，用于分块加密凑长度)<ul><li>appends to back of original datagram (which includes original header fields!) an “ESP trailer” field. </li></ul></li><li>encrypts result using algorithm &amp; key specified by SA.(对整个数据报和填充的ESP trl加密)</li><li>ESP hdr(header):<ul><li>appends to front of this encrypted quantity the “ESP header, creating “enchilada”(墨西哥辣肉馅玉米卷). </li><li>SPI, receiving entity knows what to do</li><li>Sequence number: thwart replay attacks(应对重放攻击，因为如果收到同样序号的文件大概率会默认丢弃)<ul><li>for new SA, sender initializes seq# to 0(从0开始计数)</li><li>each time datagram in sent on SA:<ul><li>sender increments seq# counter(序号靠计数器累加)</li><li>places value in seq# field(序号靠直接分配)</li></ul></li><li>goal:<ul><li>prevent attacker from sniffing and replaying a packet</li><li>destination checks for duplicates</li></ul></li><li>method:<ul><li>receipt of duplicate, authenticated IP packets may disrupt service(重复的通过验证的IP数据报引起中断服务)</li><li>doesn’t keep track of all received packets; instead uses a window(不持续跟踪所有接收到的数据报，只在接收窗口检查序号？)</li></ul></li></ul></li></ul></li><li>creates authentication MAC over the whole enchilada, using algorithm and key specified in SA; (对ESP hdr+IP数据报+ESP trl做完整性认证)<ul><li>MAC in ESP auth field is created with shared secret key</li></ul></li><li>ESP auth: appends MAC to back of enchilada, forming payload;(认证结果放在数据报后面)</li><li>new IP header: creates brand new IP header, with all the classic IPv4 header fields, which it appends before payload</li></ul><h4 id="Security-Policy-Database-SPD"><a href="#Security-Policy-Database-SPD" class="headerlink" title="Security Policy Database(SPD)"></a>Security Policy Database(SPD)</h4><ul><li>policy: <ul><li>For a given datagram, sending entity needs to know if it should use IPsec(确认是否使用了IPsec)</li><li>needs also to know which SA to use(使用的是哪个SA)<ul><li>may use: source and destination IP address; protocol number(通过IP header包含的源IP地址和目的IP地址，所用协议的序号)</li></ul></li></ul></li><li>info in SPD indicates “what” to do with arriving datagram(SPD决定要对数据报做什么动作) </li><li>info in SAD indicates “how” to do it(SAD说明要做的动作怎么做)</li></ul><h4 id="Internet-Key-Exchange-IKE"><a href="#Internet-Key-Exchange-IKE" class="headerlink" title="Internet Key Exchange(IKE)"></a>Internet Key Exchange(IKE)</h4><ul><li>manual keying is impractical for VPN with many endpoints, instead use IPsec IKE(Internet Key Exchange)</li><li>IKE message exchange for algorithms, secret keys, SPI numbers, and authenticating (prove who you are) with either<ul><li>pre-shared secret (PSK, 预公钥): both sides start with secret<ul><li>run IKE to authenticate each other and to generate IPsec SAs (one in each direction, 所以有两遍，一遍一个方向), including encryption, authentication keys</li></ul></li><li>PKI(pubic/private keys and certificates): both sides start with public/private key pair, certificate<ul><li>run IKE to authenticate each other, obtain IPsec SAs (one in each direction).</li><li>similar with handshake in SSL.</li></ul></li></ul></li><li>IKE has two phase<ul><li>phase 1: establish bi-directional IKE SA<ul><li>note: IKE SA different from IPsec SA</li><li>has two mode:<ul><li>aggressive mode: using fewer messages</li><li>main mode: providing identity protection and is more flexible</li></ul></li></ul></li><li>phase 2: ISAKMP is used to securely negotiate IPsec pair of SAs<ul><li>aka ISAKMP security association: Internet Security Association and Key Management Protocol(ISAKMP)</li></ul></li></ul></li></ul><h3 id="Routing"><a href="#Routing" class="headerlink" title="Routing"></a>Routing</h3><h4 id="Internet-approach-to-scalable-routing-增加路由的方法"><a href="#Internet-approach-to-scalable-routing-增加路由的方法" class="headerlink" title="Internet approach to scalable routing(增加路由的方法)"></a>Internet approach to scalable routing(增加路由的方法)</h4><ul><li>aggregate routers into regions known as “autonomous systems” (AS，聚合路由器形成自治系统) (a.k.a. “domains”)<ul><li>intra-AS (aka “intra-domain”，内部自治系统，管理同一IP网络号内的多个子网): routing among within same AS (“network”)<ul><li>all routers in AS must run same intra-domain protocol(相同域内协议)</li><li>routers in different AS can run different intra-domain routing protocols</li><li>gateway router(网关路由器，也称边缘路由器): at “edge” of its own AS, has link(s) to router(s) in other AS’es</li></ul></li><li>inter-AS (aka “inter-domain”，域间自治系统): routing among AS’es<ul><li>gateways perform inter-domain routing (as well as intra-domain routing，边缘路由器扮演域内域间两种角色)</li></ul></li></ul></li><li>The Internet comprises of Autonomous Routing Domains (ARDs)<ul><li>An ARD is a collection of resources under the administrative control of a single entity<ul><li>CMU network is an ARD</li><li>Routers, links, networks, etc</li><li>Policies, interconnections with other ARDs(ARDs们如何互联)</li><li>Big or small: Campus, corporate, ISP networks</li><li>Allocated numbers, names and addresses</li></ul></li></ul></li><li>scale: billions of destinations:(AS规模不能太大，否则一是记录路由表会占据内存，二是频繁交换路由表会覆盖有效链路通信)<ul><li>can’t store all destinations in routing tables!</li><li>routing table exchange would swamp links! </li></ul></li><li>administrative autonomy:<ul><li>Internet: a network of networks</li><li>each network admin may want to control routing in its own network</li></ul></li></ul><h4 id="Autonomous-Systems-自治系统"><a href="#Autonomous-Systems-自治系统" class="headerlink" title="Autonomous Systems(自治系统)"></a>Autonomous Systems(自治系统)</h4><ul><li>An Autonomous System (AS) is an ARD with an AS number assigned by IANA<ul><li>AS number(ASN): <ul><li>16-bit, 1 to 64511 are public, 64512 to 65535 are private<ul><li>CMU has ASN 9, UUNet has ASN 701, 702, 703, 704, 705(An organization can have multiple AS numbers)</li><li>This can be there is one AS number for each geographical region, one for America, one for Asia, one for Europe.</li><li>Can also be a result of consolidation of companies and their networks. So when ISPs merge right the bubble burst, they probably keep all the AS numbers from the individual companies. </li><li>Last count, there are more than 46,000 ASs (CIDR report, mar 2014)</li></ul></li><li>Not every ARD has a public AS number(不一定有ASN)<ul><li>Only if talks to more than one ASs(只在三个以上ASs的情景需要ASs拥有ASN)</li><li>Nowadays, must justify to IANA why you need one(名额有限，所以申请要向IANA说明原因)</li></ul></li></ul></li></ul></li></ul><h4 id="Interconnected-ASes"><a href="#Interconnected-ASes" class="headerlink" title="Interconnected ASes"></a>Interconnected ASes</h4><ul><li><p>forwarding tableL configured by intra-AS routing algorithms and inter-AS routing algorithms</p><ul><li>intra-AS routing determine entries for destinations within AS(内部自治系统的路由决定内部的目的地址)</li><li>inter-AS and intra-AS determine entires for external destinations(自治系统间的路由决定外部的目的地址)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/edge%20routing.png" alt="img"></li></ul></li><li><p>Example:</p><ul><li>AS1 inter-domain routing(边缘路由器) must:<ul><li>learn which destinations reachable through AS2, which through AS3(了解可通过其他邻域到达的目标)</li><li>propagate this reachablility info to all routers in AS1(让AS1中所有路由器知道可达的这些目标)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/inter-domian%20routing%20example.png" alt="img"></li></ul></li></ul></li></ul><h4 id="Most-common-intra-AS-routing-protocols"><a href="#Most-common-intra-AS-routing-protocols" class="headerlink" title="Most common intra-AS routing protocols"></a>Most common intra-AS routing protocols</h4><ul><li>RIP: Routing Information Protocol [RFC 1723]<ul><li>classic DV: DVs exchanged every 30 secs(distance vector, 距离向量)</li><li>no longer widely used(不怎么用了)</li></ul></li><li>EIGRP: Enhanced Interior Gateway Routing Protocol<ul><li>DV based</li><li>formerly Cisco-proprietary for decades (became open in 2013 [RFC 7868])</li></ul></li><li>OSPF: Open Shortest Path First  [RFC 2328]<ul><li>link-state routing(基于链路状态)</li><li>IS-IS protocol (ISO standard, not RFC standard) essentially same as OSPF</li></ul></li></ul><h4 id="Internet-inter-AS-routing-BGP"><a href="#Internet-inter-AS-routing-BGP" class="headerlink" title="Internet inter-AS routing: BGP"></a>Internet inter-AS routing: BGP</h4><ul><li>BGP(Boder Gateway Protocol, 边界网关协议, 域间路由协议): the de facto inter-domain routing protocol<ul><li>allows subnet to advertise its existence, and the destinations it can reach, to rest of Internet: “I am here, here is who I can reach, and how”</li><li>允许子网自发向其他广播自己的相关路由信息</li></ul></li><li>BGP provides each AS a means to:<ul><li>eBGP: obtain subnet reachability information from neighboring ASes(边缘路由器从相邻AS获取子网的可达性信息)</li><li>iBGP: propagate reachability information to all AS-internal routers.(边缘路由器将获得的可达性信息告知内部AS的所有路由器)</li><li>determine “good” routes to other networks based on reachability information and policy(基于可达性信息和策略决定一个路由是不是好的路由选择)</li></ul></li><li>BGP advertised route: prefix + attributes<ul><li>prefix: destination being advertised</li><li>two important attributes:<ul><li>AS-Path: list of ASes through which prefix advertisement has passed</li><li>NEXT-HOP: indicates specific internal-AS router to next-hop AS(下一跳的AS)</li></ul></li></ul></li><li>BGP policy-based routing:<ul><li>gateway receiving route advertisement uses import policy to accept/decline path (e.g., never route through AS Y).</li><li>AS policy also determines whether to advertise path to other neighboring ASes(是否告知相邻AS)</li></ul></li><li>Example1: 路径广告的运作原理<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/BGPExample1.png" alt="img"><ul><li>AS2的路由器2c通过eBGP接收到来自AS3路由器3a的路径广告(path advertisement)<code>AS3, X</code></li><li>基于AS2的策略，AS2路由器2c将接收到的<code>AS3, X</code>通过iBGP广播到AS2中的所有路由器</li><li>基于AS2的策略，AS2的路由器2a通过eBGP向AS1的路由器1c发出路径广告<code>AS2, AS3, X</code></li></ul></li><li>Example2: 网关路由器(gateway router)可能获得不止一条到达目的地的路径<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/BGPExample2.png" alt="img"><ul><li>AS1的网关路由器1c从AS2的网关路由器2a获得路径<code>AS2, AS3, X</code></li><li>AS1的网关路由器1c从AS3的网关路由器3a获得路径<code>AS3, X</code></li><li>基于AS1的策略，网关路由器1c会选择路径<code>AS3, X</code>(最短？)并将该路径通过iBGP告知AS1中的所有路由器</li></ul></li><li>Example3<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/BGPExample3.png" alt="img"><ul><li>AS1的路由器1d是OSPF域内路由器(OSPF intra-domain routing)，要到达AS1的路由器1c或AS3的路由器X，会使用自身的1号接口</li><li>AS1的路由器1a是OSPF域内路由器(OSPF intra-domain routing)，要到达AS1的路由器1c或AS3的路由器X，会使用自身的2号接口</li></ul></li><li>BGP messages: BGP messages exchanged between peers over TCP connection(TCP连接中的连接对互相交换BGP消息)<ul><li><code>OPEN</code>: opens TCP connection to remote BGP peer and authenticates sending BGP peer(打开连接并验证)</li><li><code>UPDATE</code>: advertises new path or withdraws old(广播新路径或撤回旧路径)<ul><li>Announced prefixes(通告前缀), a.k.a. NLRI(network layer reachability information) </li><li>Path attributes associated with annoucement(路径长度?下一跳地址?)</li><li>Withdrawn prefixes(撤回前缀)</li></ul></li><li><code>KEEPALIVE</code>: keeps connection alive in absence of UPDATES; also ACKs <code>OPEN</code> request(在没有<code>UPDATES</code>的情况下保持有效连接，确认<code>OPEN</code>的请求)</li><li><code>NOTIFICATION</code>: reports errors in previous msg; also used to close connection(报告错误消息，也用于关闭连接)</li></ul></li><li>Hot potato routing(烫山芋路由策略)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/hot%20potato%20routing.png" alt="img"><ul><li>AS2的路由器2d通过iBGP有两条到达AS3的路由器X的路径<ul><li>2d-&gt;2a-&gt;1c-&gt;3a-&gt;3d-&gt;X(选择这个，因为2d到2a比2d到2c成本低)</li><li>2d-&gt;2c-&gt;3a-&gt;3d-&gt;X</li></ul></li><li>choose local gateway that has least intra-domain cost (域内成本最低的本地网关，即使可能跳数会更多。e.g., 2d chooses 2a, even though more AS hops to X): don’t worry about inter-domain cost!</li></ul></li><li>ISP(互联网服务供应商)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/BGP%20policy.png" alt="img"><ul><li>The ISP can offer a service to its customers where the ISP creates and maintains CA certificates for the customers’ resources and ROAs for the customers’ prefixes.(为客户提供服务，为客户的资源创建和维护CA证书，为客户的前缀创建和维护ROA) </li><li>only wants to route traffic to/from its customer networks(只将路由信息发送给消费者网络，不发给其他提供转发服务的网络?即不会出现转发环) (does not want to carry transit traffic between other ISPs – a typical “real world” policy)</li><li>provider networks: 包含提供转发服务的网络</li><li>customer networks: 包含使用转发服务的网络</li><li>dual-homed: attached to two provider networks</li></ul></li><li>BGP route selection: router may learn about more than one route to destination AS, selects route based on(多条路径，即多个下一跳路由)<ul><li>local preference value attribute: policy decision</li><li>shortest AS-PATH (最短的路径)</li><li>closest NEXT-HOP router: hot potato routing (最低的成本)</li><li>additional criteria <ul><li>Drop routes with inaccessible Nexthops(不选择没有下一跳的路由器)</li><li>Prefer route with largest LocalPref(偏爱有最大优先级的路由器)</li><li>Prefer lowest origin type IGP&lt; EGP &lt; Incomplete</li><li>Prefer route with smallest MED Compare MEDs from same AS only</li><li>Prefer path with lowest IGP metric</li><li>Prefer path by lowest BGP IDs</li><li>(vendor-specific hacks …)</li></ul></li></ul></li></ul><h3 id="BGP-Attack"><a href="#BGP-Attack" class="headerlink" title="BGP Attack"></a>BGP Attack</h3><h4 id="BGP-Vulnerabilities"><a href="#BGP-Vulnerabilities" class="headerlink" title="BGP Vulnerabilities"></a>BGP Vulnerabilities</h4><ul><li>Routers run an operating system which hackers now target(路由器有操作系统，容易受到攻击)</li><li>Potential attack objectives<ul><li>Blackholing: make something unreachable(使消息无法到达目的地，如何进入转发死循环)</li><li>Redirection(重定向，篡改转发方向): congestion(拥塞), eavesdropping(窃听)<ul><li>Unauthorized origin ISP(未经验证的宣称，被误导)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/unauthorized%20origin%20ISP.png" alt="img"></li><li>AS-path truncation(截断)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/truncation.png" alt="img"></li><li>AS-path alteration(篡改)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/alteration.png" alt="img"></li></ul></li><li>Instability(不稳定)</li></ul></li></ul><h4 id="Attack-method"><a href="#Attack-method" class="headerlink" title="Attack method"></a>Attack method</h4><ul><li>Prefix Hijacking and Announcement of Unallocated Address Space <ul><li>an autonomous system (AS) accidentally or maliciously originates a prefix that it is not authorized (by the prefix owner) to originate(即实际不能到达该前缀对应的网络，因为没有被授权，会被拒绝)</li><li>IP route selection: most specific (i.e., longest) matching entry in a router’s FIB. Offending AS falsely announces a more-specific prefix, the longer, unauthorized prefix will be widely accepted and used to route data.(利用最长匹配规则，扩大前缀的掩码，误导路由选择错误的下一跳地址) </li><li>AS may also falsely originate allocated but currently unused address space(for send spam or for some other malicious purpose.)(伪装成合法的但暂时无人使用的地址)</li><li>sub-prefix hijack can be serious(可能很严重)</li><li>DOS, eavesdropping, misdirection to imposter servers (to steal login credentials or inject malware)(DOS攻击，窃取，误导以冒名顶替服务器从而窃取登录姘居或注入恶意软件)</li><li>defeat of IP reputation systems to launch spam email. (破坏IP的信誉系统以发送垃圾邮件)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Attack%20method1.png" alt="img"></li></ul></li><li>AS-path Modification<ul><li>AS-path是一个AS数字序列，表示数据将在互联网流过的路径，用于实现路由策略(reflect the business agreements and peering policies that have been negotiated between networks.<br>)</li><li>a malicious AS which receives a BGP update may illegitimately remove some of the preceding ASes in the AS_PATH attribute of the update to make the path length seem shorter.(由于BGP是广播的，因此可能有接收到BGP更新的恶意AS。它可以在更新AS-path属性时非法删除一些前面的AS，以使得通过自己的那条路径的长度看起来更短)<ul><li>illegitimately increase its revenue from its customers(从而可以非法增大它的客户的成本，因为实际路径长度没那么短)</li><li>may be able to eavesdrop on traffic that would otherwise not transit through their AS. (也能够窃听原本不会通过该恶意AS传输的流量，因为修改了传播路径)</li></ul></li><li>adversary AS replaces a prefix in a received update with a more-specific prefix(利用最长匹配原则，恶意AS将接收到的BGP更新消息中的前缀替换为更具体的前缀) and then forwards the update to neighbors.(然后将该更新消息转发给邻居) <ul><li>ASes on the internet would widely accept and use the adversary AS’s advertisement for the more-specific prefix. (于是根据最长匹配原则，接收到该更新消息的AS都会使用这个更具体的前缀)</li><li>The adversary would be able to force almost all traffic for the more-specific prefix to be routed via their AS. Therefore, the adversary can eavesdrop on the data without being detected, because data reach destination correctly!(恶意AS可以汇聚几乎所有的流量，因为都通过它更具体的前缀进行转发。从而能够窃听数据而不被发现，因为数据确实成功到达了目的地)</li></ul></li></ul></li><li>Route Leaks: is the propagation of routing announcements beyond their intended scope(路由的作用范围超出了预期)<ul><li>Peering policies often specify limits on what routing announcements will be accepted by each party. (限制路由广告)</li><li>a stub or customer AS should never be used to route between two transit ASes.(使用转发服务的AS不应该告知两个本身就已有连接的AS自己与它们各自的路由信息) </li><li>The result of a route leak can include redirection of traffic through an unintended path, (会有重定向问题)<ul><li>enable eavesdropping or malicious traffic analysis. (窃听或恶意流量分析)</li><li>causes blackholing and denial-of-service for the affected prefixes. (前缀出现黑点和拒绝服务)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/route%20leak.png" alt="img"></li></ul></li></ul></li><li>IP Address Spoofing &amp; Reflection Amplification Attack <ul><li>Spoofed Source Addresses(伪造源地址，可以发起DDoS攻击，即不断发送请求使服务器宕机)</li><li>Reflection and Amplification Attacks<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/DDos.png" alt="img"></li></ul></li></ul><h3 id="Securing-BGP"><a href="#Securing-BGP" class="headerlink" title="Securing BGP"></a>Securing BGP</h3><h4 id="BGP-Security-Requirements"><a href="#BGP-Security-Requirements" class="headerlink" title="BGP Security Requirements"></a>BGP Security Requirements</h4><ul><li>Verification of address space “ownership”(需要验证地址空间的所有权)</li><li>Authentication of Autonomous Systems (AS)(AS认证)</li><li>Router authentication and authorization (relative to an AS)(相对于AS来说进行路由器身份验证和授权，防止AS中的非法路由器?还是防止路由器并不是在AS中)</li><li>Route and address advertisement authorization(路由和地址的广告授权)</li><li>Route withdrawal authorization(路由撤回授权)</li><li>Integrity and authenticity of all BGP traffic on the wire(线路上所有BGP流量的完整性和真实性)</li><li>Timeliness of BGP traffic(BGP流量的及时性)</li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li>Authentication at BGP layer(BGP层的认证)<ul><li>MD5 checksum option protects TCP layer connection in BGP, provides authentication between BGP speakers(MD5校验和用来保护BGP中的TCP层连接，保障BGP路由之间的身份验证)<ul><li>Prevents external attacker from injecting bogus information into TCP connection, e.g., TCP poisoning(防止外部攻击者窃取TCP连接的数据报并注入虚假信息)</li><li>Does not provide authenticity for routing information, all 3 attacks are still possible!(但不提供路由信息的真实性认证，仍然可能受到除修改路由信息外其他攻击方式攻击)</li></ul></li></ul></li><li>Route filtering<ul><li>Use Internet routing registries<ul><li>Database of who owns what prefix</li></ul></li><li>Typically route filtering used for “business”<ul><li>don’t want to go through this AS</li><li>don’t want to reveal route to this AS</li></ul></li><li>Ingress filters: Does AS own the prefix? If no, don’t accept</li><li>Problem is incomplete databases, multiorigin conflicts etc</li></ul></li></ul><h4 id="S-BGP-design-overview"><a href="#S-BGP-design-overview" class="headerlink" title="S-BGP design overview"></a>S-BGP design overview</h4><ul><li>IPSec: authenticity and integrity of peer-to-peer communication, automated key management(用于对等通信的真实性和完整性认证，自动密钥的管理)</li><li>Public Key Infrasturctures(PKIs): secure identification of BGP speakers and of owners of AS’s and of address blocks(BGP发言人和AS所有者以及地址块所有者的安全识别)</li><li>Attestations -&gt; authorization: of the subject (by the issuer) to advertise specified address blocks(由发行人即主体发布指定地址块的广告)</li><li>Validation of UPDATEs: based on a new path attribute, using certificates and attestations(基于新的路径属性，使用证书和认证验证BGP的更新消息)<ul><li>Internet Corporation for Assigned Names and Numbers(ICANN，用于名称与数字地址分配的互联网机构) issues certificates(颁布证书) for AS ownership(AS所有权证书) to ISPs and organizations that run BGP</li><li>AS operators issue certificates to routers, as AS representatives(AS运营商作为AS代表向路由器颁发证书)</li><li>Holders of AS(or router) certificates generate route attestations(AS或路由的证书持有人生成路由证明), authorizing advertisement of a route by a specified next hop AS(授权指定的下一跳AS发布路由广告)</li><li>Route attestations are used to express a secure route as a sequence of AS hops(路由证明用于将安全路由表示为AS跳的序列)</li></ul></li><li>Distribution of countermeasure data(分布式的对策数据): certificates(证书), Certificate Revocation List (CRLs，证书吊销列表，即黑名单?), attestations(证明)</li></ul><h4 id="Registration-of-Route-Objects-in-Internet-Routing-Registries"><a href="#Registration-of-Route-Objects-in-Internet-Routing-Registries" class="headerlink" title="Registration of Route Objects in Internet Routing Registries"></a>Registration of Route Objects in Internet Routing Registries</h4><ul><li>Internet路由注册表中路由对象的注册</li><li>regional internet registries (RIRs, 区域互联网注册中心)<ul><li>ARIN in North America</li><li>LACNIC in Latin America,</li><li>RIPE in Europe</li><li>APNIC in Asia-Pacific</li><li>AfriNIC in Africa</li></ul></li><li>internet routing registries (IRRs, 互联网路由注册中心)<ul><li>RIPE NCC, APNIC, AfriNIC, and ARIN </li><li>major internet service providers (ISPs, 互联网服务供应商) </li></ul></li><li>Roles of RIRs and IRRs<ul><li>Maintain declarative data about internet resource allocations and routing policies(维护有关互联网资源分配和路由策略的声明性数据) </li><li>route objects available in the IRRs provide routing information declared by network operators (IRR中可用的路由对象提供由网络运营商声明的路由信息)</li><li>Merit’s Routing Assets Database (RADb) and other similar entities provide a collective routing information base consisting of registered (at their site) as well as mirrored (from the IRRs) data. (路由资产数据库RADb和其他类似实体提供一个由注册和镜像数据组成的集体路由信息库，其中注册数据在站点，镜像在IRR)</li><li>Network operators often obtain route object information from the IRRs and/or RADb, and they can make use of the data in the creation of prefix filters  in their BGP routers. (网络运营商经常从IRR或RADb获取路由对象信息，利用这些数据在BGP路由器中创建前缀过滤器，过滤掉不符合标准的过长前缀，防止最长匹配机制被利用)</li></ul></li><li>data exchanged in RIRs/IRRs<ul><li>Routing Policy Specification Language (RPSL, 路由策略规范语言)  (most RIR and ISP use)</li><li>Shared Whois Project (SWIP, 共享Whois项目)  (ARIN, LACNIC) </li></ul></li></ul><ul><li><p>greater efforts should be devoted to creating route origin authorizations (ROAs, 路由源授权)</p><ul><li>because RPKI provides a stronger authentication and validation framework for network operators than IRR(RPKI为网络运营商提供了比IRR更强的身份验证和确认框架)</li></ul></li><li><p>Security Recommendation 1: All internet number resources (e.g., address blocks and AS numbers) should be covered by an appropriate registration services agreement with an RIR, and all point-of-contact (POC) information should be up to date. The granularity of such registrations should reflect all sub-allocations to entities (e.g., enterprises within the parent organization, branch offices) that operate their own network services (e.g., internet access, DNS). </p><ul><li>所有的互联网号码资源(地址块，AS号码)都应该与RIR签订适当的注册服务协议</li><li>所有的联系点(POC)信息都应该是最新的</li><li>此类注册的粒度应该能反映对运营自己网络服务(如互联网接入、DNS)的实体(如母组织内的企业、分支机构)的所有子分配</li></ul></li><li>Security Recommendation 2: In the case of address block (NetRange) registration in ARIN, the originating autonomous system (origin AS) should be included. <ul><li>在ARIN中进行地址块(NetRange)注册的情况下，应该包括地址块对应的原始自治系统(原始AS)</li></ul></li><li>Security Recommendation 3: Route objects corresponding to the BGP routes originating from an AS should be registered and actively maintained in an appropriate RIR’s IRR. Enterprises should ensure that appropriate IRR information exists for all IP address space used directly and by their outsourced IT systems and services. <ul><li>与源自AS的BGP路由相对应的路由实体对象应该在恰当的RIR的IRR中注册和积极维护。</li><li>企业应该确保其外包的IT系统和服务直接使用的所有IP地址空间都有恰当的IRR信息(都合法)</li></ul></li></ul><h4 id="Certification-of-Resources-in-Resource-Public-Key-Infrastructure-（RPKI）"><a href="#Certification-of-Resources-in-Resource-Public-Key-Infrastructure-（RPKI）" class="headerlink" title="Certification of Resources in Resource Public Key Infrastructure （RPKI）"></a>Certification of Resources in Resource Public Key Infrastructure （RPKI）</h4><ul><li>资源公钥基础设施(RPKI)中的资源认证</li><li>RPKI is standards-based approach for providing cryptographically secured registries of internet resources and routing authorization(基于标准的方法，用于提供安全加密的互联网资源注册表和路由授权)</li><li>global certificate authority (CA, 全球证书颁发机构) and registry service offered by all regional internet registries (RIRs, 提供注册服务)<ul><li>five RIRs (AFRINIC, APNIC, ARIN, LACNIC, and RIPE) maintains an independent trust anchor (TA) for RPKI certification services in its respective region.(五个RIR为其各自地区的RPKI认证服务维护独立的信任锚) </li></ul></li><li>The Internet Assigned Numbers Authority (ICANA, 互联网号码分配机构) allocates resources to the regional internet registries (RIRs, 为区域互联网注册中心分配资源) <ul><li>Internet Corporation for Assigned Names and Numbers (ICANN, 互联网名称与号码分配机构)</li><li>RIRs suballocate to local internet registries (LIRs, RIRs分配给本地互联网注册中心, RIRs &gt; LTRs), </li><li>LIRs suballocate to ISPs and enterprises. (LIRs分配给ISPs互联网服务供应商和公司)</li></ul></li><li>RPKI is based on the X.509 standard with RFC 3779 extensions that describe special certificate profiles for internet number resources (prefixes and AS numbers) <ul><li>描述了互联网号码资源(如前缀和号码)的特殊证书配置文件</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RPKI.png" alt="img"></p><ul><li>Security Recommendation 4: Internet number resource holders with IPv4/IPv6 prefixes and/or AS numbers (ASNs) should obtain RPKI certificate(s) for their resources. (具有IPv4/IPv6前缀和/或AS号码ASN这些互联网号码资源的持有者应该为其拥有的这些资源获取RPKI证书，保证其合法性)</li><li>Security Recommendation 5: Transit providers should provide a service where they create, publish, and manage subordinate resource certificates for address space and/or ASNs suballocated to their customers.(传输服务供应商应该提供额外一项服务，为分配给客户的地址空间和/或ASN创建、发布和管理从属资源证书)</li></ul><h4 id="BGP-Origin-Validation-BGP-OV"><a href="#BGP-Origin-Validation-BGP-OV" class="headerlink" title="BGP Origin Validation (BGP-OV)"></a>BGP Origin Validation (BGP-OV)</h4><ul><li>BGP源验证</li><li>Once an address prefix owner obtains a CA certificate, they can generate an end-entity (EE) certificate and use the private key associated with the EE certificate to digitally sign a route origin authorization (ROA).(一旦地址前缀所有者获得了CA证书，就可以生成端实体EE证书，并使用与EE证书关联的私钥对路由源授权ROA进行数字签名) <ul><li>An ROA declares a specific AS as an authorized originator of BGP announcements for the prefix (ROA声明特定AS作为授权发起者，授权BGP发布前缀相关的广告)</li><li>ROAs can also be created (signed) by an ISP (transit provider) on behalf of its customer based on a service agreement provided that the ISP suballocated the address space to the customer. (如果ISP将地址空间重新分配给客户，则ISP或传输供应商也可以根据服务协议代表其客户创建或签署ROA)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Creation%20of%20Route%20Origin%20Authorization%20(ROA" alt="img">%20by%20prefix%20owner.png)</li></ul></li><li>Once created, RPKI data is used throughout the internet by relying parties (RPs).(一旦创建，依赖方RP会在整个互联网上使用RPKI数据) <ul><li>RPs, such as RPKI-validating servers, can access RPKI data from the repositories  using either the rsync protocol or the RPKI Repository Delta Protocol (RRDP) [RFC8182].(例如RPKI验证服务器的RP，可以使用rsync协议或RPKI存储库增量协议RRDP从存储库访问RPKI数据) <ul><li>The RRDP protocol is often called “delta protocol” as shorthand. A BGP router typically accesses the required ROA data from one or more RPKI cache servers that are maintained by its AS. (RRDP协议通常被速记为delta协议，BGP路由器通常从其AS维护的一个或多个RPKI缓存服务器访问所需的ROA数据)</li></ul></li></ul></li><li>The RPKI-to-router protocol is used for communication between the RPKI cache server and the router (RPKI到路由器协议用于RPKI缓存服务器和路由器之间的通信)</li><li>A BGP router can use the ROA information retrieved from an RPKI cache server to mitigate the risk of prefix hijacks and some forms of route leaks in advertised routes.(BGP路由器可以使用从RPKI缓存服务器检索到的ROA信息降低前缀劫持和广告路由器中某些形式的路由泄露风险)  </li><li><p>A BGP router would typically receive a validated white list of {prefix, maxlength, origin AS} tuples (derived from valid ROAs) from one or more RPKI cache servers. A BGP route is deemed to have a “Valid” origin if the {prefix, origin AS} pair in the advertised route can be corroborated with the white list (i.e., the pair is permissible in accordance with at least one ROA; (BGP路由器通常会从一个或多个RPKI缓存服务器接收经过验证的{prefix, maxlength, originAS}元组白名单，该白名单从有效的ROA派生。如果广告路由器中的{prefix, originAS}对可以用白名单验证，即根据至少一个ROA，该对是允许的，则BGP路由被视为具有”有效”的起点，)</p><ul><li>The router makes use of this white list with the BGP origin validation (BGP-OV) process depicted to determine the validation state of an advertised route [RFC6811].(路由器利用此白名单和描述的BGP源验证过程，即BGP-OV，来确定广告路由的验证状态) </li><li>A BGP route is deemed to have a “Valid” origin if the {prefix, origin AS} pair in the advertised route can be corroborated with the white list (i.e., the pair is permissible in accordance with at least one ROA; A route is considered “Invalid” if there is a mismatch with the white list (i.e., AS number does not match, or the prefix length exceeds maxlength) (如果广告路由中的{prefix, originAS}对可以用白名单证实，则BGP路由被称为具有有效的源，如果与白名单不匹配，则是无效的)</li><li>a route is deemed “NotFound” if the prefix announced is not covered by any prefix in the white list (i.e., there is no ROA that contains a prefix that equals or subsumes the announced prefix).(如果路由所宣布的前缀未被白名单中的任何前缀匹配，没有包含或等于所宣布前缀的ROA，则该路由被称为未发现NotFound)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RPKI%20data%20retrieval%2C%20caching%2C%20and%20propagation%20to%20routers.png" alt="img"></li></ul></li><li><p>The RPKI-based origin validation may be supplemented by validation based on IRR data.(基于RPKI的源验证可以通过基于IRR数据的验证来作为补充)</p><ul><li>Although RPKI-based BGP- OV is already implemented in commercial BGP routers[Juniper1] [Cisco1] [Patel] [Scudder] [NIST-SRx] [Parsons2] [goBGP] [RTRlib]. , the activation and ubiquitous use of RPKI and BGP-OV in BGP routers require motivation and commitment on the part.(即使基于RPKI的BGP-OV已经在商业BGP路由器中实现，但并没有完全覆盖所有的路由器，目前大概只有60%)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Algorithm%20for%20origin%20validation%20(based%20on%20RFC%206811" alt="img">.png)</li></ul></li><li>Security Recommendation 6: Resource holders should register Route Origin Authorization (ROA(s) in the global RPKI for all prefixes that are announced or intended to be announced on the public internet. (资源持有者应该在全球RPKI中注册路由源授权ROA，用于在公共互联网上宣布或计划宣布前缀)</li><li>Security Recommendation 7: Each transit provider should provide a service where they create, publish, and maintain ROAs for prefixes suballocated to their customers. Alternatively, as part of the service, customers can be allowed to create, publish, and maintain their ROAs in a repository maintained by the transit provider. (每个传输供应商都应该提供一项服务，在其中创建、发布和维护分配给用户的前缀ROA。或者作为服务的一部分，可以允许客户在运输供应商维护的存储库中创建、发布和维护它们的ROA。要么自己做，要么客户做)</li><li>Security Recommendation 8: If a prefix that is announced (or intended to be announced) is multi-homed and originated from multiple ASes, then one ROA per originating AS should be registered for the prefix (possibly in combination with other prefixes which are also originated from the same AS). (如果宣布或打算宣布的前缀是多归属的，并且源自多个AS，则每个源AS都应该为该前缀注册一个ROA，可能与源自同一AS的其他前缀组合注册到一个ROA)</li><li>Security Recommendation 9: When an ISP or enterprise owns multiple prefixes that include less-specific and more-specific prefixes, they should ensure that the more- specific prefixes have ROAs before creating ROAs for the subsuming less-specific prefixes. (当ISP或企业拥有多个不太具体和更具体的前缀时，应该在为包含不太具体的前缀的前缀创建ROA之前，确保更具体的前缀具有ROA，即先具体后不太具体)</li><li>Security Recommendation 10: An ISP should ensure that more specific prefixes announced from within their customer cone have ROAs prior to the creation of its own ROAs for subsuming less-specific prefix(es).(ISP应该确保在为包含不太具体的前缀创建ROA之前，其客户群体中宣布的更具体的前缀都具有ROA) </li><li>Security Recommendation 11: An ISP or enterprise should create an AS0 ROA (Route Origin Authorization) for any prefix that is currently not announced to the public internet. However, this should be done only after ensuring that ROAs exist for any more-specific prefixes subsumed(纳入) by the prefix that are announced or are intended to be announced. (ISP或企业为目前未向公共互联网公布的任何前缀创建AS0 ROA。但只有在确保已宣布或打算宣布的任何更具体的前缀都存在ROA后，才能这么做。先公布具体的，后未公布的)</li><li>Security Recommendation 12: A BGP router should not send updates with AS_SET or AS_CONFED_SET in them (in compliance with BCP 172 [RFC6472]). (BGP路由器不应该发送包含AS_SET或AS_CONFED_SET的更新)<ul><li>When an AS_SET is present in a BGP update, it is not possible to clearly determine the origin AS from the AS_PATH [RFC6811]. Thus, an update containing an AS_SET in its AS_PATH can never receive an assessment of “Valid” in the origin validation process(如果BGP更新中包含AS_SET，无法从AS_PATH中清楚地确定源AS。因此在AS_PATH中包含的AS_SET的更新在源验证过程中永远不会收到结果为”有效”的评估) </li></ul></li><li>Security Recommendation 13: ISPs and enterprises that operate BGP routers should also operate one or more RPKI-validating caches. (运营BGP路由器的ISP和企业也应该运行一个或多个RPKI验证缓存)</li><li>Security Recommendation 14: A BGP router should maintain an up-to-date white list consisting of {prefix, maxlength, origin ASN} that is derived from valid ROAs (Route Origin Authorization ) in the global RPKI. The router should perform BGP-OV. (BGP路由器应该维护一个最新的白名单，其中包括从全局RPKI中的有效ROA导出的{prefix, maxlength, origiin ASN}。路由器应该执行BGP-OV)</li><li>Security Recommendation 15: In partial/incremental deployment state of the RPKI, the permissible {prefix, origin ASN} pairs for performing BGP-OV should be generated by taking the union of such data obtained from ROAs, IRR data, and customer contracts. (在RPKI的部分部署或增量部署状态下，应该通过合并从ROA、IRR数据和客户合同中获得的数据来生成用于执行BGP-OV的可行的{prefix, origin ASN}对)</li><li>Security Recommendation 16: BGP-OV results should be incorporated into local policy decisions to select BGP best paths. (BGP-OV的结果应该纳入本地策略决策，以选择BGP最佳路径，保证路径的可靠性)</li></ul><h4 id="Minimize-Forged-Origin-Hijacks"><a href="#Minimize-Forged-Origin-Hijacks" class="headerlink" title="Minimize Forged-Origin Hijacks"></a>Minimize Forged-Origin Hijacks</h4><ul><li>a purposeful malicious hijacker can forge the origin AS of any update by prepending the number of an AS found in an ROA (Route Origin Authorization ) for the target prefix onto their own unauthorized BGP announcement. (有目的的恶意劫持者可以通过将目标前缀的ROA中发现的ASN预先添加到他们自己的未经授权的BGP广告中，伪造来自源AS的更新)<ul><li>With ROA-based origin validation alone, it is possible to prevent accidental misoriginations. (仅通过基于ROA的源验证，就可以防止意外的源错误)</li></ul></li><li>replace the prefix in the route with a more-specific prefix that has a length not exceeding the maxlength in the ROA. (将路由器中的前缀替换为长度不超过ROA最大长度的更具体的前缀，利用了最大匹配机制)<ul><li>subsumed(包含在) under the announced prefix</li></ul></li><li>Security Recommendation 17: The maxlength in the ROA should not exceed the length of the most specific prefix (subsumed under the prefix in consideration) that is originated or intended to be originated from the AS listed in the ROA. (ROA中的最大长度不应该超过源自或意图源自ROA中列出的AS的最具体前缀的长度)</li><li>Security Recommendation 18: If a prefix and select more-specific prefixes subsumed under it are announced or intended to be announced, then instead of specifying a maxlength, the prefix and the more-specific prefixes should be listed explicitly in multiple ROAs (i.e., one ROA per prefix or more-specific prefix). (对于被宣布或打算被宣布的一个前缀和其包含的更具体的前缀，与其指定最大长度，不如在多个ROA中明确列出前缀和更具体的前缀，即一个ROA一个前缀)</li></ul><h4 id="Enable-Prefix-Filters"><a href="#Enable-Prefix-Filters" class="headerlink" title="Enable Prefix Filters"></a>Enable Prefix Filters</h4><ul><li>Prefix Filtering with Lateral Peer</li><li>Prefix Filtering with Transit Provider </li><li>Prefix Filtering with Customer </li><li>Prefix Filtering Performed in a Leaf Customer Network </li><li>Role of RPKI in Prefix Filtering </li></ul><h4 id="AS-Path-Validation-Emerging-Future"><a href="#AS-Path-Validation-Emerging-Future" class="headerlink" title="AS Path Validation (Emerging/Future)"></a>AS Path Validation (Emerging/Future)</h4><h4 id="Generalized-TTL-Security-Mechanism-GTSM"><a href="#Generalized-TTL-Security-Mechanism-GTSM" class="headerlink" title="Generalized TTL Security Mechanism (GTSM)"></a>Generalized TTL Security Mechanism (GTSM)</h4><h4 id="Route-Leak-Detection-and-Mitigation"><a href="#Route-Leak-Detection-and-Mitigation" class="headerlink" title="Route Leak Detection and Mitigation"></a>Route Leak Detection and Mitigation</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Security&quot;&gt;&lt;a href=&quot;#Security&quot; class=&quot;headerlink&quot; title=&quot;Security&quot;&gt;&lt;/a&gt;Security&lt;/h1&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; cla</summary>
      
    
    
    
    <category term="高级计算机网络" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="高级计算机网络" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>如何提高性能</title>
    <link href="http://zjn-astonishe.github.io/2024/10/09/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-09-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD/"/>
    <id>http://zjn-astonishe.github.io/2024/10/09/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/2024-10-09-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD/</id>
    <published>2024-10-09T07:38:07.000Z</published>
    <updated>2024-11-09T04:33:57.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何提高性能"><a href="#如何提高性能" class="headerlink" title="如何提高性能"></a>如何提高性能</h1><h2 id="Take-Advantage-of-Parallelism"><a href="#Take-Advantage-of-Parallelism" class="headerlink" title="Take Advantage of Parallelism"></a>Take Advantage of Parallelism</h2><h3 id="Level-of-the-system"><a href="#Level-of-the-system" class="headerlink" title="Level of the system"></a>Level of the system</h3><ul><li>multiple processors and multiple storgae devices can be used</li><li>The workload of handling requests can then be spread among the processors and storage devices(分摊到各个处理器和存储设备)</li></ul><h3 id="Level-of-individual-processor"><a href="#Level-of-individual-processor" class="headerlink" title="Level of individual processor"></a>Level of individual processor</h3><ul><li>pipelining: the best-known example of ILP(流水线并行)</li></ul><h3 id="Level-of-detailed-digital-design"><a href="#Level-of-detailed-digital-design" class="headerlink" title="Level of detailed digital design"></a>Level of detailed digital design</h3><ul><li>set-associative caches use multiple banks of memory that are typically searched in parallel to find a desired item(使用能并行查找的缓存)</li></ul><h2 id="Principle-of-Locality-局部性原理"><a href="#Principle-of-Locality-局部性原理" class="headerlink" title="Principle of Locality(局部性原理)"></a>Principle of Locality(局部性原理)</h2><h3 id="The-principle-of-locality"><a href="#The-principle-of-locality" class="headerlink" title="The principle of locality"></a>The principle of locality</h3><ul><li>programs tend to reuse data and instructions they have used recently(倾向于使用刚使用过的数据、指令)</li></ul><h3 id="An-implication-of-locality"><a href="#An-implication-of-locality" class="headerlink" title="An implication of locality"></a>An implication of locality</h3><ul><li>predict with reasonable accuracy what instructions and data a program will use in the near future(预测要使用的数据、指令)<ul><li>based on its accesses in the recent past</li></ul></li></ul><h3 id="Two-different-types-of-locality"><a href="#Two-different-types-of-locality" class="headerlink" title="Two different types of locality"></a>Two different types of locality</h3><h4 id="Temporal-locality"><a href="#Temporal-locality" class="headerlink" title="Temporal locality"></a>Temporal locality</h4><ul><li>recently accessed items are likely to be accessed soon(例如循环指令)</li></ul><h4 id="Spatial-locality"><a href="#Spatial-locality" class="headerlink" title="Spatial locality"></a>Spatial locality</h4><ul><li>items whose addresses are near one another tend to be referenced close together in time(例如访问数组)</li></ul><h2 id="Foucus-on-the-Common-Case"><a href="#Foucus-on-the-Common-Case" class="headerlink" title="Foucus on the Common Case"></a>Foucus on the Common Case</h2><ul><li>in making a design trade-off, favor the frequent case over the infrequent case(优先考虑常见的例子)</li><li>The common case is often simpler and can be done faster than the infrequent case<ul><li>first optimize the more common case of no overflow, because overflow is rare(优先考虑不溢出时候的优化，因为溢出是非常特殊的情况，很少发生)</li></ul></li></ul><h2 id="并行算法-程序-的设计步骤"><a href="#并行算法-程序-的设计步骤" class="headerlink" title="并行算法(程序)的设计步骤"></a>并行算法(程序)的设计步骤</h2><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%AD%A5%E9%AA%A4.png" alt="img"></p><h3 id="任务划分-Partitioning"><a href="#任务划分-Partitioning" class="headerlink" title="任务划分(Partitioning)"></a>任务划分(Partitioning)</h3><ul><li>将整个计算分解为一些小的任务，其目的是尽量开拓并行执行的机会</li><li>一个并行程序通常同时存在数据和功能并行的机会<ul><li>功能并行的并行度通常比较有限，并且不会随着问题规模的扩大而增加；不同的函数所涉及的数据集的大小可能差异很大，因此也难于实现负载均衡(基本看消耗大的，时间长的)</li><li>数据并行有很好的可扩展性，易于实现负载均衡(容易平均分)</li></ul></li><li>合理性检查<ul><li>所划分的任务数是否高于目标机上处理器数目1-2个量级？否则缺少灵活性</li><li>划分是否避免了冗余的计算和存储要求？否则可扩展性不大，因为会随着问题规模的扩大增加更多的冗余</li><li>各任务的尺寸是否大致相当？否则很难负载均衡</li><li>划分的任务数是否与问题尺寸成比例？否则可扩展性不大<ul><li>理想情况下，问题尺寸的增加应该引起任务数量的增加而不是任务尺寸的增加。也就是需要执行的任务更多而不是一个任务要执行更久</li></ul></li><li>是否采用了几种不同的划分法？多角度看待更全面</li></ul></li></ul><h4 id="域分解-Domain-Decomposition-数据划分，大多数采用"><a href="#域分解-Domain-Decomposition-数据划分，大多数采用" class="headerlink" title="域分解(Domain Decomposition, 数据划分，大多数采用)"></a>域分解(Domain Decomposition, 数据划分，大多数采用)</h4><ul><li>划分对象是数据<ul><li>算法(程序)的输入数据</li><li>计算产生的中间结果</li><li>最后得到的输出数据</li></ul></li><li>步骤: <ul><li>分解与问题相关的数据，使得这些小的数据篇尽可能大小上大致相等</li><li>将每个计算关联到操作的数据上<ul><li>由此每个任务包括一些数据和对应操作</li><li>当一个操作需要别的任务的数据时，就会产生任务间通信需求</li></ul></li></ul></li></ul><h4 id="功能分解-Functional-Decomposition-计算划分"><a href="#功能分解-Functional-Decomposition-计算划分" class="headerlink" title="功能分解(Functional Decomposition, 计算划分)"></a>功能分解(Functional Decomposition, 计算划分)</h4><ul><li>关注于被执行的计算上，而非计算所需的数据(每个任务最好只做一种计算)</li><li>如果计算划分成功，再继续研究计算所需的数据，如果数据基本上不相交，意味着划分成功</li><li>不常用，主要用来分析域分解结果产生的问题</li></ul><h3 id="通信分析-Communication-analysis"><a href="#通信分析-Communication-analysis" class="headerlink" title="通信分析(Communication analysis)"></a>通信分析(Communication analysis)</h3><ul><li>分析确定各个任务执行中所需交换的数据和协调各个任务的执行，由此可检测上述任务划分的合理性</li></ul><h4 id="局部-全局通信"><a href="#局部-全局通信" class="headerlink" title="局部/全局通信"></a>局部/全局通信</h4><ul><li>局部通信: 每个任务只和较少的几个近邻通信</li><li>全局通信: 每个任务与很多其他任务通信</li></ul><h4 id="结构化-非结构化通信"><a href="#结构化-非结构化通信" class="headerlink" title="结构化/非结构化通信"></a>结构化/非结构化通信</h4><ul><li>结构化: 规整结构，如树、网格等</li><li>非结构化: 通信网络可能是任意图，如稀疏矩阵-向量乘(消息传递编程困难)</li></ul><h4 id="静态-动态通信"><a href="#静态-动态通信" class="headerlink" title="静态/动态通信"></a>静态/动态通信</h4><ul><li>静态: 通信伙伴身份不随时间改变，如矩阵相乘</li><li>动态: 通信伙伴的身份可能由运行时数据决定且是可变的，如15-puzzle问题，消息传递编程较困难</li></ul><h4 id="同步-异步通信"><a href="#同步-异步通信" class="headerlink" title="同步/异步通信"></a>同步/异步通信</h4><ul><li>同步: 接收方和发送方协同操作需要协同</li><li>异步: 接收方和发送方协同操作无需协同</li></ul><h4 id="one-way-two-way通信"><a href="#one-way-two-way通信" class="headerlink" title="one-way/two-way通信"></a>one-way/two-way通信</h4><ul><li>one-way: 通信存在生产者-消费者关系，如读写(阻塞？)</li><li>two-way: 通信只需要一方发起并完成，如只读(非阻塞？)</li></ul><h3 id="任务组合-Agglomeration"><a href="#任务组合-Agglomeration" class="headerlink" title="任务组合(Agglomeration)"></a>任务组合(Agglomeration)</h3><ul><li>按性能要求和实现的代价来考察前两阶段的结果，必要时可将一些小的任务组合成更大的任务以提高性能或减少通信开销</li></ul><h4 id="增加粒度"><a href="#增加粒度" class="headerlink" title="增加粒度"></a>增加粒度</h4><ul><li>大量细粒度任务有可能增加通信代价和任务创建代价，所以可以适当合并细粒度的任务</li><li>表-容效应(Surface-Volume Effect)<ul><li>一个任务通信需求比例于它所操作的子域的表面积，而计算需求却比例于子域的容积</li></ul></li></ul><h4 id="保持灵活性"><a href="#保持灵活性" class="headerlink" title="保持灵活性"></a>保持灵活性</h4><ul><li>可移植性</li><li>可扩展性</li></ul><h3 id="处理器映射-Mapping"><a href="#处理器映射-Mapping" class="headerlink" title="处理器映射(Mapping)"></a>处理器映射(Mapping)</h3><ul><li>将每个任务分配到一个处理器上，其目的是最小化全局执行时间和通信成本以及最大化处理器的利用率</li></ul><h4 id="映射策略"><a href="#映射策略" class="headerlink" title="映射策略"></a>映射策略</h4><ul><li>指定任务到哪个处理器上去执行，其主要目标是减少算法的总执行时间，策略有二：<ul><li>把那些可并发执行的任务放在不同的处理器上以增强并行度</li><li>把那些需频繁通信的任务置于同一个处理器上以提高局部性</li></ul></li></ul><h4 id="负载均衡-使得所有处理器完成等量的任务"><a href="#负载均衡-使得所有处理器完成等量的任务" class="headerlink" title="负载均衡(使得所有处理器完成等量的任务)"></a>负载均衡(使得所有处理器完成等量的任务)</h4><ul><li>减少同步等待的时间，这包括等待其它进程结束运行的时间和串行执行的代码部分(包括临界区代码和因数据相关造成的串行执行)</li><li>通常采用的一种策略是在任务分配中，先集中目标使负载尽量均衡，然后再对任务分配进行调整，使得交互尽量少</li></ul><h3 id="任务分配与调度"><a href="#任务分配与调度" class="headerlink" title="任务分配与调度"></a>任务分配与调度</h3><h4 id="静态调度-程序员完成"><a href="#静态调度-程序员完成" class="headerlink" title="静态调度(程序员完成)"></a>静态调度(程序员完成)</h4><ul><li>静态地为每个处理器分配连续的循环迭代(要求处理器计算能力同构)</li><li>轮转(将第i个循环迭代分配给第 $i \mod P$ 个处理器)</li></ul><h4 id="动态调度"><a href="#动态调度" class="headerlink" title="动态调度"></a>动态调度</h4><ul><li>基本自调度SS(Self Scheduling)：每个处理器空闲时从全局队列取一个任务</li><li>块自调度BSS(Block Self Scheduling)：每次取k个任务(块)</li><li>指导自调度GSS(Guided Self Scheduling) ：每次取剩余任务的 $\frac{1}{P}$</li><li>因子分解调度FS(Factoring Scheduling)：$C_i=\frac{R_i}{2P}$，每阶段所有处理器任务大小相等</li><li>梯形自调度TSS(Trapezoid Self Scheduling) ：连续的块之间的差距固定不变</li><li>安全自调度SSS(Safe Self Scheduling)：任务分配使得累计执行时间刚刚超过平均负载</li><li>亲和性调度AS(Affinity Scheduling)：分布式任务队列(本地调度+远程调度)</li><li>自适应耦合调度AAS(Adapt Affinity Scheduling)：初始分配不平衡、计算能力异构<ul><li>重载</li><li>轻载</li><li>常载</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何提高性能&quot;&gt;&lt;a href=&quot;#如何提高性能&quot; class=&quot;headerlink&quot; title=&quot;如何提高性能&quot;&gt;&lt;/a&gt;如何提高性能&lt;/h1&gt;&lt;h2 id=&quot;Take-Advantage-of-Parallelism&quot;&gt;&lt;a href=&quot;#Take-Adv</summary>
      
    
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="高级计算机体系结构" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>逻辑时间</title>
    <link href="http://zjn-astonishe.github.io/2024/09/29/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-09-29-%E9%80%BB%E8%BE%91%E6%97%B6%E9%97%B4/"/>
    <id>http://zjn-astonishe.github.io/2024/09/29/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-09-29-%E9%80%BB%E8%BE%91%E6%97%B6%E9%97%B4/</id>
    <published>2024-09-29T07:42:01.000Z</published>
    <updated>2024-11-09T04:33:57.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑时间"><a href="#逻辑时间" class="headerlink" title="逻辑时间"></a>逻辑时间</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>The concept of causality between events(事件的因果) is fundamental to the design and analysis of parallel and distributed computing and operating systems<ul><li>Usually causality is tracked using physical time, but in distributed systems, it is not possible to have a global physical time(分布式系统中很难使用统一的全局物理时钟)</li><li>As asynchronous distributed computations make progress in spurts, the logical time is sufficient to capture the fundamental monotonicity property associated with causality in distributed systems(逻辑时间足以捕捉分布式系统中与因果关系相关的基本单调性)</li></ul></li><li>three ways to implement logical time <ul><li>scalar time(标量时间)</li><li>vector time(矢量时间)</li><li>matrix time(矩阵时间)</li></ul></li><li>Causality among events in a distributed system is a powerful concept in reasoning, analyzing, and drawing inferences about a computation</li><li>The knowledge of the causal precedence relation among the events of processes helps solve a variety of problems in distributed systems, such as distributed algorithms design, tracking of dependent events, knowledge about the progress of a computation, and concurrency measures(因果进程关系很重要)</li></ul><h2 id="A-Framework-for-a-System-of-Logical-Clocks"><a href="#A-Framework-for-a-System-of-Logical-Clocks" class="headerlink" title="A Framework for a System of Logical Clocks"></a>A Framework for a System of Logical Clocks</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h4 id="Logical-clock"><a href="#Logical-clock" class="headerlink" title="Logical clock"></a>Logical clock</h4><ul><li>A system of logical clocks consists of a time domian $T$(逻辑时钟组成的系统)</li><li>a logical clock $C$<ul><li>$C$ is a function that maps an event $e$ in a distributed system to an element in the time domain $T$(一个事件映射一个逻辑时钟的时间戳), denoted as $C(e)$ and called the timestamp of $e$, and is defined as follows;<ul><li>$C:\Eta \rightarrowtail T$</li></ul></li><li>such that the following property is satisfied:<ul><li>for two events $e_i$ and $e_j$, $e_i\rightarrow e_j \Rightarrow C(e_i) &lt; C(e_j)$(互为因果的两个事件，因的逻辑时间应该比果的逻辑时间小)</li></ul></li></ul></li></ul><h4 id="Relation-“-lt-”"><a href="#Relation-“-lt-”" class="headerlink" title="Relation “&lt;”"></a>Relation “&lt;”</h4><ul><li>Elements of $T$ form a partically ordered set(部分有序集合) over a relation $&lt;$<ul><li>Relation $&lt;$ is called the “happened before” or “causal precedure”(在…前发生或因果序列). This relation is analogous to the earlier than relation provided by the physical time</li></ul></li></ul><h4 id="The-Clock-Consistency-Condition"><a href="#The-Clock-Consistency-Condition" class="headerlink" title="The Clock Consistency Condition"></a>The Clock Consistency Condition</h4><ul><li>This monotonicity property is called the clock consistency condition(时钟一致性的条件)<ul><li>When $T$ and $C$ satisfy the following condition, for two events $e_i$ and $e_j$, $e_i\rightarrow e_j \Leftrightarrow C(e_i) &lt; C(e_j)$, the system of clocks is said to be strongly consistent(强一致性，只有向右箭头的话只是弱一致性)</li></ul></li></ul><h3 id="Implementing-Logical-Clocks"><a href="#Implementing-Logical-Clocks" class="headerlink" title="Implementing Logical Clocks"></a>Implementing Logical Clocks</h3><ul><li>Implementation of logical clocks requires addressing two issues(解决两个问题)<ul><li>data structures</li><li>protocol</li></ul></li><li>Systems of logical clocks differ in their representation of logical time and also in the protocol to update the logical clocks(不同的逻辑时钟系统是逻辑时钟的表达(数据结构)和如何更新逻辑时钟的协议不同)</li></ul><h4 id="Data-Structures"><a href="#Data-Structures" class="headerlink" title="Data Structures"></a>Data Structures</h4><ul><li>data structures local to every process to represent logical time(每个进程的本地数据结构，用于表示逻辑时间)<ul><li>Each process $p_i$ maintains data structure that allow it the following two capabilities:</li><li>A logical local clock(逻辑本地时间): denoted by $lc_i$, which helps process $p_i$ measure its own progress(用于进程估量自己)</li><li>A logical global clock(逻辑全局时间): denoted by $gc_i$, which is a representation of process $p_i$’s local view of the logical global time.(用于进程自己估量全局)</li><li>Typically, $lc_i$ is a part of $gc_i$</li></ul></li></ul><h4 id="Protocol"><a href="#Protocol" class="headerlink" title="Protocol"></a>Protocol</h4><ul><li>a protocol to update the data structures to ensure the consistency condition(更新数据结构的数据和确保一致性条件的协议)<ul><li>The protocol ensures that a process’s logical clock, and thus its view of the global time, is managed consistently(一致地管理逻辑时钟和全局时间). The protocol consists of the following two rules:<ul><li>R1: This rule governs how the logical local clock is updated by a process when it executes an event(进程在执行事件的时候如何更新逻辑本地时间)</li><li>R2: This rule governs how a process updates its logical global clock to update its view of the global time and global progress(进程更新全局逻辑时钟，以更新全局时间和全局进程的视图)</li></ul></li></ul></li></ul><h3 id="Scalar-Time-标量时间-Lamport’s-Logical-Clocks"><a href="#Scalar-Time-标量时间-Lamport’s-Logical-Clocks" class="headerlink" title="Scalar Time(标量时间, Lamport’s Logical Clocks)"></a>Scalar Time(标量时间, Lamport’s Logical Clocks)</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><ul><li>Proposed by Lamport in 1978 as an attempt to totally order events in a distributed system(尝试对事件进行完全排序)</li><li>Time domain is the set of non-negative integers(时间域是非负整数的集合)</li><li>The logical local clock of a process $p_i$, and its local view of the global time are squashed into one integer variable $C_i$(本地角度对全局时间的看法)</li><li>Rules R1 and R2 to update the clocks are as follows:<ul><li>R1: Before executing an event(send, receive, or internal), process $p_i$ executes the following:(执行事件之前对时间进行增加)<ul><li>$C_i := C_i + d ; (d &gt; 0)$</li><li>In general, every time R1 is executed, $d$ can have a different value; however, typically $d$ is kept at 1(通常是加1)</li></ul></li><li>R2: Each message piggybacks the clock value of its sender at sending time.(每条信息都附带了它的发送者在发送时的时钟值) When a process $p_i$ receives a message with timestamp $C_{msg}$, it executes the following actions:(选取本地的时间戳和接收到的消息中的时间戳中更大的作为新的本地时间戳)<ul><li>$C_i = max(C_i, C_{msg})$</li><li>Execute R1(再更新后执行一次R1，因为接收到的消息是在OS层的，要交付给应用层的过程中需要触发事件)</li><li>Deliver the message<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/The%20space-time%20diagram%20of%20a%20distributed%20execution_scalar.png" alt="img"></li></ul></li></ul></li></ul><h4 id="Basic-Properties"><a href="#Basic-Properties" class="headerlink" title="Basic Properties"></a>Basic Properties</h4><ul><li>Consistency Property(一致性的特性)<ul><li>Scalar clocks satisfy the monotonicity and hence the consistency property<ul><li>for two events $e_i$ and $e_j$: $e_i\rightarrow e_j \Rightarrow C(e_i) &lt; C(e_j)$(弱一致性，因为反之不成立)</li></ul></li></ul></li><li>No Strong Consistency(非强一致性)<ul><li>The system of scalar clocks is not strongly consistent, that is for two events $e_i$ and $e_j$, $C(e_i) &lt; C(e_j)\nRightarrow e_i\rightarrow e_j$(无法根据逻辑时间的顺序推导出因果顺序)</li><li>The reason that scalar clocks are not strongly consistent is that the logical local clock and logical global clock of a process are squashed into one(进程的逻辑本地时钟和逻辑全局时钟压缩在一维标量), resulting in the loss causal dependency information among events at different processes(会出现因果依赖信息的损失)</li></ul></li><li>Total Ordering(完全排序)<ul><li>Scalar clocks can be used to totally order events in a distributed system</li><li>The main problem in totally ordering events is that two or more events at different processes may have identical timestamp(但也存在问题：在不同的进程中可能会有相同时间戳的事件)</li><li>A tie-breaking mechanism(平局打破机制) is needed to order such events. A tie is broken as follows:<ul><li>Process identifiers are linearly ordered and tie among events with identical scalar timestamp is broken on the basis of their process identifiers(具有相同标量时间戳的事件之间根据其所在进程的标识符来区分，标识符是线性排序的)<ul><li>The lower the process identifier in the ranking, the higher the priority(进程标识符越小有越高的优先权)</li></ul></li><li>The timestamp of an event is denoted by a tuple $(t, i)$ where $t$ is its time of occurence and $i$ is the identity of the process where it occurred(事件的时间戳表示需要加上进程的编号)</li><li>The total order relation $\prec$ on two events $x$ and $y$ with timestamps $(h, i)$ and $(k, j)$, respectively, is defined as follows:<ul><li>$x \prec y \Leftrightarrow (h&lt;k\quad or \quad (h = k\quad and\quad i &lt; j))$<ul><li>时间戳不同比大小，小的肯定排前面</li><li>时间戳相同，则比较进程号的大小，进程号小的排前面</li></ul></li></ul></li></ul></li></ul></li><li>Event Counting(事件计数，前提是累加值为1)<ul><li>If the increment value $d$ is always 1, the scalar time has the following interesting property: <ul><li>if event $e$ has a timestamp $h$, then $h-1$ represents the minimum logical duration(最小的逻辑持续), counted in units of events(前面事件的数量), required before producing the event e(or call it the height of the event e，事件e的高度)<ul><li>$h-1$ events have been produced sequentially(h-1个事件线性产生) before the event e regardless of the processes that produced these events(无论产生的顺序如何，这只是个整体集合)</li></ul></li></ul></li></ul></li></ul><h3 id="Vector-Time-Logical-Clocks"><a href="#Vector-Time-Logical-Clocks" class="headerlink" title="Vector Time(Logical Clocks)"></a>Vector Time(Logical Clocks)</h3><ul><li>The system of vector clocks was developed independently by Fidge, Mattern and Schmuck</li><li>In the system of vector clocks, the time domain is represented by a set of n-dimensional non-negative integer vectors(时间域用一系列n维的非负整数向量表示)</li><li>Each process $p_i$ maintains a vector $vt_i[1…n]$, where <ul><li>$vt_i[i]$ is the local logical clock of $p_i$ and describes the logical time progress at process $p_i$(真正的本地逻辑时间)</li><li>$vt_i[j]$ represents process $p_i$’s latest knowledge of process $p_j$ local time(本地视角下其他进程的逻辑时间)<ul><li>If $vt_i[j] = x$, then process $p_i$ knows that local time at process $p_j$ has progressed till $x$</li></ul></li></ul></li><li>The entire vector $vt_i$ constitutes $p_i$’s view of the global logical time and is used to timestamp events</li><li>Process $p_i$ uses the following two rules R1 and R2 to update its clock:(和标量时间的更新方法是类似的)<ul><li>R1: Before executing an event, process $p_i$ updates its local logical time as follows:(执行事件之前对逻辑时间进行累加)<ul><li>$vt_i[i]:=vt_i[i]+d (d &gt; 0)$</li></ul></li><li>R2: Each message $m$ is piggybacked with the vector clock $vt$ of the sender process at sending time. On the receipt of such a message $(m, vt)$, process $p_i$ executes the following sequence of actions:<ul><li>Update its global logical time as follows:<ul><li>$1\leq k\leq n : vt_i[k]:=max(vt_i[k], vt[k])$</li><li>Execute R1</li><li>Deliver the message $m$</li></ul></li></ul></li></ul></li><li>The timestamp of an event is the value of the vector clock of its process when the event is executed(一个事件的时间戳是所在进程的向量时钟在事件被执行时的值)<ul><li>Initially, a vector clock is [0, 0, 0, …, 0]</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Evolution%20of%20vector%20time.png" alt="img"></p><h4 id="Comparing-Vector-Timestamps-比较两个向量时间戳"><a href="#Comparing-Vector-Timestamps-比较两个向量时间戳" class="headerlink" title="Comparing Vector Timestamps(比较两个向量时间戳)"></a>Comparing Vector Timestamps(比较两个向量时间戳)</h4><ul><li>The following relations are defined to compare two vector timestamps, $vh$ and $vk$<ul><li>$vh = vk \Leftrightarrow \forall x: vh[x] = vk[x]$(相等)</li><li>$vh \leq vk \Leftrightarrow \forall x: vh[x] \leq vk[x]$(小于等于)</li><li>$vh &lt; vk \Leftrightarrow vh\leq vk$ and $\exists x: vh[x] &lt; vk[x]$(小于)</li><li>$vh \mid\mid vk \Leftrightarrow \neg (vh &lt; vk) \bigwedge \neg(vk &lt; vh)$(并发，则互相没有大小关系)</li></ul></li><li>If the process at which an event occurred is known, the test to compare two timestamps can be simplified as follows:<ul><li>if events $x$ and $y$ respectively occurred at processes $p_i$ and $p_j$ and are assigned timestamps $vh$ and $vk$, respectively, then<ul><li>$x \rightarrow y \Leftrightarrow vh[i] \leq vk[i]$(x和y有因果关系，且x为因，y为果。则x发生时所在进程的逻辑时间不大于y发生时所在进程的逻辑时间)</li><li>$x\mid\mid y \Leftrightarrow vh[i] &gt; vk[i] \bigwedge vh[j] &lt; vk[j]$(事件x和事件y是并发的，x不晚于y，y也不晚于x)</li></ul></li></ul></li></ul><h4 id="Properties-of-Vector-Time"><a href="#Properties-of-Vector-Time" class="headerlink" title="Properties of Vector Time"></a>Properties of Vector Time</h4><ul><li>Isomorphism(同构性，即事件的因果关系和向量时间的比较关系等价)<ul><li>If events in a distributed system are timestamped using a system of vector clocks, we have the following property:<ul><li>If two events $x$ and $y$ have timestamps $vh$ and $vk$, respectively, then:<ul><li>$x\rightarrow y \Leftrightarrow vh &lt; vk$</li><li>$x\mid\mid y \Leftrightarrow vh\mid\mid vk$</li></ul></li></ul></li><li>Thus, there is an isomorphism between the set of partially ordered events produced by a distributed computation and their vector timestamps(部分有序的事件和它们的向量时间戳同构)</li></ul></li><li>Strong Consistency(强一致性)<ul><li>The system of vector clocks is strongly consistent; thus, by examining the vector timestamp of two events, we can determine if the events are causally related(检测两个事件的向量时间戳，可以确定事件是否存在因果联系)</li><li>However, Charron-Bost showed that the dimension of vector clocks cannot be less than $n$, the total number of processes in the distributed computation, for this property to hold(但是向量时钟必须有进程个数的维度大小)</li></ul></li><li>Event Counting<ul><li>If $d=1$ (in rule R1), then the $i^{th}$ component of vector clock at process $p_i$, $vt_i[i]$, denotes the number of events that have occurred at $p_i$ until that instant(发生了多少个事件可以看向量时间戳中对应进程的维度部分)</li><li>So, if an event $e$ has timestamp $vh$, $vh[j]$ denotes the number of events executed by process $p_j$ that causally precedure $e$. Clearly, $\Sigma vh[j] - 1$ represents the total number of events that causally precede $e$ in the distributed computation</li></ul></li></ul><h4 id="Efficient-Implementations-of-Vector-Clock"><a href="#Efficient-Implementations-of-Vector-Clock" class="headerlink" title="Efficient Implementations of Vector Clock"></a>Efficient Implementations of Vector Clock</h4><ul><li>If the number of processes in a distributed computation is large, then vector clocks will require piggybacking of huge amount of information in messages(如果分布式计算中的进程数量很多，那么在传递消息的时候需要附带很多的向量时间戳信息)</li><li>The message overhead grows linearly with the number of processors in the system and when there are thousands of processors in the system, the message size becomes huge even if there are only a few events occurring in few processors(消息开销随着系统中的处理器的数量线性增长)</li><li>We discuss an efficient way to maintain vector clocks</li><li>Charron-Bost showed that if vector clocks have to satisfy the strong consistency property, then in general vector timestamps must be at least of size $n$, the total number of processes</li><li>However, optimizations are possible and next, and we discuss a technique to implement vector clocks efficiently</li></ul><h4 id="Singhal-Kshemkalyani’s-Differential-Technique-差分优化技术"><a href="#Singhal-Kshemkalyani’s-Differential-Technique-差分优化技术" class="headerlink" title="Singhal-Kshemkalyani’s Differential Technique(差分优化技术)"></a>Singhal-Kshemkalyani’s Differential Technique(差分优化技术)</h4><ul><li>Singhal-Kshemkalyani’s differential technique is based on the observation that between successive message sends to the same process, only a few entires of the vector clock at the sender process are likely to change(在向同一进程发送的连续消息之间，发送方进程中只有少量的向量时钟发生变化)</li><li>When a process $p_i$ sends a message to a process $p_j$, it piggybacks only those entires of its vector clock that differ since the last message sent to $p_j$(进程i发给进程j消息的时候，附带的只有那些与前一次发送时向量时钟不同的部分)<ul><li>If entries $i_1, i_2, …, i_{n_1}$ of the vector clock at $p_i$ have changed to $v_1, v_2, …,v_{n_1}$, respectively, since the last message sent to $p_j$, then process $p_i$ piggybacks a compressed timestamp of the form:<ul><li>$\{(i_1, v_1), (i_2, v_2), …, (i_{n_1}, v_{n_1})\}$ to the next message to $p_j$</li></ul></li></ul></li><li><p>When $p_j$ receives this message, it updates its vector clock as follows:(接收到后根据tuple进行更新)</p><ul><li>$vt_i[i_k] = max(vt_i[i_k], v_k)$ for $k=1, 2, …, n_1$</li></ul></li><li><p>Thus this technique cuts down the message size, communication bandwidth and buffer (to store message) requirements</p></li><li>In the worst of case, every element of the vector clock has been updated at $p_i$ since the last message to process $p_j$, and the next message from $p_i$ to $p_j$ will need to carry the entire vector timestamp of size $n$</li><li><p>However, on the average the size of the timestamp on a message will be less than $n$</p></li><li><p>Implementation of this technique requires each process to remember the vector timestamp in the message last sent to every other process</p><ul><li>Direct implementation of this will result in $O(n^2)$ storage overhead at each process</li></ul></li><li>Singhal and Kshemkalyani developed a clever technique that cuts down this storage overhead at each process to $O(n)$. The technique works in the following manner<ul><li>Process $p_i$ maintains the following two additional vectors:<ul><li>$LS_i[1…n]$: (‘Last Sent’)<ul><li>$LS_i[j]$ indicates the value of $vt_i[i]$ when process $p_i$ last sent a message to process $p_j$</li></ul></li><li>$LU_i[1…n]$: (‘Last Update’)<ul><li>$LU_i[j]$ indicates the value of $vt_i[i]$ when process $p_i$ last updated the entry $vt_i[j]$</li></ul></li><li>Clearly, $LU_i[i] = vt_i[i]$ at all times and $LU_i[j]$ needs to be updated only when the receipt of a mesage cause $p_i$ to update entry $vt_i[j]$.(LU只在接收消息的时候更新)</li><li>Also, $LS_i[j]$ needs to be updated only when $p_i$ sends a message to $p_j$(LS只在发送消息的时候更新)</li></ul></li></ul></li><li>Since the last communication from $p_i$ to $p_j$, only those elements of vector clock $vt_i[k]$ have changed for which $LS_i[j] &lt; LU_i[k]$ holds</li><li>Hence, only these elements need to be sent in a message from $p_i$ to $p_j$. When $p_i$ sends a message to $p_j$, it sends only a set of tuples<ul><li>$\{(x, vt_i[x])\mid LS_i[j] &lt; LU_i[x]\}$</li><li>as the vector timestamp to $p_j$, instead of sending a vector of $n$ entries in a message</li></ul></li><li>Thus the entire vector of size $n$ is not sent along with a message. Instead, only the elements in the vector clock that have changed since the last message send to that process are sent in the format $\{(p_1, latest_value), (p_2, latest_value), …\}$, where $p_i$ indicates that the $p_i^{th}$ component of the vector clock has changed(在LS和LU找不同，发送的消息前者是不同的位置，后者是要更新的值)</li><li>This technique requires that the communication channels follow FIFO discipline for message delivery(消息传输遵守FIFO先进先出)</li><li>This technique substantially reduces the cost of maintaining vector clocks in large systems, especially if the process interations exhibit temporal or spatial localities(降低在大型系统中维护向量时钟的成本，特别是进程交互包含了时空位置)</li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Vector%20clocks%20progress%20in%20Singhal-Kshemkalyani%20technique.png" alt="img"></p><h3 id="Matrix-Time-Logical-Clocks"><a href="#Matrix-Time-Logical-Clocks" class="headerlink" title="Matrix Time(Logical Clocks)"></a>Matrix Time(Logical Clocks)</h3><ul><li>In a system of matrix clocks, the time is represented by a set of $n\times n$ matrices of non-negative integers</li><li>A process $p_i$ maintains a matrix $mt_i[1..n, 1..n]$, where:<ul><li>$mt_i[i, i]$ denotes the local logical clock of $p_i$ and tracks the progress of the computation at process $p_i$(进程i本地的逻辑时钟)</li><li>$mt_i[i, j]$ denotes the latest knowledge that process $p_i$ has about the local logical clock, $mt_j[j, j]$, of process $p_j$(进程i最近获得的关于其他进程的本地逻辑时钟，就相当于是向量时钟vector time)</li><li>$mt_i[j, k]$ represents the knowledge that process $p_i$ has about the latest knowledge that $p_j$ has about the local logical clock, $mt_k[k, k]$, of $p_k$(进程i最近获得的关于其他进程对再其他进程逻辑时钟)</li><li>The entire matrix $mt_i$ denotes $p_i$’s local view of the global logical time(整个矩阵时间表示的是进程对全局逻辑时钟的本地视角)</li></ul></li><li>Process $p_i$ uses the following rules R1 and R2 to update its clock:(更新规则)<ul><li>R1: Before executing an event, process $p_i$ updates its local logical time as follows:(执行事件之前更新本地的逻辑时钟)<ul><li>$mt_i[i, i] := mt_i[i, i] + d \quad (d &gt; 0)$</li></ul></li><li>R2: Each message $m$ is piggybacked with matrix time $mt$. When $p_i$ receives such a message $(m, mt)$ from a process $p_j$, $p_i$ executes the following sequence of actions:(接收消息之后更新全局逻辑时间，和标量向量时钟类似)<ul><li>Update its global logical time as follows:<ul><li>a. $1\leq k\leq n: mt_i[i, k]:=max(mt_i[i, k], mt[j, k])$<ul><li>That is, update its row $mt_i[i, *]$with the $p_j$’s row in the received timestamp, $mt$.(先更新本地视角的逻辑时钟)</li></ul></li><li>b. $1\leq k, l\leq n : mt_i[k, l]:=max(mt_i[k, l], mt[k, l])$(更新非本地视角的全局逻辑时钟，而且只需要更新所收消息进程编号之前的)</li></ul></li><li>Execute R1</li><li>Deliver message $m$</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Evolution%20of%20matrix%20time.png" alt="img"></p><ul><li>Let us consider the following events:<ul><li>$e_i^m$ which is the $(x_i^m)^{th}$ event at process $p_i$<ul><li>$e_k^1$ and $e_k^2$ which are the $(x_k^1)^{th}$ and $(x_k^2)^{th}$ event at process $p_k$</li><li>$e_j^1$ and $e_j^2$ which are the $(x_j^1)^{th}$ and $(x_j^2)^{th}$ event at process $p_j$</li></ul></li><li>$mt_e$ denote the matrix timestamp associated with event $e$</li></ul></li><li>Due to message $m_4$, $e_k^2$ is the last event of $p_k$ that causally precedes $e$, therefore, we have $mt_e[i, k] = mt_e[k, k] = x_k^2$<ul><li>Likewise, $mt_e[i, j] = mt_e[j, j] = x_j^2$</li></ul></li><li>The last event of $p_k$ known by $p_j$, to the knowledge of $p_i$ when it executed event, is e_k^1. Therefore, $mt_e[j, k] = x_k^1$<ul><li>Likewise, $mt_e[k. j] = x_j^1$</li></ul></li></ul><h4 id="Basic-Properties-1"><a href="#Basic-Properties-1" class="headerlink" title="Basic Properties"></a>Basic Properties</h4><ul><li>Vector $mt_i[i, .]$ contains all the properties of vector clocks(矩阵时钟在进程编号一行表示的就是向量时钟)</li><li>In addition, matrix clocks have the following property:<ul><li>$min_k(mt_i[k, l]) \geq t \Rightarrow process\quad p_i$ knows that every other process $p_k$ knows that $p_l$’s local time has progressed till $t$(在进程i的视角下，进程k所知道的直到时间t进程l所对应的逻辑时间)<ul><li>If this is true, it is clear that process $p_i$ knows that all other processes know that $p_l$ will never send information with a local time $\leq t$(如果不等式成立，则说明进程i知道其他所有的进程都知道进程l在时间t之前根本没有发送过任何消息)</li><li>In many applications, this implies that processes will no longer require from $p_l$ certain information and can use this fact to discard obsolete information(意味着可以丢弃关于进程l的消息，因为不太需要)</li></ul></li></ul></li><li>If $d$ is always 1 in the rule R1, then $mt_i[k, l]$ denotes the number of events occurred at $p_l$ and known by $p_k$ as far as $p_i$’s knowledge is concerned(如果d为1，则这个逻辑时间为执行的事件数)</li></ul><h3 id="Virtual-Time-虚拟时间"><a href="#Virtual-Time-虚拟时间" class="headerlink" title="Virtual Time(虚拟时间)"></a>Virtual Time(虚拟时间)</h3><ul><li>参考论文: <a href="https://cobweb.cs.uga.edu/~maria/pads/papers/p404-jefferson.pdf">Virtual Time </a></li></ul><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>主要用于进行事务处理(电商系统、数据库系统)</li><li>Virtual time system is a paradigm for organizing and synchronizing distributed systems</li><li>The implementation of virtual time using Time Warp mechanism works on the basis of an optimistic assumption(乐观假设)<ul><li>Time Warp relies on the general lookahead-rollback mechanism(往前回滚) where each process executes without regard to other processes having synchronization conflicts(不需要考虑与其他进程是否有同步矛盾)</li><li>If a conflict is discovered, the offending processes are rolled back to the time just before the conflict and executed forward along the revised path(出错就回滚)</li><li>Detection of conflicts and rollbacks are transparent to users</li><li>optimistic assumption: synchronization conflicts and thus rollbacks generally occurs rarely(理想情景是冲突和回滚很少发生)</li></ul></li></ul><h4 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h4><ul><li>Virtual time is a global, one dimensional, temporal coordinate system on a distributed computation to measure the computational progress and to define synchronization<ul><li>A virtual time system is a distributed system executing in coordination with an imaginary virtual clock that uses virtual time(使用想象的虚拟时钟记录虚拟时间)<ul><li>these local virtual clocks are loosely synchronized</li><li>these local virtual clocks move forward to higher virtual times, and occasionaly move backwards</li></ul></li><li>Virtual times are real values that are totally ordered by the less than relation “$&lt;$”(递增序)</li></ul></li><li>Processes run concurrently and communicate with each other by exchanging messages(进程并行运行，通过互相通信交换消息)<ul><li>Every message is characterized by four values:<ul><li>Name of the sender(发送者名字)</li><li>Virtual send time(虚拟发送时间)<ul><li>the virtual time at the sender when the message is sent</li></ul></li><li>Name of the receiver(接收者的名字)</li><li>Virtual receive time(虚拟接收时间)<ul><li>the virtual time when the message must be received(are processed)by the receiver(发送者指定的该消息最晚被接收处理的时间，是一个期望时间)</li></ul></li></ul></li></ul></li><li>Virtual time systems are subject to two semantic rules similar to Lamport’s clock conditions:<ul><li>Rule1: Virtual send time of each message &lt; virtual receive time of that message(发送时间肯定比接收时间早)</li><li>Rule2: Virtual time of each event in a process &lt; Virtual time of next event in that process(前一个事件的虚拟时间肯定比后一个的早)</li></ul></li><li>The above two rules imply that a process sends all messages in increasing order of virtual send time and a process receives(and processes) all messages in the increasing order of virtual receive time(发送消息事件的虚拟时间是递增的，接收消息事件的虚拟时间也应该是递增的)<ul><li>A problem arises when a message arrives at process late, that is, the virtual receive time of the message is less than the local virtual time at the receiver process when the message arrives(消息到得晚了，因为消息携带的期望接收时间比接收者接收消息时的本地虚拟时间更小，即消息错过了)</li></ul></li><li>Causality of events is an important concept in distributed systems and is also a major constraint in the implementation of virtual time(事件的因果关系)<ul><li>It is important an event that causes another should be completely executed before the caused event can be processed(因事件必须在果事件发生前结束)</li><li>The constraint in the implementation of virtual time can be stated as follows: “If an event A causes event B, then the execution of A and B must be scheduled in real time so that A is completed before B starts.<ul><li>events with virtual time $&lt; ‘t’$ complete before the starting of events at time ‘t’</li><li>events with virtual time $&gt; ‘t’$ will start only after events at time ‘t’ are complete</li></ul></li></ul></li></ul><h4 id="Characteristics"><a href="#Characteristics" class="headerlink" title="Characteristics"></a>Characteristics</h4><ul><li>Virtual time systems are not all isomorphic, it may be either discrete or continuous(虚拟时间可以是离散或连续的)</li><li>Virtual time may be only partially ordered(部分有序，因为有回滚?)</li><li>Virtual time may be related to real time or may be independent of it(可以用真实时钟作为虚拟时间，也可以不用)</li><li>Virtual time systems may be visible to programmers and manipulated explicitly as values, or hidden and manipulated implicity according to some system-defined discipline(程序员可见且显示赋值控制，或者隐式的由系统规则决定)</li><li>Virtual times associated with events may be explicitly caculated by user programs or they may be assigned by fixed rules(程序计算或者按规则固定分配)</li></ul><h4 id="Time-Warp-Mechanism"><a href="#Time-Warp-Mechanism" class="headerlink" title="Time Warp Mechanism"></a>Time Warp Mechanism</h4><ul><li>virtual receive time of message is considered as its timestamp</li><li>The necessary and sufficient conditions for the correct implementation of virtual time are that each process must handle incoming messages in timestamp order(正确实现虚拟时间的充要条件: 每个进程必须将接收到的消息按时间戳排序，递增序)<ul><li>This is highly undesirable and restrictive because process speeds and message delays are likely to highly variable. It natural for some processes to get ahead in virtual time of other processes(进程速度和消息延迟不同，因此很难满足充要条件)</li></ul></li><li>It is impossible for a process on the basis of local information alone to block and wait for the message with the next timestamp(不能仅依赖本地的虚拟时间)<ul><li>When a process executes a message, it is very difficult for it determine whether a message with an earlier timestamp will arrive later(central problem, 如何确定要处理的消息没有超时)<ul><li>It is always possible that a message with earlier timestamp arrives later</li></ul></li></ul></li><li>Time Warp Mechanism assumes that message communication is reliable, and messages may not be delivered in FIFO order(传输消息可靠，即不会丢包，但可能不会有序接收)</li><li>local control mechanism: insures that events are executed and messages are processed in the correct order(本地控制机制保证事件执行和消息处理顺序是正确的)<ul><li>There is no global virtual clock variable in this implementation, each process has a local virtual clock variable(各自进程持有本地虚拟时钟，没有全局的虚拟时钟)</li><li>The local virtual clock of a process doesn’t change during an event at that process but it chages only between events(虚拟时钟只在事件之间进行更新，即事件执行过程中不能更新时钟)</li><li>On the processing of next message from the input queue(接收队列), the process increases its local clock to the timestamp of the message (接收并处理消息的时候，将本地时钟更新为所接受消息携带的虚拟接收时间，这个时间是一个期望时间，发送者期望接收者接收消息的时间，或者说是最晚被接收的时间)<ul><li>When a message is sent, the virtual send time is copied from the sender’s virtual clock while the name of the receiver and virtual receive time are assigned based on application specific context</li></ul></li><li>At any instant, the value of virtual time may differ for each process but the value is transparent to other processes in the system(任何时刻，进程的虚拟时间可能和其他进程的虚拟时间不同，且是透明的，其他进程并不知道)</li><li>All arriving messages at a process are stored in an input queue in the increasing order of timestamps (receive times).(所有的接收消息会在队列中按照各自携带的虚拟接收时间排序，递减序)</li><li>The semantics of virtual time demands that incoming messages be received by each process strictly in the timestamp order.<ul><li>But processes will receive late messages due to factors such as different computation rates of processes and network delays.</li></ul></li><li>Runtime representation of a process is composed of the following:<ul><li>Process name: Virtual spaces coordinate which is unique in the system(唯一标识)</li><li>Local virtual clock: Virtual time coordinate</li><li>State: Data space of the process including execution stack, program counter and its own variables(用于后面回滚恢复的记忆上下文)</li><li>State queue: Contains saved copies of process’s recent states as roll back with Time warp mechanism requires the state of the process being saved(最近执行过的事件的状态副本队列)</li><li>Input queue: Contains all recently arrived messages in order of virtual receive time. Processed messages from the input queue are not deleted as they are saved in the output queue with a negative sign (antimessage) to facilitate future roll backs.(输入队列，执行完后建立个副本，并将标记取反放到输出队列作为回滚的备份)<ul><li>Whenever a process sends a message, a copy of the message is transmitted to receiver’s input queue and a negative copy (antimessage) is retained in the sender’s output queue for use in sender rollback.</li></ul></li><li>Output queue: Contains negative copies of messages the process has recently sent in virtual send time order. They are needed in case of a rollback.</li></ul></li><li>When a message arrives at the input queue of a process with timestamp greater than virtual clock time of its destination process, it is simply enqueued.</li><li>When the destination process’ virtual time is greater than the virtual time of message received, the process must do a rollback.(接收消息的期望时间比当前进程的虚拟时间小，说明超时了，就要回滚)</li><li>Antimessages: For every message, there exists an antimessage that is the same in content but opposite in sign(内容一样，但标记相反，例如是正负标记)<ul><li>Whenever a message and its antimessage appear in the same queue no matter in which order they arrived, they immediately annihilate each other resulting in shortening of the queue by one message.(标记相反的相同消息出现在同一个队列，会被抵消掉，当作无事发生)</li><li>Advantages:<ul><li>It is extremely robust and works under all possible circumstances.(很稳定)</li><li>It is free from deadlocks as there is no blocking.(不会死锁，因为没有阻塞)</li><li>It is also free from domino effects.(不会有多米诺骨牌效应，即不会一个出错连带所有出错，因为可以级联回滚)</li><li>In the worst case, all processes in system roll back to same virtual time as original one did and then proceed forward again.(最糟糕的情况也不过是重新来过，不会崩)</li></ul></li></ul></li><li>Rollback Mechanism(回滚机制)<ul><li>Search the ”State queue” for the last saved state with timestamp that is less than the timestamp of the message received and restore it.(查找状态队列中比出错时间戳更早的最后保存状态)</li><li>Make the timestamp of the received message as the value of the local virtual clock and discard from the state queue all states saved after this time. Then the resume execution forward from this point.(将虚拟接收时间作为当前的本地虚拟时间，丢掉状态队列中在此时间之后执行的所有结果，重新从这个时间点执行)</li><li>Now all the messages that are sent between the current state and earlier state must be “unsent”. This is taken care of by executing a simple rule: “To unsend a message, simply transmit its antimessage.”(所有在丢弃掉的事件中发送的消息都应该撤回，也就是发送一个该消息的反消息antimessage)</li><li>Depending on the timing, there are several possibilities at the receiver’s end:<ul><li>The original (positive) message has arrived but not yet been processed at the receiver. The negative message causes no rollback, however, it annihilates with the positive message leaving the receiver with no record of that message.(消息还没来得及被执行就收到了负责撤回的负消息，直接抵消，无事发生)</li><li>The original positive message has already been partially or completely processed by the receiver. The negative message causes the receiver to roll back to a virtual time when the positive message was received.(要被撤回的消息正在执行或已完成执行，则触发回滚，然后嵌套过程，有可能又触发其他回滚——级联回滚)</li><li>A negative message can also arrive at the destination before the positive one. In this case, it is enqueued and will be annihilated when positive message arrives. (撤回的消息比消息本身到的还快，这个和链路情况有关，是可能会发生的。那就把该消息放在输入队列里等待正消息的到来，然后相互抵消)<ul><li>If it is negative message’s turn to be executed at a processs’ input queqe, the receiver may take any action like a no-op. Any action taken will eventually be rolled back when the corresponding positive message arrives.(如果在正消息到来前就轮到执行负消息了，可以让进程执行空操作no-op，但时间戳还是会被更新，就会导致正消息到来后触发回滚，这种情况会浪费计算资源)</li><li>An optimization would be to skip the antimessage from the input queue and treat it as a no-op, and when the corresponding positive message arrives, it will annihilate the negative message, and inhibit any rollback.(可以采用跳过该消息的方式，就是空操作也不更新时间，直接往后处理，就不会需要回滚了)</li></ul></li></ul></li></ul></li></ul></li><li>global control mechanism: take care of global issues such as global progress, termination detection, I/O error handling, flow control(全局控制机制关注整个过程的问题，例如中断检测，I/O错误管理和流量控制)</li></ul><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><ul><li>In Lamport’s logical clock, an artificial clock is created one for each process with unique labels from a totally ordered set in a manner consistent with partial order(逻辑时钟是为每一个进程给一个时钟，全局看逻辑时间是偏序的)<ul><li>all clocks are conservatively maintained so that they never violate causality, a process advances its clock as soon as it learns of new causal dependency(保守地推进时间，天然保证了因果性，)</li></ul></li><li>In virtual time, the reverse of the above is done by assuming that every event is labeled with a clock value from a totally ordered virtual time scale satisfying Lamport’s clock conditions(虚拟时间是每个进程的每个事件单独给一个时钟值标注，这个标注与前面的标注是没有逻辑关系的，例如直接记录为事件发生的物理时钟，从全局看时钟就是全序的)<ul><li>clocks are optimistically advanced and corrective actions are taken whenever a violation is detected(虚拟时钟乐观推进，只在出问题的时候用机制纠正问题)</li></ul></li></ul><h3 id="Physical-Clock-Synchronization-NTP"><a href="#Physical-Clock-Synchronization-NTP" class="headerlink" title="Physical Clock Synchronization: NTP"></a>Physical Clock Synchronization: NTP</h3><ul><li>当任务要求使用高精度时间分辨率时，不能使用逻辑时钟，而要用到物理时钟</li><li>Clocks that must not only be synchronized with each other but also have to adhere to physical time are termed physical clocks(互相同步且需要遵守物理时间)</li></ul><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><ul><li>In centralized systems, there is only single clock. A process gets the time by simply issuing a system call to the kernel(一个系统一个时钟，向内核发起系统调用就能获得时间)</li><li>In distributed systems, there is no global clock or common memory. Each processor has its own internal clock and its own notion of time(分布式系统没有全局时钟或共享的内存，每个进程都有自己的时钟和自己对时钟的度量)</li><li>These clocks can easily drift seconds per day, accumulating significant errors over time(每天都会偏移几秒钟，随时间会累计成非常大的错误)</li><li>Also, because different clocks tick at different rates, they may not remain always synchronized although they might be synchronized when they start. (不同进程的时钟会有不同的速率，所以难以同步)<ul><li>This clearly poses serious problems to applications that depend on a synchronized notion of time</li></ul></li><li>For most applications and algorithms that run in a distributed system, we need to know time in one or more of the following contexts:<ul><li>The time of the day at which an event happened on a specific machine in the network.(事件发生的时间)</li><li>The time interval between two events that happened on different machines in the network.(在不同机器上发生的两个事件的时间间隔)</li><li>The relative ordering of events that happened on different machines in the network.(不同机器上发生的事件的相对顺序)</li></ul></li><li>Unless the clocks in each machine have a common notion of time, time-based queries cannot be answered(除非机器拥有相同的时间度量，否则基于时间的请求不会被响应)</li><li>Clock synchronization has a significant effect on many problems like secure systems, fault diagnosis and recovery, scheduled operations, database systems, and real-world clock values(时间同步很重要)</li></ul><h4 id="Clock-synchronization"><a href="#Clock-synchronization" class="headerlink" title="Clock synchronization"></a>Clock synchronization</h4><ul><li>Clock synchronization is the process of ensuring that physically distributed processors have a common notion of time(时间同步是让物理上的分布式进程拥有相同时间度量的过程)</li><li>Due to different clocks rate, the clocks at various sites may diverge with time and periodically a clock synchronization must be performed to correct this clock skew in distributed systems</li><li>Clocks are synchronized to an accurate real-time standard like UTC(Universal Coordinated Time)</li></ul><h4 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h4><ul><li>Let $C_a$ and $C_b$ be any two clocks<ul><li>Time: The time of a clock in a machine $p$ is given by the function $C_p(t)$, where $C_p(t)=t$ for a perfect clcok(没有偏差的时钟)</li><li>Frequency: Frequency is the rate at which a clock progresses. The frequency at time $t$ of clock $C_a$ is $C’_a(t)$(频率是时钟的变化率，是时间函数的导数)</li><li>Offset: Clock offset is the difference between the time reported by a clock and the real time. The offset of the clock $C_a$ is given by $C_a(t)-t$. The offset of clock $C_a$ relative to $C_b$ at time $t\geq0$ is given by $C_a(t)-C_b(t)$(时钟偏移是时钟之间的差值)</li><li>Skew(倾斜): The skew of a clock is the difference in the frequencies of the clock and the perfect clock. The skew of a clock $C_a$ relative to clock $C_b$ at time t is $(C’_a(t)-C’_b(t))$. If the skew is bounded by $\rho$, then as per Equation(1), clock values are allowed to diverge at a arte in the range of $1-\rho$ to $1+\rho$(时钟倾斜是时钟频率的差值，通常要限定在一个范围$\rho$内)<ul><li>a timer(clock) is said to be working within its specification if(where constant $\rho$ is the maximum skew rate specified by the manufacturer(制造商))</li><li>$1-\rho \leq \frac{dC}{dt}\leq 1+\rho$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/repect%20to%20UTC.png" alt="img"></li></ul></li><li>Drift(rate, 漂移): The drift of clock $C_a$ is the second derivative of the clock value with respect to time(漂移是对速度的变化率的度量，所以是时间的二阶导), namely, $C’’_a(t)$. The drift of clock $C_a$ relativ eto clock $C_a$ at time $t$ is $C’’_a(t)-C’’_b(t)$</li></ul></li></ul><h4 id="The-Network-Time-Protocol-NTP"><a href="#The-Network-Time-Protocol-NTP" class="headerlink" title="The Network Time Protocol(NTP)"></a>The Network Time Protocol(NTP)</h4><ul><li>is widely used for clock synchronization on the Internet uses the The Offset Delay Estimation method(使用偏移延迟的估算方法，进行时钟同步)</li><li>The design of NTP involves a hierarchical tree of time servers(分层树结构)<ul><li>The primary server at the root synchronizes with the UTC(根节点是拥有UTC时间的服务器)</li><li>The next level contains secondary servers, which act as a backup to the primary server(下一层是上层根节点主服务器的备份)</li><li>At the lowest level is the synchronization subnet which has the clients(最底层是拥有客户端的同步子网)</li></ul></li><li>The Offset Delay Estimation method<ul><li>a source node cannnot accurately estimate the local time on the target node due to varying message or network delays between the nodes(节点之间有网络和消息延迟，无法准确目标节点的本地时间)</li><li>This protocol employs a common pratice of performing several trials and chooses the trial with the minimum delay(协议选定最小延迟作为系统之间的时延，因为更贴近真实的消息传播时延)</li></ul></li><li>Assume clocks A and B are stable and running at the same speed. Let $T_1, T_2, T_3, T_4$ be the values of the four most recent timestamps as shown:<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/offset%20and%20delay%20estimation.png" alt="img"><ul><li>Each NTP message includes the latest three timestamps $T_1, T_2$ and $T_3$, while $T_4$ is determined upon arrival</li><li>Let $a = T_1-T_3$ and $b=T_2-T_4$</li><li>If the delay difference from A to B and from B to A, called differential delay, is small, the clock offset $\theta$ and roundtrip delay $\delta$ of B relative to A at Time $T_4$ are approximately given by the following<ul><li>$\theta = \frac{(a+b)}{2}=\frac{T_1-T_3+T_2-T_4}{2}$(由下面两式联立得到)<ul><li>$\theta = T_1-(T_3 + \frac{\delta}{2})$</li><li>$\theta = T_4-(T_2 + \frac{\delta}{2})$</li></ul></li><li>$\delta = a-b=T_1-T_3-(T_2-T_4)=(T_4-T_3)-(T_2-T_1)$(消息在A待的时间减去在B待的时间就是在信道中传输的时间)</li></ul></li></ul></li><li>Thus both peers A and B can independently calculate delay and offset using a single bidirectional messages stream<ul><li>A pair of servers in symmetric mode exchange pairs of timing messages<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Timing%20diagram.png" alt="img"></li><li>A store of data is then built up about the relationship between the two servers(pairs of offset and delay). Specifically, assume that each peer maintains pairs $(O_i, D_i)$, where <ul><li>$O_i$: measure of offset ($\theta$)</li><li>$D_i$: transmission delay of two messages ($\delta$)</li></ul></li><li>The offset corresponding to the minimum delay is chosen. Specifically, the delay and offset are calculated as follows.(用最小延迟作为偏移，不以最小延迟为偏移可能导致偏移偏大、偏小或无影响，取决于两个时钟的快慢关系) </li><li>Assume that message $m$ takes time $t$ to transfer and $m’$ takes $t’$ to transfer</li><li>The offset between A’s clock and B’s clock is $O$. If A’s local clock time is $A(t)$ and B’s local clock time is $B(t)$, <ul><li>$A(t)=B(t)+O$</li><li>$T_{i-2}=T_{i-3}+t+O$</li><li>$T_i=T_{i-1}-O+t’$</li></ul></li><li>Assuming $t=t’$, the offset $O_i$ can be estimated as:<ul><li>$O_i=(T_{i-2}-T_{i-3}+T_{i-1}-T{i})/2$</li></ul></li><li>The round-trip delay is estimated as:<ul><li>$D_i=(T_i-T_{i-3})-(T_{i-1}-T_{i-2})$</li></ul></li><li>The eight most recent pairs of $(O_i, D_i)$ are retained</li><li>The value of $O_i$ that corresponds to minimum $D_i$ is chosen to estimate $O$</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;逻辑时间&quot;&gt;&lt;a href=&quot;#逻辑时间&quot; class=&quot;headerlink&quot; title=&quot;逻辑时间&quot;&gt;&lt;/a&gt;逻辑时间&lt;/h1&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>分布式计算模型</title>
    <link href="http://zjn-astonishe.github.io/2024/09/28/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-09-28-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/"/>
    <id>http://zjn-astonishe.github.io/2024/09/28/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2024-09-28-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/</id>
    <published>2024-09-28T03:41:34.000Z</published>
    <updated>2024-11-09T04:33:57.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式计算模型"><a href="#分布式计算模型" class="headerlink" title="分布式计算模型"></a>分布式计算模型</h1><h2 id="典型分布式系统-Ray"><a href="#典型分布式系统-Ray" class="headerlink" title="典型分布式系统(Ray)"></a>典型分布式系统(Ray)</h2><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Ray.png" alt="img"></p><h2 id="Distributed-Program"><a href="#Distributed-Program" class="headerlink" title="Distributed Program"></a>Distributed Program</h2><ul><li>a distributed program is composed of a set of asynchronous processes $p_1, p_2, … , p_i, … , p_n$(异步进程的集合)</li><li>The processes do not share a global memory and communicate solely by passing messages(消息传递，没有共享内存)</li><li>The processes do not share a global clock that is instantaneously accessible to these processes(不共享可以即时访问的全局时钟)</li><li>Process execution and message transfer are asynchronous(进程运行和消息收发是异步发)</li><li>Without loss of generality(为了不失去一般性), we assume that each process is running on a different processor(假设每个进程运行在不同的处理器上)</li><li>Let $C_{ij}$  denote the channel(信道) from process $p_i$ to process $p_j$ and let $m_{ij}$ denote a message(消息) sent by $p_i$ to $p_j$</li><li>The message transmission delay is definite and unpredictable(传输延迟确定的但不可预测，只要传了，延迟就是确定的，但不知道什么时候传，所以是不可预测？)</li></ul><h2 id="A-Model-of-Distributed-Executions-分布式执行的模型"><a href="#A-Model-of-Distributed-Executions-分布式执行的模型" class="headerlink" title="A Model of Distributed Executions(分布式执行的模型)"></a>A Model of Distributed Executions(分布式执行的模型)</h2><ul><li>The execution of a process consists of a sequential execution of its actions(进程由一系列顺序执行的动作组成)</li><li>The actions are atomic(动作是原子化的) and the actions of a process are modeled as three types of events, namely, <ul><li>internal events, 内部事件</li><li>message send events, 消息发送事件</li><li>message receive events, 消息接收事件</li></ul></li><li>Let $e_i^x$ denote the $x^{th}$ event at process $p_i$</li><li>For a message $m$, let $send(m)$ and $rec(m)$ denote its send and receive events, respectively</li><li>The occurrence of events changes the states of respective processes and channels(每个事件的发生会改变对应进程和信道的状态)<ul><li>An internal event changes the state of the process at which it occurs(内部事件只影响进程)</li><li>A send event changes the state of the process that sends the message and the state of the channel on which the message is sent(发送事件影响发送消息的进程和发送消息的信道)</li><li>A receive event changes the state of the process that receives the message and the state of the channel on which the message is received(接收事件影响接收消息的进程和接收消息的信道)</li></ul></li><li>The events at a process are linearly ordered by their order of occurence(一个进程的事件根据发生的顺序能线性排序)</li><li>The execution of process $p_i$ produces a sequence of events $e_i^1, e_i^2, … , e_i^x, e_i^{x+1}, …$ and is denoted by $H_i$ where<ul><li>$ H_i = (h_i, \rightarrow_i)$</li><li>$h_i$ is the set of events produced by $p_i$</li><li>binary relation $\rightarrow_i$ defines a linear order on these events, expresses causal dependencies among the events of $p_i$(右箭头表示的是具有因果关系)</li></ul></li><li>The send and the receive events signify the flow of information between processes and establish causal dependency from the sender process to the receiver process(进程之间的发送和接收事件的关系，从发送方到接收方建立因果依赖关系)</li><li>A relation $\rightarrow_{msg}$ that captures the causal dependency due to message exchange, is defined as follows. For every message $m$ that is exchanged between two processes, we have<ul><li>$send(m)\rightarrow_{msg}\quad rec(m)$</li><li>defines causal dependencies between the pairs of corresponding send and receiver events</li></ul></li><li>A space-time diagram Example:<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/The%20space-time%20diagram%20of%20a%20distributed%20execution.png" alt="img"><ul><li>The evolution of a distributed execution is depicted by a space-time diagram(时空图).</li><li>A horizontal line represents the progress of the process(水平线表示进程的进度), a dot indicates an event(点表示事件), a slant arrow indicates a message transfer(斜箭头表示消息传递)<ul><li>Since we assume that an event execution is atomic(hence, indivisible and instantaneous(不可再分以及瞬时)), it is justified to denote it as a dot on a process line</li></ul></li><li>In the Figure, <ul><li>for process $p_1$, the second event is a message send event, </li><li>the third event is an internal event, </li><li>and the fourth event is a message receive event</li></ul></li></ul></li></ul><h3 id="Causal-Precedence-Relation-因果过程关系"><a href="#Causal-Precedence-Relation-因果过程关系" class="headerlink" title="Causal Precedence Relation(因果过程关系)"></a>Causal Precedence Relation(因果过程关系)</h3><ul><li>The execution of a distributed application results in a set of distributed events produced by the processes(分布式应用的结果由进程产生的分布式事件得到的)</li><li>Let $H=\bigcup_i h_i$ denote the set of events executed in a distributed computation(分布式计算中的事件集合)</li><li>Define a binary relation $\rightarrow$ on the set $H$ as follows that express causal dependencies between events in the distributed execution <script type="math/tex; mode=display">\forall e_i^x, \forall e_j^y \in H, e_i^x\rightarrow e_j^y \Leftrightarrow   \begin{cases}    e_i^x\rightarrow_i e_j^y :i.e., (i=j)\bigwedge(x<y)\\    or\\    e_i^x\rightarrow_{msg}e_j^y\\    or\\    \exists e_k^z\in H : e_i^x\rightarrow e_k^z \bigwedge e_k^z \rightarrow e_j^y  \end{cases}</script></li><li>The causal precedure relation induces an irreflexive partial order(不可弯曲的偏序) on the events of a distributed computation that is denoted as $H=(H,\rightarrow)$</li><li>Note that the relation $\rightarrow$ is nothing but Lamport’s “happens before” relation(这个关系就是在…之前发生)</li><li>For any two event $e_i$ and $e_j$, if $e_i\rightarrow e_j$, then event $e_j$ is directly or transitively(直接或传递性地) dependent on event $e_i$(Graphically, it means that there exists a path consisting of message arrows and process-line segments(along increasing time) in the space-time diagram that starts at $e_i$ and ends at $e_j$)<ul><li>For example, in Figure above, $e_1^1\rightarrow e_3^3$ and $e_3^3\rightarrow e_2^6$</li></ul></li><li>The relation $\rightarrow$ denotes flow of information in a distributed computation and $e_i\rightarrow e_j$ dictates that all the information available at $e_i$ is potentially accessible at $e_j$<ul><li>For example, in Figure above, event $e_2^6$ has the knowledge of all other events shown in the figure</li></ul></li><li>For any two events $e_i$ and $e_j$, $e_i \nrightarrow e_j$ denotes the fact that event $e_j$ does not directly or transitively dependent on event $e_i$. That is, event $e_i$ does not causally affect event $e_j$</li><li>In this case, event $e_j$ is not aware of the execution of $e_i$ or any event executed after $e_i$ on the same process</li><li>For example, in Figure above, $e_1^3 \nrightarrow e_3^3$ and $e_2^4\nrightarrow e_3^1$</li><li>For any two events $e_i$ and $e_j$, $e_i\nrightarrow e_j\nRightarrow e_j\nrightarrow e_i$(因为可能$e_j\rightarrow e_i$)</li><li>For any two events $e_i$ and $e_j$, $e_i\rightarrow e_j\Rightarrow e_j\nrightarrow e_i$</li></ul><h3 id="Concurrent-events-并发事件"><a href="#Concurrent-events-并发事件" class="headerlink" title="Concurrent events(并发事件)"></a>Concurrent events(并发事件)</h3><ul><li>For any two events $e_i$ and $e_j$, if $e_i\nrightarrow e_j$ and $e_j\nrightarrow e_i$, then events $e_i$ and $e_j$ are said to be concurrent(denoted as $e_i||e_j$，互相没有因果关系)</li><li>In the execution of Figure above, $e_1^3||e_3^3$ and $e_2^4||e_3^1$</li><li>The relation $||$ is not transitive(并发不是传递性的); that is, $(e_i||e_j)\bigwedge(e_j||e_k)\nRightarrow e_i || e_k$<ul><li>For example, in Figure above, $e_3^3||e_2^4$ and $e_2^4 || e_1^5$, however, $e_3^3 \nmid\nmid e_1^5$</li></ul></li><li>For any two events $e_i$ and $e_j$ in a distributed execution, 要不就有因果关系，要不就是并发<ul><li>$e_i\rightarrow e_j$</li><li>$e_j\rightarrow e_i$</li><li>$e_i||e_j$</li></ul></li></ul><h3 id="Logical-Concurrency-and-Physical-Concurrency-逻辑并发和物理并发"><a href="#Logical-Concurrency-and-Physical-Concurrency-逻辑并发和物理并发" class="headerlink" title="Logical Concurrency and Physical Concurrency(逻辑并发和物理并发)"></a>Logical Concurrency and Physical Concurrency(逻辑并发和物理并发)</h3><ul><li>In a distribued computation, two events are logically concurrent if and only if they do not casually affect each other(互相之间没有因果就是逻辑并发的)</li><li>Physical concurrency, on the other hand, has a connotation that the events occur at the same instant in physical time(在同一物理时间的瞬间发生)</li><li>Two or more events may be logically concurrent even though they do not occur at the same instant in physical time</li><li>However, if processor speed and message delays would have been different, the execution of these events could have very well coincided in physical time</li><li>Whether a set of logically concurrent events coincide in the physical time or not, does not change the outcome of the computation(逻辑并发的结果不受物理时间的影响)</li><li>Therefore, even though a set of logically concurrent events may not have occured at the same instant in physical time, we can assume that these events occured at the same instant in physical time</li></ul><h2 id="Models-of-Communication-Networks"><a href="#Models-of-Communication-Networks" class="headerlink" title="Models of Communication Networks"></a>Models of Communication Networks</h2><ul><li>There are several models of the service provided by communication networks, namely, </li></ul><h3 id="FIFO"><a href="#FIFO" class="headerlink" title="FIFO,"></a>FIFO,</h3><ul><li>each channel acts as a first-in first-out message queue(先进先出队列) and thus, message ordering is preserved by a channel(消息顺序由信道维护)</li></ul><h3 id="Non-FIFO"><a href="#Non-FIFO" class="headerlink" title="Non-FIFO,"></a>Non-FIFO,</h3><ul><li>a channel acts like a set in which the sender process adds messages and the receiver process removes messages from it in a random order(随机序列)</li></ul><h3 id="causal-ordering-因果序"><a href="#causal-ordering-因果序" class="headerlink" title="causal ordering(因果序)"></a>causal ordering(因果序)</h3><ul><li>is based on Lamport’s “happens before” relation</li><li>A system that supports the causal ordering model satisfies the following property:<ul><li>CO: For any two messages $m_{ij}$ and $m_{kj}$, if $send(m_{ij})\rightarrow send(m_{kj})$, then $rec(m_{ij})\rightarrow rec(m_{kj})$</li><li>This property ensures that causally related messages destined to the same destination are delivered in an order that is consistent with their causality relation(因果相关的信息与其因果关系一致地顺序传递)</li></ul></li><li>Causally ordered delivery of messages implies FIFO message delivery.(Note that $CO \subset FIFO \subset Non-FIFO$，CO是特殊的FIFO，FIFO又是特殊的Non-FIFO)</li><li>Causal ordering model considerably simplifies the design of distributed algorithms because it provides a built-in synchronization(内置同步)</li></ul><h2 id="Global-State-of-a-Distributed-System"><a href="#Global-State-of-a-Distributed-System" class="headerlink" title="Global State of a Distributed System"></a>Global State of a Distributed System</h2><ul><li><p>“A collection of the local states of its components, namely, the processes and the communication channels”(全局状态包括进程状态和信道状态)</p><ul><li>The state of a process is defined by the contents of processor registers, stacks, local memory, etc. and depends on the local context of the distributed application</li><li>The state of channel is given by the set of messages in transit in the channel</li></ul></li><li><p>The occurrence of events changes the states of respective processes and channels(状态改变由事件的发生引起)</p><ul><li>An internal event changes the state of the process at which it occurs(内部事件只影响对应进程)</li><li>A send event changes the state of the process that sends the message and the state of the channel on which the message is sent(发送事件除了影响进程还影响信道)</li><li>A receive event changes the state of the process that or receives the message and the state of the channel on which the message is received(接收事件除了影响进程还影响信道)</li></ul></li></ul><h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><h4 id="Local-State"><a href="#Local-State" class="headerlink" title="Local State"></a>Local State</h4><ul><li>$LS_i^x$ denotes the state of process $p_i$ after the occurrence of event $e_i^x$ and before the event $e_i^{x+1}$(local state)<ul><li>$LS_i^0$ denotes the initial state of process $p_i$</li><li>$LS_i^x$ is a result of the execution of all the events executed by process $p_i$ till $e_i^x$(之前的所有事件)</li></ul></li><li>Let $send(m) \leq LS_i^x$ denote the fact that $\exists y:1\leq y\leq x :: e_i^y=send(m)$(发送事件早于某个LS状态，说明该状态以前的事件有该发送事件)</li><li>Let $rec(m) \nleq LS_i^x$ denote the fact that $\forall y:1\leq y\leq x :: e_i^y\neq rec(m)$(接收时间不早于某个LS状态，说明该状态以前的事件中不包括该接收事件)</li></ul><h4 id="A-Channel-State-信道状态"><a href="#A-Channel-State-信道状态" class="headerlink" title="A Channel State(信道状态)"></a>A Channel State(信道状态)</h4><ul><li>The state of a channel depends upon the states of the process it connects(取决于信道所连接的进程的状态)</li><li>Let $SC_{ij}^{x, y}$ denote the state of a channel $C_{ij}$</li><li>The state of a channel is defined as follows:<ul><li>$SC_{ij}^{x, y}$ denotes all messages that $p_i$ sent upto event $e_i^x$ and which process $p_j$ had not received until event $e_j^y$(进程i在事件x发送消息，并且进程j在事件y接收到消息)</li></ul></li></ul><h4 id="Global-State-全局状态，不太可能，只存在与理论"><a href="#Global-State-全局状态，不太可能，只存在与理论" class="headerlink" title="Global State(全局状态，不太可能，只存在与理论)"></a>Global State(全局状态，不太可能，只存在与理论)</h4><ul><li>The global state of a distributed system is a collection of the local states of the processes and the channels(进程本地状态和信道状态的集合)</li><li>Notationally, global state $GS$ is defined as,<ul><li>$GS=\{\bigcup_iLS_i^{x_i}, \bigcup_{j, k}SC_{jk}^{y_j, z_k}\}$</li></ul></li><li>For a global state to be meaningful, the states of all the components of the distributed system must be recorded at the same instant(分布式系统中的所有组件的状态需要在相同的时刻记录)</li><li>This will be possible if the local clocks at processes were perferctly synchronized or if there were a global system clock that can be instantaneously read by the process(However, both are impossible)(如果要实现的话，要求时钟完美同步或者存在一个全局系统的时钟可以被进程即时读取)</li></ul><h4 id="A-Consistent-Global-State-一致的全局状态"><a href="#A-Consistent-Global-State-一致的全局状态" class="headerlink" title="A Consistent Global State(一致的全局状态)"></a>A Consistent Global State(一致的全局状态)</h4><ul><li>Even if the state of all the components is not recorded at the same instant, such a state will be meaningful provided every message that is recorded as received is also recorded as sent(要求每个接收事件都有对应的发送事件)</li><li>Basic idea is that a state should not violate causality(不能违反因果律)<ul><li>an effect should not be present without its cause. A message cannot be received if it was not sent.(没有发送不可能有接收)</li></ul></li><li>Such states are called consistent global states and are meaningful global states.</li><li>Inconsistent global states are not meaningful in the sense that a distributed system can never be in an inconsistent state(分布式系统永远不可能处于不一致状态).</li><li>A global state $GS=\{\bigcup_iLS_i^{x_i}, \bigcup_{j, k}SC_{jk}^{y_j, z_k}\}$ is a consistent global state iff(if and only if):<ul><li>$\forall m_{ij}:send(m_{ij})\nleq LS_i^{x_i} \Leftrightarrow m_{ij} \notin SC_{ij}^{x_i, y_j}\bigwedge rec(m_{ij})\nleq LS_j^{y_j}$(发送的消息要不还在信道中，要不就已经被接收，不能出现有接收但没发送)</li><li>channel state $SC_{ij}^{y_i, z_k}$ and process state $LS_j^{z_k}$ must not include any message that process $p_i$ sent after executing event $e_i^{x_i}$</li></ul></li><li>Example<ul><li>A global state $GS_1=\{LS_1^1, LS_2^3, LS_3^3, LS_4^2\}$ is inconsistent, because the state of $p_2$ has recorded the receipt of message $m_{12}$, however, the state of $p_1$ has not recorded its send</li><li>A global state $GS_2$ consisting of local states $\{LS_1^2, LS_2^4, LS_3^4, LS_4^2\}$ is consistent, all the channels are empty except $C_{21}$ that contains message $m_{21}$<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/The%20space-time%20diagram%20of%20a%20distributed%20execution2.png" alt="img"></li></ul></li></ul><h2 id="Cuts-of-a-Distributed"><a href="#Cuts-of-a-Distributed" class="headerlink" title="Cuts of a Distributed"></a>Cuts of a Distributed</h2><ul><li>In the space-time diagram of a distributed computation, a cut is a zigzag line(锯齿线) joining one arbitrary point on each process line(连接每个进程线上任意一点)</li><li>A cut slices the space-time diagram, and thus the set of events in the distributed computation, into a <code>PAST</code> and a <code>FUTURE</code>.(cut将时空图分为过去和将来)<ul><li>The <code>PAST</code> contains all the events to the left of the cut </li><li>The <code>FUTURE</code> contains all the events to the right of the cut.</li><li>For a cut <code>C</code>, let <code>PAST(C)</code> and <code>FUTURE(C)</code> denote the set of events in the <code>PAST</code> and <code>FUTURE</code> of <code>C</code>, respectively.</li></ul></li><li>Every cut corresponds to a global state and every global state can be graphically represented as a cut in the computation’s space-time diagram.(每个cut对应一个全局状态，每个全局状态都可以在计算的时空图中以图形的方式表示cut)<ul><li>Cuts in a space-time diagram provide a powerful graphical aid in representing and reasoning about global states of a computation.</li><li>In a consistent cut, every message received in the PAST of the cut was sent in the PAST of that cut. (In Figure 2.3, cut C2 is a consistent cut. 过去接收到的消息只能是过去发送的)</li><li>All messages that cross the cut from the PAST to the FUTURE are in transit in the corresponding consistent global state.(未来收到的消息可能是过去发送的，也可能是未来发送的)</li><li>A cut is inconsistent if a message crosses the cut from the FUTURE to the PAST. (In Figure 2.3, cut C1 is an inconsistent cut.)(但是未来发送的消息不可能在过去被接收)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Illustration%20of%20cuts%20in%20a%20distributed%20exection.png" alt="img"></li></ul></li></ul><h2 id="Past-and-Future-Cones-of-an-Event"><a href="#Past-and-Future-Cones-of-an-Event" class="headerlink" title="Past and Future Cones of an Event"></a>Past and Future Cones of an Event</h2><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/Illustration%20of%20past%20and%20future%20cones.png" alt="img"></p><h3 id="Past-Cone-of-an-Event-事件因果关系的因"><a href="#Past-Cone-of-an-Event-事件因果关系的因" class="headerlink" title="Past Cone of an Event(事件因果关系的因)"></a>Past Cone of an Event(事件因果关系的因)</h3><ul><li>An event $e_j$ could have been affected only by all events $e_i$ such that $e_i\rightarrow e_j$</li><li>In this situation, all the information available at $e_i$ could be made accessible at $e_j$</li><li>All such events $e_i$ belong to the past of $e_j$<ul><li>Let $Past(e_j)$ denote all events in the past of $e_j$ in a computation $(H, \rightarrow)$. Then, <ul><li>$Past(e_j) = \{e_i\mid \forall e_i\in H, e_i\rightarrow e_j\}$</li><li>$Past(e_j)$ represents all events on the past light cone that affect $e_j$  </li></ul></li></ul></li><li>Let $Past_i(e_j)$ be the set of all those events of $Past(e_j)$ that are on process $p_i$<ul><li>$Past_i(e_j)$ is a totally ordered set(顺序集), ordered by the relation $\rightarrow_i$, whose maximal element is denoted by $max(Past_i(e_j))$<ul><li>$max(Past_i(e_j))$ is the <strong>latest event</strong> at process $p_i$ that affected event $e_j$</li></ul></li><li>Let $Max_Past(e_j) = \bigcup_{(\forall i)} \{max(Past_i(e_j))\}$(所有进程中的latest event)<ul><li>$Max_Past(e_j)$ consists of the latest event at every process that affected event $e_j$ and is referred to as the surface of the past cone of $e_j$(past cone的latest平面)</li></ul></li></ul></li></ul><h3 id="Future-Cone-of-an-Event-事件因果关系中的果"><a href="#Future-Cone-of-an-Event-事件因果关系中的果" class="headerlink" title="Future Cone of an Event(事件因果关系中的果)"></a>Future Cone of an Event(事件因果关系中的果)</h3><ul><li>The future of an event $e_j$, denoted by $Future(e_j)$, contains all events $e_i$ that are causally affected by $e_j$</li><li>In a computation $(H, \rightarrow)$, $Future(e_j)$ is defined as:<ul><li>$Future(e_j) = \{e_i\mid \forall e_i \in H, e_j\rightarrow e_i\}$</li></ul></li><li>Define $Future_i(e_j)$ as the set of those events of $Future(e_j)$ that are on process $p_i$<ul><li>define $min(Future_i(e_j))$ as the <strong>first event</strong> on process $p_i$ that is affected by $e_j$</li><li>Define $Min_Future(e_j)$ as $\bigcup_{(\forall i)}\{min(Future_i(e_j))\}$, which consists of the first event at every process that is causally affected by event $e_j$(所有进程中的first event)</li><li>$Min_Future(e_j)$ is referred to as the surface of the future cone of $e_j$(事件j的future cone平面)</li><li>All events at a process $p_i$ that occurred after $max(Past_i(e_j))$ but before $min(Future_i(e_j))$ are concurrent with $e_j$(与事件j无因果关系)</li><li>Therefore, all and only those events of computation $H$ that belong to the set “$H - Past(e_j) - Future(e_j)$” are concurrent with event $e_j$(所有事件去掉有因果关系的得到的便是并发的事件)</li></ul></li></ul><h2 id="Model-of-Process-Communications"><a href="#Model-of-Process-Communications" class="headerlink" title="Model of Process Communications"></a>Model of Process Communications</h2><ul><li>There are two basic models of process communications(进程通信)<ul><li>synchronous communication model(同步通信模型，阻塞的)<ul><li>a blocking type where on a message send, the sender process blocks until the message has been received by the receiver process</li><li>The sender process resumes execution only after it learns that the receiver process has accepted the message</li><li>Thus, the sender and the receiver processes must synchronize to exchange a message</li></ul></li><li>asynchronous communication models of process communicatiion(异步通信模型，非阻塞的)<ul><li>a non-blocking type where the sender and the receiver do not synchronize to exchange a message</li><li>After having sent a message, the sender process does not wait for the message to be delivered to the receiver process</li><li>The message is bufferred by the system(消息需要操作系统进行缓存) and is delivered to the receiver process when it is ready to accept the message</li></ul></li></ul></li><li>Neither of the communication models is superior to the other.(两者各有优劣)<ul><li>Asynchronous communication provides higher parallelism because the sender process can execute while the message is in transit to the receiver(异步提供更高的并行性)<ul><li>However, a buffer overflow may occur if a process sends a large number of message in a burst to another process(可能会缓存溢出)</li><li>Thus, an implementation of asynchronous communication requires more complex buffer management(异步系统要求更复杂的缓存管理)</li><li>In addition, due to higher degree of parallelism and non-determinism(更高的并行性和更高的非确定性), it is much more difficult to design, verify, and implement distributed algorithms for asynchronous communications(难以为异步通信设计、验证、实现分布式算法)</li></ul></li><li>Synchronous communication is simpler to handle and implement(同步通信更简单管理和实现)<ul><li>However, due to frequent blocking, it is likely to have poor performance and is likely to be more prone to deadlocks(频繁地阻塞，性能更差，更可能出现死锁)</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;分布式计算模型&quot;&gt;&lt;a href=&quot;#分布式计算模型&quot; class=&quot;headerlink&quot; title=&quot;分布式计算模型&quot;&gt;&lt;/a&gt;分布式计算模型&lt;/h1&gt;&lt;h2 id=&quot;典型分布式系统-Ray&quot;&gt;&lt;a href=&quot;#典型分布式系统-Ray&quot; class=&quot;he</summary>
      
    
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/categories/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="高级分布式系统" scheme="http://zjn-astonishe.github.io/tags/%E9%AB%98%E7%BA%A7%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
