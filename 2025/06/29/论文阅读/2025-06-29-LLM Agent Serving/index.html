<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>LLM Agent Serving | ZJN_BLOG</title><meta name="keywords" content="论文阅读"><meta name="author" content="ZJN"><meta name="copyright" content="ZJN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLM Agent ServingThroughput-Optimal Scheduling Algorithms for LLM Inference and AI AgentsMotivation由于 LLM 引擎以自回归方式生成 token，因此处理单个请求需要多次运行模型，每次迭代生成一个输出 token。为了优化 GPU 利用率，重要的是在解码阶段批量处理多个请求。本文研究了各种调度算法在">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM Agent Serving">
<meta property="og:url" content="http://zjn-astonishe.github.io/2025/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-29-LLM%20Agent%20Serving/index.html">
<meta property="og:site_name" content="ZJN_BLOG">
<meta property="og:description" content="LLM Agent ServingThroughput-Optimal Scheduling Algorithms for LLM Inference and AI AgentsMotivation由于 LLM 引擎以自回归方式生成 token，因此处理单个请求需要多次运行模型，每次迭代生成一个输出 token。为了优化 GPU 利用率，重要的是在解码阶段批量处理多个请求。本文研究了各种调度算法在">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png">
<meta property="article:published_time" content="2025-06-29T03:22:18.000Z">
<meta property="article:modified_time" content="2025-07-03T05:17:08.125Z">
<meta property="article:author" content="ZJN">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://zjn-astonishe.github.io/2025/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-29-LLM%20Agent%20Serving/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":800},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: ZJN","link":"链接: ","source":"来源: ZJN_BLOG","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#000000","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM Agent Serving',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-03 13:17:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3207144_mqiyof22xva.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ZJN_BLOG" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ZJN_BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM Agent Serving</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-29T03:22:18.000Z" title="发表于 2025-06-29 11:22:18">2025-06-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-03T05:17:08.125Z" title="更新于 2025-07-03 13:17:08">2025-07-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>37分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LLM-Agent-Serving"><a href="#LLM-Agent-Serving" class="headerlink" title="LLM Agent Serving"></a>LLM Agent Serving</h1><h2 id="Throughput-Optimal-Scheduling-Algorithms-for-LLM-Inference-and-AI-Agents"><a href="#Throughput-Optimal-Scheduling-Algorithms-for-LLM-Inference-and-AI-Agents" class="headerlink" title="Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents"></a>Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于 LLM 引擎以自回归方式生成 token，因此处理单个请求需要多次运行模型，每次迭代生成一个输出 token。为了优化 GPU 利用率，重要的是在解码阶段批量处理多个请求。本文研究了各种调度算法在形成批次时的性能保证。<br>通过排队理论，分析“对于一个LLM推理系统，其最大吞吐率的根本限制是什么？哪些类型的调度算法能够达到这个限制？”</p>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>FasterTransformer在请求级别执行批处理，从而提高解码阶段的吞吐量。优先考虑解码阶段。当解码阶段没有足够的请求进行批处理时，这些迭代的吞吐量就会受到影响。</p>
<p>Orca和vLLM在 token 级别实现批处理，并优先处理新到达请求的预填充阶段，从而使这些请求能够更快地进入解码阶段。与 FasterTransformer 相比，Orca 和 vLLM 是为了减少延迟，尽管它们在是否支持混合批处理（即将 prefill 和 decode 两个阶段合并在一起进行批量处理）方面有所不同。</p>
<p>Sarathi-Serve引入了 chunk-prefill 来限制单个批次中的 token 数量和 预填充 token 的长度，从而防止具有较长预填充的请求阻塞解码。</p>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h4 id="LLM的生成式推理"><a href="#LLM的生成式推理" class="headerlink" title="LLM的生成式推理"></a>LLM的生成式推理</h4><p>在处理输入请求时，LLM 推理包括两个阶段：预填充(prefill)和解码(decoding)。</p>
<ul>
<li>预填充阶段处理整个请求，以计算 KV 缓存（Key-Value cache，用于存储每一层的 attention keys 和 values 以避免冗余计算），并在单个步骤中生成初始响应 token。</li>
<li>解码阶段然后利用先前的上下文或 KV 缓存，一次生成一个后续 token。</li>
</ul>
<p>这些阶段具有不同的资源利用模式：预填充是计算密集型的（compute-bound），而解码是内存 I/O 密集型的（memory I/O-bound）。传统的 LLM 推理引擎将这两个阶段都放在同一个 GPU 组上以最大化资源利用率，尽管它们的计算特性不同。但这种方式会造成两个阶段互相干扰，且资源分配和并行策略紧密耦合。现在更提倡对两个阶段分离解耦放在不同的GPU上，详见(<a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/37129">LLM Serving有效吞吐量的最大化实现</a>)。</p>
<h4 id="Inference-and-serving-goal"><a href="#Inference-and-serving-goal" class="headerlink" title="Inference and serving goal"></a>Inference and serving goal</h4><p>有两个关键指标用于评估LLM服务的性能: 吞吐量和推理延迟。</p>
<ul>
<li>吞吐量(Throughput): 衡量在给定时间内生成的 token 数量。</li>
<li>推理延迟(Inference latency): 表示完成一个请求所需的时间。主要的延迟指标: <ul>
<li>TTFT(Time to first token): LLM输出第一个生成词元所需的时间，主要是预填充阶段。</li>
<li>TBT(time between token or TPOT, Time per output token): 衡量两个连续生成的词元之间的平均时延，主要是解码阶段。</li>
</ul>
</li>
<li>服务级别目标(Service level objective, SLO): 衡量延迟性能。</li>
</ul>
<p>具体衡量标准: </p>
<ul>
<li>对于在线服务，目标是在延迟 SLO 约束下优化吞吐量。</li>
<li>对于离线推理或批量推理，目标是优化批量吞吐量。</li>
</ul>
<h4 id="Inference-optimization"><a href="#Inference-optimization" class="headerlink" title="Inference optimization"></a>Inference optimization</h4><p>为了应对在满足性能要求的同时，最大限度地减少资源浪费的挑战，过去的研究利用 GPU 混合并行化策略，结合数据并行、张量并行和流水线并行来优化跨 GPU 的计算。</p>
<ul>
<li>张量模型并行(TP): 通过按行和按列划分 transformer 权重矩阵，将计算分布在 GPU 上，并通过两次 AllReduce 操作聚合层输出(会将所有参与计算的 GPU 上的数据进行某种操作(例如求和)，并将最终结果分发回所有 GPU。)。</li>
<li>流水线并行(PP): 将模型分割成多个阶段，分配给特定的 GPU 或 GPU 组，并在各阶段之间传递层间激活(本阶段的输出作为下一阶段的输入)。</li>
<li>先进的内存管理技术，如 paged attention，有助于缓解内存压力，提高资源利用率。</li>
<li>调度器优化，包括动态批处理和分块预填充处理，以及分离 prefill 和 decoding 阶段，确保高吞吐量，同时满足延迟约束，使这些系统能够有效地处理各种 LLM 推理工作负载。</li>
</ul>
<h4 id="Batching"><a href="#Batching" class="headerlink" title="Batching"></a>Batching</h4><p>计算中 prefill 阶段和 decoding 阶段的差异会导致应用批处理策略时产生不同的性能结果。 一种称为 continuous batching 的优化技术将新的请求 prefill 操作与正在进行的请求 decoding 操作合并在同一批处理中，以提高 GPU 利用率，从而在 token 级别做出批处理决策。 然而，这种方法会在 prefill 阶段和 decoding 阶段之间产生显着的干扰。 即使将单个 prefill 作业引入到 decoding token 的批处理中，也会大大降低这两个操作的性能，并且随着 prefill 输入长度的增加，性能损失会变得更加严重。</p>
<h3 id="A-stochastic-processing-model-for-LLM-inference"><a href="#A-stochastic-processing-model-for-LLM-inference" class="headerlink" title="A stochastic processing model for LLM inference"></a>A stochastic processing model for LLM inference</h3><p>根据经验发现，批处理的时延主要取决于token的总数，而不是批处理的具体内容，因此不再需要考虑批处理内容的复杂组合。且这种关系是分段线性的，这意味着在一定范围内，处理时间随令牌数量线性增长，但可能在某个阈值（如达到某个硬件限制或优化点）后，增长率会发生变化，形成多个线性段。</p>
<h4 id="Model-Setup"><a href="#Model-Setup" class="headerlink" title="Model_Setup"></a>Model_Setup</h4><p>以当前先进的LLM服务系统Sarathi-serve构建一个随机处理模型。</p>
<ul>
<li>建模请求到来(requests arrival)<ul>
<li>时间是离散的，辅以下标$n\in \mathbb{N}\equiv \{1, 2, …, \}$。</li>
<li>对于每个时隙$n$，允许在同一时隙到达多个请求，于是使用$a_n$表示在该时隙到达系统的请求的数量。从物理上讲，时隙可以对应于时钟频率单位或其他时间单位。</li>
<li>假设所有的请求都是有序的(也就是有标号的?)。对于$i=1, 2, …, $，假设请求$i$具有$v_p(i)\in \mathbb{N}$个有序的预填充tokens，$v_d(i)\in\mathbb{N}$个有序的解码tokens和特征$z(i)\in \mathcal{K}$(其中$\mathcal{K}$是一个有限集)。特征$z(i)$可以捕获请求的类型/内容。于是使用$(v_p(i), v_d(i), z(i))$描述系统，不过调度算法可能无法观察到它们(特别是$v_d(i)$)。</li>
<li>假设两个独立的iid序列:<script type="math/tex">\{a_n: n\in \mathbb{N}\} \quad \{(v_p(i), v_d(i), z(i)), i \in\mathbb{N}\}</script><ul>
<li>其中，单个请求内的$(v_p(i), v_d(i), z(i))$允许相关，且每个量都具有一阶矩。<script type="math/tex">\lambda=\mathbb{E}(a_1), m_p = \mathbb{E}(v_p(1)), m_d=\mathbb{E}(v_d(1))</script></li>
</ul>
</li>
</ul>
</li>
<li>迭代级别的批量服务(Iteration-level batch serving)<ul>
<li>假设系统有一个处理预填充token和解码token的LLM引擎，该引擎的调度器会从多个请求中选取一个批量的token同时处理，以提升计算效率。特别地，来自每个请求的一定数量的未处理tokens可以根据以下可行性约束划分为一个批量。<ul>
<li>同一个请求的所有的预填充token必须在第一个解码token被加载前处理。</li>
<li>在请求的预填充和解码阶段，一个token只有在处理完前一个token才能被加载到一个批量中，即一个批量中不能出现同一请求的两个token。</li>
<li>除非在预填充阶段，允许连续多个token在同一批量。</li>
<li>同一个批次中不能加载同一个请求的两个解码token(因为解码是自回归一个个出现的？)，也不允许有来自同一个请求的一个预填充token和一个解码token(因为要避免干扰？)。</li>
</ul>
</li>
<li>将一个批次中加载的token的总数$b’$称为$token_load$，并且需满足$b’\le b$，$b$是预定义的$token_budget$。一次迭代为完成对一个批次的处理。<ul>
<li>以上定义可以参见”Sarathi-serve”的定义。</li>
<li>$token_budget$是为了确保迭代时间不会因长输入prompt而产生巨大差异，从而对解码阶段的请求的TBT产生负面影响。通常设置为256或512。</li>
<li>一个批次中的请求的数量才是$batch_size$。</li>
</ul>
</li>
<li>调度算法可以选择$b’$并在每次迭代时确定批次的token组成。<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Visualization%20of%20key%20scheduling%20terminologies%20in%20LLM%20engine.png" alt="img"><ul>
<li>图中展示的是将预填充和解码阶段的token混合在一个批次处理的选择过程(即使来自不同的请求，也是不提倡混合的，这样会产生干扰)。图中还展示了定义的规则: 预填充阶段的token是可以有多个在同一个批次的，但是解码阶段的token不可以。</li>
</ul>
</li>
</ul>
</li>
<li>批次处理时间(Batch processing time)<ul>
<li>意图建立一个解析形式或分析性的批次处理时间预测建模。</li>
<li>一个批次的运行时间实际可以很好地近似为一个分段的线性函数，该函数仅取决于token的负载$b’$: $t_{b’} = c + a \cdot \max(0, b’-b_0)$。以上是一个经验公式，通过两类实验获得：<ul>
<li>一是探究阈值$token_budget$对批次处理时间的关系，其中的重要发现是当阈值固定时，其实token内容的构成对批次处理时间的影响非常小。</li>
<li>二是探究随着所加载token总数的变化，批次处理时间的增长轨迹可以很好地近似为一个分段线性函数。</li>
<li>不过该公式与实际处理时间有高度一致性，因为在LLM推理中线性层(前馈神经网络)的计算占有主导地位。再具体些的话，则需要考虑注意力层计算和KV缓存内存开销。</li>
</ul>
</li>
<li>决策是在某个时隙的开始时进行。一个批次的处理会跨越多个时隙，但具有原子性，是不间断的。批次完成是在某个时隙的结束时完成。对于一个请求来说，最后一个解码在一个时隙结束时处理完成后，该请求就会立即离开系统。</li>
</ul>
</li>
</ul>
<h4 id="马尔可夫链-A-Markov-chain"><a href="#马尔可夫链-A-Markov-chain" class="headerlink" title="马尔可夫链(A Markov chain)"></a>马尔可夫链(A Markov chain)</h4><p>引入离散时间马尔可夫链(DTMCs, discrete-time Markov chains)描述系统的动态。</p>
<ul>
<li>在每个时隙$n$，令$Q_n$表示尚未离开的请求集合。假设$\{(P_i(n), D_i(n), Z_i(n)):i\in\mathscr{Q}_n\}$是有序的(按请求到达时间排序)。<ul>
<li>$P_i(n)$表示在预填充阶段未处理的token的数量，不包括当前在批处理中正在处理的token。</li>
<li>$D_i(n)$表示在解码阶段未处理的token的数量，不包括当前在批处理中正在处理的token。</li>
</ul>
</li>
<li>令$R(n)$表示当前批处理的剩余时间。当$R(n)=0$时，则可以形成新的批次。否则不需要进行决策。时隙$n$开始时的系统状态定义为:<script type="math/tex">X(n)=(R(n), \{(P_i(n), D_i(n), Z_i(n)):i\in\mathscr{Q}_n\})</script><ul>
<li>将$\mathscr{X}$定义为在某个时隙所有可能的状态$X(n)$的集合，其中$n\in N$。</li>
</ul>
</li>
<li>调度算法(Scheduling algorithms):<ul>
<li>令$x=(r, \{(p_i, d_i, z_i):i\in Q\})\in X$表示任意系统状态。</li>
<li>给定一个状态$x$且$r=0$，该状态表明上个批次已经处理完毕，可以开始新批次处理的状态。调度算法选择一个批处理配置:$\pi(x) = (\delta p_i, \delta d_i)_{i\in Q}$。<ul>
<li>$\delta p_i$表示来自请求$i$的，要包含在该次批处理中的预处理阶段token的数量。</li>
<li>$\delta d_i$表示来自请求$i$的，要包含在该次批处理中的解码阶段token的数量。</li>
</ul>
</li>
<li>调度算法$\pi(x)$需要满足以下可行性约束条件:<script type="math/tex; mode=display">p_i \gt 0\quad implies\quad \delta_i^d = 0\\\delta_i^p \le p_i\\\delta_i^d \le 1\\\sum_{i\in\mathscr{Q}}(\delta_i^d + \delta_i^p) = b'\le b</script><ul>
<li>第一个约束要求在解码之前完成对预填充阶段token的处理。</li>
<li>第二个约束限制了在某次批处理中的预填充阶段token的数量不大于请求中剩余的预填充阶段token数量。</li>
<li>第三个约束限制了批处理中的解码阶段只能进行顺序解码(不能一次处理多个token)。</li>
<li>第四个约束限制了一次批处理中的token总数不大于$token_budget$。</li>
</ul>
</li>
<li>当$r \gt 0$时，强制让$b’=0$。因为在此过程中批处理正在进行中。当然$r=0$时，也可以这样设置。</li>
</ul>
</li>
<li>系统调度(System dynamics)<ul>
<li>令系统在时隙$n$的状态为: $X(n)=(R(n), \{P_i(n), D_i(n), Z_i(n), i\in Q_n\})$。使用$X’(n)$表示时隙$n$进行决策后的状态，该状态是在考虑时隙$n$中新请求到达之前的状态。特别地，令$\pi(X(n))=(\delta p_i, \delta d_i)_{i\in Q}$：<script type="math/tex">P_i'(n) - P_i(n) - \delta_i^p, i\in\mathscr{Q}_n,\\ D_i'(n) = D_i(n) - \delta_i^d, i\in\mathscr{Q}_n,\\ \mathscr{Q}_n'=\mathscr{Q}_n \setminus\{i\in\mathscr{Q}_n:D'(n) = 0\}</script><ul>
<li>$\setminus$为集合差运算，表示从左边的集合中移除属于右边集合的元素。</li>
<li>第一个公式表示经过该时隙处理后剩余的预填充token的数量。</li>
<li>第二个公式表示经过该时隙处理后剩余的解码token的数量。</li>
<li>第三个公式表示经过该时隙处理后剩余的请求的数量。</li>
</ul>
</li>
<li>考虑在时隙$n$中会有新请求到达，则时隙$n+1$的状态可以表示为: $X(n+1)=(R(n+1), \{P_i(n+1), D_i(n+1), Z_i(n+1), i\in\mathscr{Q}_{n+1}\})$。时隙$n+1$的请求集合为$\mathscr{Q}_{n+1} = \mathscr{Q}_n’ \bigcup \{\mathscr{A}_n\}$。<script type="math/tex">\forall i\in \mathscr{Q}_n': P_i(n+1)=P_i'(n), D_i(n+1)=D_i'(n), Z_i(n+1)=Z_i(n)\\ \forall i\in \mathscr{A}_n: P_i(n+1)=v_p(i), D_i(n+1)=v_d(i), Z_i(n+1)=z'(i)</script><ul>
<li>前者是未完成的旧批次，后者是新的批次。</li>
</ul>
</li>
<li>对于剩余的处理事件$R(n+1)$:<script type="math/tex">R(n+1)=\begin{cases} R(n) - 1 & R(n)\gt 0\\ t_{b'} - 1 & R(n) = 0 \end{cases}</script><ul>
<li>其中$t_{b’}$是处理新批次$b’:=\sum_{i\in\mathscr{Q}(n)}(\delta_i^p + \delta_i^d)$所需的时隙数。前者表示批次仍在处理，后者表示批次已经处理完成，准备处理下一个新的批次。</li>
<li>假设iid假设成立，在任何调度算法下，$\{X_n:n\in N\}$都是一个DTMC(离散马尔可夫链)。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="吞吐量优化算法-Throughput-optimal-algorithms"><a href="#吞吐量优化算法-Throughput-optimal-algorithms" class="headerlink" title="吞吐量优化算法(Throughput-optimal algorithms)"></a>吞吐量优化算法(Throughput-optimal algorithms)</h4><p>要解决的核心问题是什么是系统能够达到的最大吞吐率，哪些调度算法能够达到最大吞吐率。</p>
<p>一个调度算法被成为“吞吐量最优”，意味着在特定的请求到达率($\lambda$)下，能够保持稳定队列的系统。即请求的到达率没有超过系统的最大处理能力，那么系统中的请求数量或令牌数量不会无限增长，系统能够处理所有到达的请求并保持稳定。</p>
<ul>
<li>“稳定”: 在数学上是指关联的离散时间马尔可夫链(DTMC)是不可约且正常返(irreducible and positive recurrent)的。<ul>
<li>不可约: 系统可以从任何一种状态(队列很长、正在处理某种类型的批次)转移到任何其他状态(队列变短、正在处理另一种不同类型的批次)，系统行为不会被限制在某个子集中。</li>
<li>正常返(positive recurrent): 系统状态不会无限地“漂移”到状态空间之外(队列无限增长)。系统会周期性地回到之前的状态，并且回到任何状态的平均时间是有限的。系统能够在长期运行中保持其状态(如队列长度)在一定界限内，不会无限膨胀。</li>
</ul>
</li>
</ul>
<p>一类为”work-conserving”的调度算法能够几乎实现系统最大化吞吐率。特别地，当$\pi(x)$能够形成一个满足$b’=b$的批次时，一个调度算法被称为是”work-conserving”的。即对于$\sum_{i\in\mathscr{Q}}(p_i+1(p_i=0))\ge b$，满足: $\pi(x)=\sum_{i\in\mathscr{Q}}(\delta_i^d + \delta_i^p) = b$(不会浪费任何带宽，保证每个批次都到达阈值，通常的实现方法是将预填充和解码的token混合到一个批次处理)。</p>
<p>$(K_p, K_d)$-FCFS算法:</p>
<ul>
<li>令$K_p, K_d$为不小于1的整数。在阶段$f\in\{p, d\}$加载token时，仅加载最多$K_f$个最旧请求的token(token的选择可以遵循任何规则，如最小剩余token优先，因为本身请求就是最旧的了？)</li>
<li>假设每个DTMC都是不可约的(通过假设$P\{a_n=0\}\gt 0$，即一段时间内没有传入新的请求，如果这段时间足够长，那么最终work-conserving策略会清空队列)。</li>
<li>假设用户的请求到达系统是独立同分布的，每个请求的预填充 token 数量、解码(decode) token 数量以及特征也是独立同分布的。这些序列彼此独立。满足iid假设和第一矩假设(<a href="#Model_Setup">Model Setup</a>)，更进一步假设$\mathbb{E}[D_1^2]\lt\infty$具有二阶矩。</li>
<li>如果系统满足以下负载条件，则说明DTMC$\{X_n, x\in N\}$在任意work-conserving的算法中是正常返(positive recurrent)的。其中$\frac{b}{t_b}$表示处理token的最大速率。<script type="math/tex">\lambda(m_p+m_d)\lt\frac{b}{t_b}</script></li>
<li>如果系统满足以下负载条件，<script type="math/tex">\lambda(m_p+m_d)\gt\frac{b}{t_b}</script><ul>
<li>则说明对于时隙n时所有未处理的token$|X_n|=\sum_{i\in\mathscr{Q}_n}(P_i(n)+D_i(n))$，永远都有未处理完的token，即不是正常返的，无法终止。<script type="math/tex">\mathbb{P}\{\lim_{n\rightarrow\infty}|X_n|=\infty\}=1</script></li>
<li>当系统过载时，显然没有任何调度算法可以稳定系统。在此过载情况下，系统内存可能会耗尽，从而导致请求被阻止或丢弃。</li>
</ul>
</li>
<li>在实际部署中，如果一些系统使用的调度算法不是”work-conserving”的，即使满足负载条件$\lambda(m_p+m_d)\lt \frac{b}{t_b}$，该系统也可能是不稳定的。当LLM实例连接到网络以处理AI智能体的工作负载时，即使算法是”work-conserving”的，也可能无法实现最大的吞吐量。</li>
<li>实际证明参考原论文附录B。</li>
</ul>
<h3 id="Stability-issues-of-incumbent-scheduling-algorithms"><a href="#Stability-issues-of-incumbent-scheduling-algorithms" class="headerlink" title="Stability issues of incumbent scheduling algorithms"></a>Stability issues of incumbent scheduling algorithms</h3><p>介绍几种现有的调度算法<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Example%20workload%20and%20where%20work-conserving%20criteria%20are%20broken.png" alt="img"></p>
<ul>
<li>Decode-prioritized schedule (without mixed batching, FasterTransformer): 根据FCFS或SJF，优先处理已经完成预填充的请求的解码阶段。对这些请求的解码token批量处理，且不允许预填充和解码的token混合处理。不稳定性在于会导致预填充队列堆积。</li>
<li>Prefill-prioritized schedule (without mixed batching, vLLM-vanilla): 根据FCFS或SJF，优先处理处于预填充阶段的请求，即对于已进入解码阶段的请求先不处理，直到所有请求的预填充阶段都处理完成才统一对解码阶段进行处理。也不允许预填充和解码的token混合处理。不稳定性在于会导致解码队列堆积。</li>
<li>Prefill-prioritized schedule (with mixed batching, Orca): 与前者的区别是，允许对预填充和解码的token混合处理。即对于已完成预填充阶段的请求的解码token，尽可能优先填充处理。</li>
<li>Decode-prioritized chunk schedule (Sarathi-Serve): 当构造待处理批量的时候，尽可能多地填充解码token(不过每个请求最多只能给出1个token，因为自回归顺序解码)。如果批次中的token数量不大于阈值token budget，则填充预填充阶段的token。因为前面已经从经验得到了批次处理时延只和token总数相关的结论，所以该算法中批次的组成并不重要。</li>
</ul>
<h3 id="Generalization-to-the-AI-agent-workload"><a href="#Generalization-to-the-AI-agent-workload" class="headerlink" title="Generalization to the AI-agent workload"></a>Generalization to the AI-agent workload</h3><p>Agent工作负载的一个请求可能包含一系列的LLM调用，设计不同LLM引擎之间的交互。此类请求的执行流程可以根据LLM的输出动态变化。针对它进行高效的调度和资源优化对于最大化吞吐量和最小化延迟至关重要。</p>
<p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/AI%20Agent%20Infrastructure.png" alt="img"></p>
<h4 id="异构并行的LLM引擎-Homogeneous-parallel-LLM-engines"><a href="#异构并行的LLM引擎-Homogeneous-parallel-LLM-engines" class="headerlink" title="异构并行的LLM引擎(Homogeneous, parallel LLM engines)"></a>异构并行的LLM引擎(Homogeneous, parallel LLM engines)</h4><p>假设有$K$个相同的LLM引擎，可以被配置为并行运行。请求可以根据负载均衡算法被路由到任何一个LLM引擎。于是负载条件(load condition)可以表示为: $\lambda(m_p + m_d)\lt \frac{Kb}{t_b}$。</p>
<ul>
<li>一种负载均衡的算法是将一个请求(以均匀概率)随机分配给$K$个引擎中的一个。</li>
<li>另一种算法是分配给目前具有最少数量请求的引擎。</li>
<li>当然，如果能够所有LLM引擎中未处理的token数量信息是已知的，则可以分配给使用具有最少token数量的引擎。<br>以上算法都是稳定的。证明过程原文中没看到，后续再观察。</li>
</ul>
<h4 id="一个处理两种请求类型的LLM引擎-An-LLM-engine-serving-two-types-of-requests"><a href="#一个处理两种请求类型的LLM引擎-An-LLM-engine-serving-two-types-of-requests" class="headerlink" title="一个处理两种请求类型的LLM引擎(An LLM engine serving two types of requests)"></a>一个处理两种请求类型的LLM引擎(An LLM engine serving two types of requests)</h4><p>假设有$j\in\{1, 2\}$两种类型的请求以$\lambda^j$的速度到达，平均token大小为$(m^p_j, m^d_j)$，假设算法满足$\sum_{j\in\{1, 2\}}\lambda^j(m_p^j + m_d^j)\lt\frac{b}{t_p}$，则该算法能使系统稳定。</p>
<h4 id="LLM引擎的两种网络-Two-networks-of-LLM-engines"><a href="#LLM引擎的两种网络-Two-networks-of-LLM-engines" class="headerlink" title="LLM引擎的两种网络(Two networks of LLM engines)"></a>LLM引擎的两种网络(Two networks of LLM engines)</h4><ul>
<li>fork-join network<ul>
<li>假设有四个Agent，记为$\{a_j\}$，$j\in\{0, 1, 2, 3\}$。其中$a_0$的请求到达速率为$\lambda$，当请求被$a_0$处理完后，会分为两个子请求，子请求1由$a_1$处理，子请求2由$a_2$处理。在同一请求的两个子请求都完成后会进行合并，并到达$a_3$进行处理。</li>
<li>假设$a_j$的token大小为$(m_j^p, m_j^d)$，满足$\lambda(m_j^p+m_j^d)\lt \frac{b^j}{t^j_b}, j\in\{0, 1, 2, 3\}$，在$a_j$中都是$(K_p^j, K_d^j)$-FCFS work-conserving调度算法。则描述系统的DTMC是正常返的。</li>
<li>如果把$a_3$去掉也不影响。</li>
</ul>
</li>
<li>Rybko-Stolyar(RS) type network<ul>
<li>假设有一个两个Agent($a_1, a_2$，也可称为LLM引擎)构成的网络。</li>
<li>假设有两种类型的请求:<ul>
<li>A类请求先由$a_1$处理一个短任务，然后由$a_2$处理一个长任务。</li>
<li>B类请求先由$a_2$处理一个短任务，然后由$a_1$处理一个长任务。</li>
</ul>
</li>
<li>不同类型的任务在不同Agent上有不同的平均token大小，表示不同的计算需求。</li>
<li>经过实验发现，虽然系统满足系统负载要求且使用了”work-conversing”的调度算法，但依然可能使得系统是不稳定的。(但经过翻转优先级设置，又变稳定了)</li>
<li>说明在 LLM Agent 网络这种更复杂的系统中，并非所有工作守恒的调度策略都能保证系统的稳定性(即吞吐量最优)。调度策略中的优先级设定等细节对于网络的整体稳定性至关重要。</li>
</ul>
</li>
</ul>
<h3 id="Latency-Optimization"><a href="#Latency-Optimization" class="headerlink" title="Latency Optimization"></a>Latency Optimization</h3><p>本文把其列为未来的工作。不过通过实验发现，Sarathi-Serve中的时延(无论是端到端时延还是预填充时延)都主要受到token budget的影响。</p>
<ul>
<li>适中的token budget(512)可以降低端到端延迟的中位数。</li>
<li>较大的token budget(1024)可以改善预填充时延。</li>
<li>当token budget较小(128)的时候，由于迭代过程中重复的cross-attention，开销会显著增加(即时延会增加)。</li>
</ul>
<p>设计调度算法时，不仅要考虑 token_budget，还要考虑 prefill 长度的分布以及目标工作负载的服务级别目标(SLO，即系统需要满足特定的延迟要求)。</p>
<h3 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h3><ul>
<li>各种负载情况下，基于TBT、TTFT、TPOT等性能指标下的最优调度策略。</li>
<li>更长上下文或测试时工作负载的KV cache内存管理。</li>
<li>多租户环境下，<ul>
<li>不同类型请求共同分配在同一组Agent，调度器需要决定如何分配模型的时间和计算资源给不同的请求，尤其是区分高优先级和低优先级请求。<ul>
<li>尽力而为型(Best-effort)请求: 对延迟不太敏感，可以尽力处理，优先级相对较低。</li>
<li>敏感型(Latency-critical)请求: 如实时交互，对延迟要求非常高，需要优先处理以确保及时响应。</li>
<li>将这两种类型的请求放在一起处理(collocated together)增加了调度的复杂性，需要在保证低延迟请求性能的同时，尽量提高整体资源利用率。</li>
</ul>
</li>
<li>将多租户的不同模型共同分配在同一组物理服务器上的调度策略。涉及到更底层的资源管理和分配，调度器需要考虑如何在这些模型之间划分计算、内存等资源，以满足每个模型处理的请求类型的性能需求。</li>
</ul>
</li>
<li>联合优化技术，解决模型自动缩放、多模型资源分配、KV cache策略和负载均衡策略问题。</li>
<li>还是得挖掘排队理论和实验实践结合。</li>
</ul>
<h2 id="Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs"><a href="#Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs" class="headerlink" title="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"></a>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><ul>
<li>提交给LLM服务引擎的程序会经历漫长的累积等待时间，主要是由于单个LLM请求和程序的队首阻塞造成的。</li>
<li>现存的LLM服务引擎(Serving Engines)存在不足:<ul>
<li>vLLM专注于通过提升KV cache的效率，加速CUDA kernels和提出更好的LLM请求调度算法，优化单个独立的LLM调用或者静态的LLM应用。但未能考虑到程序级别(program-level)的上下文，例如同一程序中的LLM调用之间的依赖关系，或者像总执行时间这样的统计信息。因此通常会因为复杂程序而导致次优的端到端性能，特别是程序的端到端延迟。</li>
<li>形式上，单线程程序的端到端延迟包括:<ul>
<li>等待时间(主要优化目标): 程序在引擎上的LLM调用的总排队时间。随着负载增加而增加。优先考虑减少该时间，不仅可以改善程序的延迟，还可以通过缓解队首阻塞(Head-of-line)提高LLM引擎的吞吐量。不仅要考虑每个LLM调用的排队时间，还要考虑程序的等待时间。</li>
<li>执行时间(主要优化目标): LLM调用的累积前馈时间。程序的执行时间主要取决于LLM引擎管理prefill和decoding阶段的效率。<ul>
<li>具有长累积prefill的Agentic Workloads中，Autexllix会专注于优化prefill阶段，通过prefill caching消除大部分的prefill计算(存储并重用相关的KV cache条目，跨LLM请求的系统prompt)</li>
<li>数据局部性: 单个程序中，所有输入长度的缓存命中率都保持在90%以上，表明同一程序的LLM调用共享相同的上下文。考虑不同的程序时，缓存命中率则随着输入长度的增加呈指数衰减，表明不同程序仅仅共享系统prompt。</li>
</ul>
</li>
<li>拦截时间: 等待外部中断花费的时间(工具调用或人为输入)</li>
</ul>
</li>
</ul>
</li>
<li>提升吞吐量。</li>
</ul>
<h3 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h3><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/AI%20Agent%20Infrastructure.png" alt="img"></p>
<h4 id="LLM-Inference"><a href="#LLM-Inference" class="headerlink" title="LLM Inference"></a>LLM Inference</h4><p>LLM推理过程针对每个请求分为两个阶段进行:</p>
<ul>
<li>预填充(Prefill)阶段: 将输入的prompt转换为中间的token状态。</li>
<li>解码阶段: 基于先前的token序列，自回归地逐个生成新的token。</li>
</ul>
<p>为了减少计算量，LLM服务系统利用KV Cache，用于存储中间的token状态，以加速生成新的token。</p>
<h4 id="LLM-Serving"><a href="#LLM-Serving" class="headerlink" title="LLM Serving"></a>LLM Serving</h4><p>LLM Serving系统需要将LLM调用跨越引擎进行路由和在每个引擎中完成LLM调用的执行。</p>
<ul>
<li>采用负载均衡技术，通过实时迁移、分离prefills和decodes、构建前缀树、在引擎之间迁移KV-cache等方法，满足请求的SLO(Service Level Objectives，类似服务指标？)并改善尾部延迟(响应最慢的请求)。</li>
</ul>
<p>LLM Serving系统的引擎内部的设计可以参考传统操作系统的成果——内存管理、内核优化和调度方案。</p>
<ul>
<li>集成虚拟内存和分页技术，减少KV-cache碎片。</li>
<li>引入共享内存以缓存跨LLM请求的前缀，管理GPU、CPU和磁盘之间的缓存层次结构，以确保数据能高效地被访问。</li>
<li>改进GPU内核的实现，加速self-attention计算，流水线化不同的算子，并实现更好的张量或流水线并行。</li>
<li>设计更好的调度策略，决定哪些LLM请求何时在GPU上执行。将prefills和decodes打包在一起处理以及LLM请求的抢占机制(允许高优先级的短进程中断正在执行的长请求)以缩短响应时间。</li>
</ul>
<p>不过以上工作大都集中于优化独立的LLM请求，相当于优化通用程序中的函数调用。</p>
<p>Autellix专注于程序级别的优化(进程级别)，特别是调度，以弥补在处理动态、多步长LLM Agent程序时的不足。</p>
<ul>
<li>类似传统操作系统管理跨CPU内核的整个进程。</li>
</ul>
<h4 id="Agentic-Workflow"><a href="#Agentic-Workflow" class="headerlink" title="Agentic Workflow"></a>Agentic Workflow</h4><p>智能体程序是一种动态执行的工作流程(Workflow)，由有向无环图(Directed acyclic graph, DAG)表示，包含来自一个或多个智能体的LLM调用和外部中断(包括工具调用、通用代码执行或人工输入)。假设程序的LLM调用模式仅在运行时出现，因此很难完全提前了解或预测整个图。its scheduler. Autellix’s non-clairvoyant scheduler requires<br>only the cumulative service times of LLM calls within the<br>same program</p>
<p>单线程的程序的动态特性体现在两个维度上:</p>
<ul>
<li>程序的长度——由用户输入的prompt决定。</li>
<li>LLM调用和中断的顺序——由程序的控制流决定。</li>
</ul>
<p>多线程的程序通常会形成有向无环图(DAG)。经典的多线程程序Map-Reduce和基于蒙特卡洛树搜索(常用的搜索和规划方法)的线程数量会随着时间的推移变化，多个线程之间不断地fork和merge，使得图的结构不断变化。而且，多线程中的每个线程可能包含不同的LLM调用和外部中断序列。</p>
<h4 id="Agentic-Programs"><a href="#Agentic-Programs" class="headerlink" title="Agentic Programs"></a>Agentic Programs</h4><p>主要是在应用层，开发者在此构建复杂的Agentic Programs以编排agent、工具和人类之间的交互。</p>
<ul>
<li>Agent被定义为一个包含了指定Agent角色的系统prompt和LLM模型类的元组。</li>
</ul>
<p>直接通过LLM调用(call)和外部中断实现交互。外部中断主要是与系统外的工具环境进行交互。现有的Agentic编排框架(LangChain和Autogen)为开发者提供了管理程序的控制流原语，以指定何时执行Agent、调用哪些工具、是否需要人工输入。原语遵循通用的编程语义，包括条件语句、循环、错误处理和终止条件。</p>
<p>程序还会维护一个全局历史记录，记录 agents、工具和人类的输出。基于 LLM 的聊天机器人会累积 LLM agents 的输出和人类输入之间的消息。Autellix便希望利用全局历史记录，动态构建程序的执行图(DAG)的内部状态，并存储到进程表中。</p>
<h4 id="Agentic-Applications"><a href="#Agentic-Applications" class="headerlink" title="Agentic Applications"></a>Agentic Applications</h4><p>扩展推理时间计算(LLM调用的次数)以及相应的总解码tokens提高在复杂任务的性能，会使用以下方法: </p>
<ul>
<li>逐步推理分解任务。</li>
<li>显式思维注入指导推理、规划和搜索以探索可能的解决方案。</li>
<li>自我批评评估行为。</li>
<li>自我反思从失败中学习。</li>
<li>多智能体协作。</li>
</ul>
<p>单线程的应用主要是结合思维链(CoT)，多线程的应用则主要使用蒙特卡洛树搜索(MCTS)，集成了并行规划、自我批评、自我反思和多智能体协作。分布式的应用程序还可以结合best-of-N采样、集束搜索(beam search)、前瞻(lookahead)技术和遗传算法探索和发现最优解。</p>
<p>Agentic programs表现出的特性: </p>
<ul>
<li>动态性: 同一程序的不同用户prompt可能会产生完全不同的执行模式。</li>
<li>非确定性: 未来是未知的，程序何时决定终止也是无法准确预测的。</li>
<li>分布式: 并行调用。</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Autellix 是一个多引擎 LLM 推理服务系统，包含前端、调度器和负载均衡器。提出的Autellix是一个旨在运行程序而非单个LLM调用的LLM推理系统。经操作系统进程调度器的启发，核心思想是根据LLM调用所属程序的先前已完成调用的总执行时间确定LLM调用的优先级。</p>
<ul>
<li>来自较长程序的LLM调用不太可能很快完成，需降低优先级，从而允许较短的程序首先完成，从而有效减少队头阻塞(Head-of-Line blocking)。</li>
<li>如果一个程序之前已经运行了很长时间(累积执行时间较长)，那么后续的LLM调用优先级会被降低。</li>
</ul>
<p>引入一个能够利用全局程序级统计信息(程序在引擎的累积执行时间)，以最大限度地减少等待时间并提高引擎的吞吐量。<br>两种非预知的调度算法:</p>
<ul>
<li>假设没有程序的先验工作负载知识，用于单线程程序的PLAS(Program-Level Attained Service): 根据其源程序当前的累积服务时间或执行时间确定LLM调度的优先级。</li>
<li>用于表示为通用动态有向无环图的多线程程序的ATLAS(Adaptive Thread-Level Attained Service): 基于同一程序中所有线程的最大累积服务时间确定LLM调用的优先级。根据程序的关键路径对调用进行排序。除了减少等待时间，还通过优先处理关键的LLM调用减少程序的完工时间，否则这些调用会阻止程序的进度。</li>
</ul>
<p>能够跨多个引擎对程序的LLM调用进行路由。因为观察到一个程序内的LLM调用通常共享共同的前缀和累积的对话状态，而跨程序(跨进程)的调用只共享了系统的prompt。为了避免重复计算程序的KV cache，通过将较长的LLM调用路由到其程序执行的引擎尊重原程序的数据局部性，将较短的调用负载均衡到其他引擎(只需要由系统prompt构成这些调用的大部分输入)。</p>
<p>Autellix是作为LLM服务引擎的顶部一层实现的，向用户开放有状态的API接口，易于部署。</p>
<h3 id="Autellix-Design"><a href="#Autellix-Design" class="headerlink" title="Autellix Design"></a>Autellix Design</h3><p>Autellix整体架构由两个关键组件组成: </p>
<ul>
<li>程序感知调度器(Program-aware scheduler): 用于减少调用级别(call-level)和程序级别(program-level)的阻塞。</li>
<li>数据局部感知负载均衡器(Data-locality-aware-load-balancer)</li>
</ul>
<h4 id="基础目标"><a href="#基础目标" class="headerlink" title="基础目标"></a>基础目标</h4><ul>
<li>提升用户程序的整体系统端到端延迟。</li>
<li>为供应商最大化GPU利用率，提升吞吐量。</li>
<li>缓解程序资源匮乏，从而提升公平性。</li>
</ul>
<h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>Autellix是不具备预见性的，无法预先知道要运行程序的到达时间、已执行工作流的结构或总体工作负载的分布。<br>因此当一个程序到达时，Autellix需要在程序运行工程中动态构建其有向无环图作为内部表示(Internal representation, IR)。因此能处理任何调用底层LLM引擎的通用程序，无需开发者提供关于程序结构的额外信息。</p>
<h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Autellix%20architecture.png" alt="img"></p>
<p>Autellix的创新还在于它的LLM调用是有状态的，程序从用户的本地机器执行时，就会与Autellix建立会话(Session)。随着时间的推移，程序会发出带有相关会话ID的LLM调用。当会话开始时，Autellix会向维护着的全局进程表添加程序的相应条目，跟踪程序的元数据(总服务时间、线程级元数据、程序LLM调用的等待时间)。引擎级调度器和有状态负载均衡器(两个组件)都利用该表调度下一个解码批处理的LLM调用，并根据其程序的数据局部性将LLM调用路由到合适的引擎。</p>
<h4 id="Program-Aware-Scheduler"><a href="#Program-Aware-Scheduler" class="headerlink" title="Program-Aware Scheduler"></a>Program-Aware Scheduler</h4><ul>
<li>目标: 最小化端到端延迟，最小化响应时延，减轻程序级别和调用级别的队首阻塞(head-of-line blocking)</li>
<li>约束: 没有先验知识。</li>
</ul>
<p>基于程序级别的统计数据(例如总的累计运行时间，记录在全局进程表)为每个LLM调用分配优先级，并且允许LLM调用根据优先级动态抢占。</p>
<p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Program-Aware%20Scheduler.png" alt="img"></p>
<ul>
<li>Program-level Prioritization<ul>
<li>Process Table(进程表)<ul>
<li>维护一个记录了所有运行程序的全局进程表，每当程序的一个LLM调用完成，该表就会相应地更新。<ul>
<li>服务时间(Serving time): <ul>
<li>对于单线程程序，是所有LLM引擎的模型执行器上已完成的调用的累积执行时间。</li>
<li>对于多线程程序，是运行时间最长的线程的关键路径的长度。</li>
</ul>
</li>
<li>等待时间(Waiting time): 调用在LLM引擎调度队列的等待时间，主要用于反饥饿机制(anti-starvation)。</li>
<li>引擎ID(Engine IDs): 程序正在运行的位置，主要用于负载均衡。</li>
<li>线程元数据(Thread Metadata): 每个线程对应一个活跃的LLM调用。元数据包括了活跃调用的到达、等待和服务时间。</li>
<li>最近的调用到来的时间(Most recent call arrival): 程序上一次有新的LLM调用到达的时间，用于追踪过时的程序。</li>
<li>最近的调用完成的时间(Most recent call completion): 上一次有LLM调用完成的时间，用于检测长时间的外部中断。</li>
</ul>
</li>
</ul>
</li>
<li>Single-Threaded Programs<ul>
<li>使用的调度策略是Least-Attained-Service(LAS) algorithm，并引入Program-Level Attained Service(PLAS)。对于一个单线程的程序来说，运行时间就是所有先前已完成的LLM调用的总运行时间。</li>
<li>如果提交了第j个LLM调用$c_j$，其程序ID为$c_j.id$，PLAS会根据所有先前具有相同ID的LLM调用的运行时间$t_k$之和，为$c_j$分配一个优先级<script type="math/tex">p(c_j)=\sum_{k < j, c_k.id=c_j.id}t_k</script></li>
<li>优先值$p(c_j)$越大表明优先级越低，这是因为Autellix倾向于帮助较短的程序更快完成，减少响应时间。</li>
</ul>
</li>
<li>Multi-Threaded Programs<ul>
<li>由于多线程的程序被建模为由LLM调用构成的动态有向无环图。因此程序的执行时间由对应的DAG上的关键路径决定。<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Critical%20path%20for%20multi-threaded%20programs.png" alt="img"></li>
<li>引入Adaptive Thread-Level Attained Service(ATLAS)，基于调用的服务的时间，根据程序的关键路径优先处理调用。旨在根据同一程序中其父节点$\mathcal{P}(c_j)$的优先级($p(c_k)$)和已完成的服务时间$t_k$，为每个新到达的LLM调用$c_j$分配一个优先级<script type="math/tex">p(c_j)=\begin{cases}0, & if\quad c_j\quad is\quad root\\ max_{c_k\in\mathcal{P(c_j)}}\{p(c_k) + t_k\} & otherwise\end{cases}</script></li>
<li>要结合处理最长关键路径线程和优先处理较短程序的目标，则需在全局进程表中加入一个标量: the longest observed critical path。程序中的每个活跃的LLM调用都会继承该值作为其初始的优先级，并且在call完成时，仅当其自身的critical path更长时才更新这个标量。由于给定程序的所有call都是从同一条目进行优先级派生的，因此调度器会自然地将程序的并行调用分组到一起，防止落后的线程延迟程序的完成时间。</li>
</ul>
</li>
</ul>
</li>
<li>Preemptive Scheduling(抢占式调度)<br><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/LLM%20call%20lifecycle%20based%20on%20discretized%20prioritization.png" alt="img"><ul>
<li>Multi-level Program-based Scheduling<ul>
<li>基于连续优先级进行的调度和抢占程序可能会退化为最坏情况的循环调度。性能比FCFS(先到先处理)更差，导致不必要的上下文切换(CPU和GPU之间频繁的KV缓存交换)。于是Autellix将优先级离散化为一组有限的队列(参考操作系统的多级反馈队列)，假设有K个优先级递减的队列$(Q_1, Q_2, …, Q_K)$，每个队列$Q_i$覆盖一个优先级范围$[Q_i^{lo}, Q_i^{hi})$，其中$Q_1^{lo}=0, Q_K^{hi}=\infty, Q_{i+1}^{lo}=Q_i^{hi}$。</li>
</ul>
</li>
<li>Anti-Starvation<ul>
<li>为了解决低优先级的较长程序一直无法被处理而产生饥饿的问题。Autellix利用全局进程表衡量程序级别的饥饿情况，对于程序$p$的某个LLM调用$c$，如果总等待时间($W_{total}=W_p+W_c$)与总服务时间$T_{total}=T_p+T_c$之比超过阈值$\beta$，则将调用$c$提升到$Q_1$队列，并需要将$c$的等待时间和执行时间重置为0。可通过修改$\beta$的值权衡平均响应和公平性。<script type="math/tex">\frac{W_{total}}{T_{total}}\ge \beta</script></li>
</ul>
</li>
<li>Memory Management(内存管理)<ul>
<li>抢占式的调度会导致频繁的GPU-CPU切换(主要是KV-cache块会被反复交换以服务不同的请求)。</li>
<li>Autellix通过多步调度减少总的交换次数，每N个解码步骤运行一次调度器，而不是每一步都运行。调度器还会过度配置一些已经在GPU上但正在排队的请求，以便在某些请求在N步之前完成时能够立即添加新请求。</li>
<li>还采用一个更高效的交换内核，不是为每个块调用单独的异步传输，而是将所有的KV块收集到一个连续的缓冲区中，通过一次操作完成传输，从而减少碎片、降低每个块的开销和降低端到端的延迟提高PCIe的带宽利用率。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Load-Balancer"><a href="#Load-Balancer" class="headerlink" title="Load Balancer"></a>Load Balancer</h4><p>随着负载的增加，部署多个引擎副本是必要的。但是不考虑数据局部性的话会导致次优性能。</p>
<ul>
<li>对于短请求，通过实验发现由于是共享system prompt，所以缓存命中率其实差异不大，无需过分要求数据局部性。于是将其路由到当前负载最低的引擎，有助于负载均衡。</li>
<li>对于长请求，则需先检查该程序是否已经在一个引擎运行过，如果已有指定引擎，则将长请求只分配给指定引擎(哪怕会排队)，从而最大化KV-cache的重用率。否则，将其路由到当前负载最低的引擎，并要将该引擎记录到全局进程表。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zjn-astonishe/image/refs/heads/main/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/LLM%20Agent%20Serving/Load%20Balancer.png" alt="img"></p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="Fronted"><a href="#Fronted" class="headerlink" title="Fronted"></a>Fronted</h4><p>提供一种底层有状态的接口，即对于开发者来说，看起来是无状态的。用户只需将Autellix的库导入到他们的Python应用程序中，并在程序初始化时，Autellix会自动向后端发出一个start_session请求。此操作会返回一个唯一的会话标识符，并在进程表中创建一个相应的条目。后续的LLM调用会被透明地标注上相应的会话ID、程序ID和线程ID，然后再被分派到后端。当程序完成或遇到错误时，Autellix会调用end_session，从进程表中移除相关的条目。但并未对用户权限进行限制，即用户能够修改底层状态。</p>
<h4 id="LLM-Engine"><a href="#LLM-Engine" class="headerlink" title="LLM Engine"></a>LLM Engine</h4><p>基于vLLM v0.6.1构建，仅集成了新策略(PLAS, ATLAS和MLFQ)和用于提高效率的内存交换内核(Load Balancer部分的算法)对调度程序进行修改。</p>
<p>在vLLM中，每个KV块都通过cudaMemcpyAsync单独传输，从而产生了许多小的碎片化传输，降低PCIe带宽的利用率，导致重复DMA设置等高开销。为解决该问题，分配一个主机缓冲区，将所有的KV块合并成一个连续的块，从而实现一次性批量传输。</p>
<h4 id="Multi-engine"><a href="#Multi-engine" class="headerlink" title="Multi-engine"></a>Multi-engine</h4><p>在现有的AsyncLLMEngine上构建AsyncMultiLLMEngine层。每个LLM引擎的副本都在一个独立的Python进程中运行，以实现隔离和并行性。这些副本通过元引擎作为中心协调者进行管理。元引擎和各个引擎副本之间使用标准的IPC原语(mp.Queue, mp.Pipe)通信。处理流程如下:</p>
<ul>
<li>当元引擎接收到一个来自前端的请求时，将请求分配给一个合适的引擎副本。同时立即向前端返回一个”未来对象(future-like object)”，表示结果将在未来可用，这样前端就不会被阻塞。</li>
<li>被选中的引擎进程异步执行接收到的任务，在任务完成后通过IPC通道将结果返回给元引擎。</li>
<li>元引擎收到结果后，解析之前返回给前端的未来对象，并将最终输出结果填写到未来对象并再次打包发送给前端。</li>
</ul>
<h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><h4 id="Graph-Optimization"><a href="#Graph-Optimization" class="headerlink" title="Graph Optimization"></a>Graph Optimization</h4><p>Autellix是假设完全没有先验知识的，虽然效果不错，但是通过实验也可以看到和拥有完全先验知识的SRPT算法有不小的差距。当然，完全先验知识是不现实的，但可以考虑像编译器的分支预测和推测执行一样的思路，预测LLM调用。</p>
<h4 id="Post-Training"><a href="#Post-Training" class="headerlink" title="Post-Training"></a>Post-Training</h4><p>后训练，如Deepseek-R1和OpenAI的o1/o3模型，端到端强化学习进行后训练，优化思维过程。在此过程中，如果使用分布式强化学习系统加速，交替进行分布式在线采样和训练，则非常适用Autellix框架(能够减少批次采样的总完成时间)。</p>
<h2 id="Tempo-Application-aware-LLM-Serving-with-Mixed-SLO-Requirements"><a href="#Tempo-Application-aware-LLM-Serving-with-Mixed-SLO-Requirements" class="headerlink" title="Tempo: Application-aware LLM Serving with Mixed SLO Requirements"></a>Tempo: Application-aware LLM Serving with Mixed SLO Requirements</h2><h2 id="Cost-Efficient-Serving-of-LLM-Agents-via-Test-Time-Plan-Caching"><a href="#Cost-Efficient-Serving-of-LLM-Agents-via-Test-Time-Plan-Caching" class="headerlink" title="Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"></a>Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching</h2></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/06/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-09-Agentic%20Workflow/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Agentic Workflow</div></div></a></div><div class="next-post pull-right"><a href="/2025/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-30-LLM%20Agent%20Memory/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LLM Agent Memory</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Ray%20conclusion/" title="Ray conclusion"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-27</div><div class="title">Ray conclusion</div></div></a></div><div><a href="/2024/10/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2024-10-27-Sia/" title="Sia"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-27</div><div class="title">Sia</div></div></a></div><div><a href="/2025/06/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-09-Agentic%20Workflow/" title="Agentic Workflow"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-09</div><div class="title">Agentic Workflow</div></div></a></div><div><a href="/2025/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-30-LLM%20Agent%20Memory/" title="LLM Agent Memory"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-30</div><div class="title">LLM Agent Memory</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ZJN</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zjn-astonishe"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS链接"><i class="iconfont icon-rss card_icon"></i></a><a class="social-icon" href="https://github.com/zjn-astonishe" target="_blank" title="Github"><i class="iconfont icon-github crad_icon"></i></a><a class="social-icon" href="https://gitee.com/zhang-jianning/" target="_blank" title="Gitee"><i class="iconfont icon-gitee2 card_icon"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=627561610&amp;website=www.oicqzone.com" target="_blank" title=""><i class="iconfont icon-QQ-circle-fill card_icon"></i></a><a class="social-icon" href="mailto:627561610@qq.com" target="_blank" title="Email"><i class="iconfont icon-email-fill card_icon"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM-Agent-Serving"><span class="toc-text">LLM Agent Serving</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Throughput-Optimal-Scheduling-Algorithms-for-LLM-Inference-and-AI-Agents"><span class="toc-text">Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivation"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Background"><span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8F%E6%8E%A8%E7%90%86"><span class="toc-text">LLM的生成式推理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference-and-serving-goal"><span class="toc-text">Inference and serving goal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference-optimization"><span class="toc-text">Inference optimization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Batching"><span class="toc-text">Batching</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-stochastic-processing-model-for-LLM-inference"><span class="toc-text">A stochastic processing model for LLM inference</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-Setup"><span class="toc-text">Model_Setup</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE-A-Markov-chain"><span class="toc-text">马尔可夫链(A Markov chain)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%9E%E5%90%90%E9%87%8F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-Throughput-optimal-algorithms"><span class="toc-text">吞吐量优化算法(Throughput-optimal algorithms)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stability-issues-of-incumbent-scheduling-algorithms"><span class="toc-text">Stability issues of incumbent scheduling algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generalization-to-the-AI-agent-workload"><span class="toc-text">Generalization to the AI-agent workload</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%82%E6%9E%84%E5%B9%B6%E8%A1%8C%E7%9A%84LLM%E5%BC%95%E6%93%8E-Homogeneous-parallel-LLM-engines"><span class="toc-text">异构并行的LLM引擎(Homogeneous, parallel LLM engines)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%A4%84%E7%90%86%E4%B8%A4%E7%A7%8D%E8%AF%B7%E6%B1%82%E7%B1%BB%E5%9E%8B%E7%9A%84LLM%E5%BC%95%E6%93%8E-An-LLM-engine-serving-two-types-of-requests"><span class="toc-text">一个处理两种请求类型的LLM引擎(An LLM engine serving two types of requests)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM%E5%BC%95%E6%93%8E%E7%9A%84%E4%B8%A4%E7%A7%8D%E7%BD%91%E7%BB%9C-Two-networks-of-LLM-engines"><span class="toc-text">LLM引擎的两种网络(Two networks of LLM engines)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Latency-Optimization"><span class="toc-text">Latency Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Future-Directions"><span class="toc-text">Future Directions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs"><span class="toc-text">Autellix: An Efficient Serving Engine for LLM Agents as General Programs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motivation-1"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Background-1"><span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM-Inference"><span class="toc-text">LLM Inference</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM-Serving"><span class="toc-text">LLM Serving</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Agentic-Workflow"><span class="toc-text">Agentic Workflow</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Agentic-Programs"><span class="toc-text">Agentic Programs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Agentic-Applications"><span class="toc-text">Agentic Applications</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Autellix-Design"><span class="toc-text">Autellix Design</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%9B%AE%E6%A0%87"><span class="toc-text">基础目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%87%E8%AE%BE"><span class="toc-text">假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Program-Aware-Scheduler"><span class="toc-text">Program-Aware Scheduler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Load-Balancer"><span class="toc-text">Load Balancer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation"><span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fronted"><span class="toc-text">Fronted</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LLM-Engine"><span class="toc-text">LLM Engine</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-engine"><span class="toc-text">Multi-engine</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Future-Work"><span class="toc-text">Future Work</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Optimization"><span class="toc-text">Graph Optimization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Post-Training"><span class="toc-text">Post-Training</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tempo-Application-aware-LLM-Serving-with-Mixed-SLO-Requirements"><span class="toc-text">Tempo: Application-aware LLM Serving with Mixed SLO Requirements</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-Efficient-Serving-of-LLM-Agents-via-Test-Time-Plan-Caching"><span class="toc-text">Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-30-LLM%20Agent%20Memory/" title="LLM Agent Memory">LLM Agent Memory</a><time datetime="2025-06-30T08:03:23.000Z" title="发表于 2025-06-30 16:03:23">2025-06-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-29-LLM%20Agent%20Serving/" title="LLM Agent Serving">LLM Agent Serving</a><time datetime="2025-06-29T03:22:18.000Z" title="发表于 2025-06-29 11:22:18">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2025-06-09-Agentic%20Workflow/" title="Agentic Workflow">Agentic Workflow</a><time datetime="2025-06-09T06:24:58.000Z" title="发表于 2025-06-09 14:24:58">2025-06-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/2025-06-08-%E5%87%B8%E9%9B%86%E3%80%81%E5%87%B8%E5%87%BD%E6%95%B0%E4%B8%8E%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/" title="凸集、凸函数与凸优化问题">凸集、凸函数与凸优化问题</a><time datetime="2025-06-08T03:23:03.000Z" title="发表于 2025-06-08 11:23:03">2025-06-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/29/%E6%99%BA%E8%83%BD%E4%BD%93/2025-04-29-Hammer%E7%AC%94%E8%AE%B0/" title="Hammer笔记">Hammer笔记</a><time datetime="2025-04-29T11:33:05.000Z" title="发表于 2025-04-29 19:33:05">2025-04-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By ZJN</div><div class="footer_custom_text">Hi, welcome to my <a href="https://zjn-astonishe.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><div class="aplayer no-destroy" data-id="7307479551" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="list" data-preload="auto" data-autoplay="true" data-volume=0.2></div><div class="Canvas" style="position:fixed; right:0px; bottom:0px;" id="L2dCanvas"></div><script src="https://cdn.jsdelivr.net/npm/promise-polyfill@8/dist/polyfill.min.js"> </script><script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pixi.js@4.6.1/dist/pixi.min.js"></script><script src="https://cdn.jsdelivr.net/gh/zjn-astonishe/CDN@1.2.9/live2dv3.min.js"></script><script>window.onload=()=>{new l2dViewer({width:window.screen.width / 18,height:window.screen.height / 7.5,el:document.getElementById('L2dCanvas'),basePath:'https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.2',modelName:'lafei_4',mobileLimit:true,sizeLimit:true })}</script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div></body></html>