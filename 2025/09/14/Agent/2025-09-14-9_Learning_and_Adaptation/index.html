<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>9_Learning_and_Adaptation | ZJN_BLOG</title><meta name="keywords" content="Agent"><meta name="author" content="ZJN"><meta name="copyright" content="ZJN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Learning and AdaptationMotivationAI agents often operate in dynamic and unpredictable environments where pre-programmed logic is insufficient. Their performance can degrade when faced with novel situa">
<meta property="og:type" content="article">
<meta property="og:title" content="9_Learning_and_Adaptation">
<meta property="og:url" content="http://zjn-astonishe.github.io/2025/09/14/Agent/2025-09-14-9_Learning_and_Adaptation/index.html">
<meta property="og:site_name" content="ZJN_BLOG">
<meta property="og:description" content="Learning and AdaptationMotivationAI agents often operate in dynamic and unpredictable environments where pre-programmed logic is insufficient. Their performance can degrade when faced with novel situa">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png">
<meta property="article:published_time" content="2025-09-14T05:18:40.000Z">
<meta property="article:modified_time" content="2025-09-17T05:46:40.737Z">
<meta property="article:author" content="ZJN">
<meta property="article:tag" content="Agent">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://zjn-astonishe.github.io/2025/09/14/Agent/2025-09-14-9_Learning_and_Adaptation/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":800},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: ZJN","link":"链接: ","source":"来源: ZJN_BLOG","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#000000","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '9_Learning_and_Adaptation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-09-17 13:46:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3207144_mqiyof22xva.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ZJN_BLOG" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">78</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">26</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ZJN_BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">9_Learning_and_Adaptation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-14T05:18:40.000Z" title="发表于 2025-09-14 13:18:40">2025-09-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-17T05:46:40.737Z" title="更新于 2025-09-17 13:46:40">2025-09-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Agent/">Agent</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Learning-and-Adaptation"><a href="#Learning-and-Adaptation" class="headerlink" title="Learning and Adaptation"></a>Learning and Adaptation</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>AI agents often operate in dynamic and unpredictable environments where pre-programmed logic is insufficient. Their performance can degrade when faced with novel situations not anticipated during their initial design. Without the ability to learn from experience, agents cannot optimize their strategies or personalize their interactions over time. This rigidity limits their effectiveness and prevents them from achieving true autonomy in complex, real-world scenarios.</p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Learning and adaptation are pivotal for enhancing the capabilities of artificial intelligence agents. These processes enable agents to evolve beyond predefined parameters, allowing them to improve autonomously through experience and environmental interaction. By learning and adapting, agents can effectively manage novel situations and optimize their performance without constant manual intervention. </p>
<h2 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h2><p>Agents adapt by changing strategy, understanding, or goals based on learning. This is vital for agents in unpredictable, changing, or new environments.</p>
<h3 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h3><p>Agents try actions and receive rewards for positive outcomes and penalties for negative ones, learning optimal behaviors in changing situations. </p>
<p>Useful for agents controlling robots or playing games.</p>
<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><p>Agents learn from labeled examples, connecting inputs to desired outputs, enabling tasks like decision-making and pattern recognition. </p>
<p>Ideal for agents sorting emails or predicting trends.</p>
<h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><p>Agents discover hidden connections and patterns in unlabeled data, aiding in sights, organization, and creating a mental map of their environment.</p>
<p>Useful for agents exploring data without specific guidance.</p>
<h3 id="Few-Shot-Zero-Shot-Learning-with-LLM-Based-Agents"><a href="#Few-Shot-Zero-Shot-Learning-with-LLM-Based-Agents" class="headerlink" title="Few-Shot/Zero-Shot Learning with LLM-Based Agents"></a>Few-Shot/Zero-Shot Learning with LLM-Based Agents</h3><p>Agents leveraging LLMs can quickly adapt to new tasks with minimal examples or clear instructions, enabling rapid responses to new commands or situations.</p>
<h3 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h3><p>Agents continuously update knowledge with new data, essential for real-time reactions and ongoing adaptation in dynamic environments. </p>
<p>Critical for agents processing continuous data streams.</p>
<h3 id="Memory-Based-Learning"><a href="#Memory-Based-Learning" class="headerlink" title="Memory-Based Learning"></a>Memory-Based Learning</h3><p>Agents recall past experiences to adjust current actions in similar situations, enhancing context awareness and decision-making.</p>
<p>Effective for agents with memory recall capabilities.</p>
<h3 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization (PPO)"></a>Proximal Policy Optimization (PPO)</h3><p>This is a reinforcement learning algorithm used to train agents in environments with a continuous range of actions, like controlling a robot’s joints or a character in a game. Its main goal is to reliably and stably improve an agent’s decision-making strategy, known as its policy.</p>
<p>The core idea behind PPO is to make small, careful updates to the agent’s policy. It avoids drastic changes that could cause performance to collapse. Here’s how it works:</p>
<ul>
<li>Train a Reward Model, and Collect Data:<ul>
<li>The agent interacts with its environment (e.g., plays a game) using its current policy and collects a batch of experiences (state, action, reward).</li>
</ul>
</li>
<li>Fine-Tune with PPO: <ul>
<li>The LLM’s goal is to generate responses that get the highest possible score from the reward model. The reward model acts as the “judge” in the training game. (However, LLM might find a loophole and learn the “hack” the reward model to get high scores for bad responses)</li>
</ul>
</li>
<li>Evaluate a “Surrogate” Goal:<ul>
<li>PPO calculates how a potential policy update would change the expected reward. However, instead of just maximizing this reward, it uses a special “clipped” objective function.</li>
</ul>
</li>
<li>The “Clipping” Mechanism: <ul>
<li>This is the key to PPO’s stability. It creates a “trust region” or a safe zone around the current policy. </li>
<li>The algorithm is prevented from making an update that is too different from the current strategy. </li>
<li>This clipping acts like a safety brake, ensuring the agent doesn’t take a huge, risky step that undoes its learning.</li>
</ul>
</li>
</ul>
<p>PPO balances improving performance with staying close to a known, working strategy, which prevents catastrophic failures during training and leads to more stable learning.</p>
<h3 id="Direct-Preference-Optimization-DPO"><a href="#Direct-Preference-Optimization-DPO" class="headerlink" title="Direct Preference Optimization (DPO)"></a>Direct Preference Optimization (DPO)</h3><p>This is a more recent method designed specifically for aligning LLMs with human preferences. It offers a simpler, more direct alternative to using PPO for this task.</p>
<p>DPO skips the reward model entirely. Instead of translating human preferences into a reward score and then optimizing for that score, DPO uses the preference data directly to update the LLM’s policy.</p>
<p>It works by using a mathematical relationship that directly links preference data to the optimal policy. </p>
<p>It essentially teaches the model:</p>
<ul>
<li>Increase the probability of generating responses like the preferred one.</li>
<li>Decrease the probability of generating responses like the disfavored one.</li>
</ul>
<p>DPO simplifies alignment by directly optimizing the language model on human preference data. This avoids the complexity and potential instability of training and using a separate reward model, making the alignment process more efficient and robust.</p>
<h2 id="Applicable-Scenarios"><a href="#Applicable-Scenarios" class="headerlink" title="Applicable Scenarios"></a>Applicable Scenarios</h2><h3 id="Personalized-assistant-agents"><a href="#Personalized-assistant-agents" class="headerlink" title="Personalized assistant agents"></a>Personalized assistant agents</h3><p>It refines interaction protocols through longitudinal analysis of individual user behaviors, ensuring highly optimized response generation.</p>
<h3 id="Trading-bot-agents"><a href="#Trading-bot-agents" class="headerlink" title="Trading bot agents"></a>Trading bot agents</h3><p>It optimizes decision-making algorithms by dynamically adjusting model parameters based on high-resolution, real-time market data, thereby maximizing financial returns and mitigating risk factors.</p>
<h3 id="Application-agents"><a href="#Application-agents" class="headerlink" title="Application agents"></a>Application agents</h3><p>It optimizes user interface and functionality through dynamic modification based on observed user behavior, resulting in increased user engagement and system intuitiveness.</p>
<h3 id="Robotic-and-autonomous-vehicles-agents"><a href="#Robotic-and-autonomous-vehicles-agents" class="headerlink" title="Robotic and autonomous vehicles agents"></a>Robotic and autonomous vehicles agents</h3><p>It enhances navigation and response capabilities by integrating sensor data and historical action analysis, enabling safe and efficient operation across diverse environmental conditions.</p>
<h3 id="Fraud-detection-agents"><a href="#Fraud-detection-agents" class="headerlink" title="Fraud detection agents"></a>Fraud detection agents</h3><p>It improves anomaly detection by refining predictive models with newly identified fraudulent patterns, enhancing system security and minimizing financial losses.</p>
<h3 id="Recommendation-agents"><a href="#Recommendation-agents" class="headerlink" title="Recommendation agents"></a>Recommendation agents</h3><p>It improves content selection precision by employing user preference learning algorithms, providing highly individualized and contextually relevant recommendations.</p>
<h3 id="Game-AI-Agents"><a href="#Game-AI-Agents" class="headerlink" title="Game AI Agents"></a>Game AI Agents</h3><p>It enhances player engagement by dynamically adapting strategic algorithms, thereby increasing game complexity and challenge.</p>
<h3 id="Knowledge-Base-Learning-Agents"><a href="#Knowledge-Base-Learning-Agents" class="headerlink" title="Knowledge Base Learning Agents"></a>Knowledge Base Learning Agents</h3><p>It can leverage Retrieval Augmented Generation (RAG) to maintain a dynamic knowledge base of problem descriptions and proven solutions. By storing successful strategies and challenges encountered, the agent can reference this data during decision-making, enabling it to adapt to new situations more effectively by applying previously successful patterns or avoiding known pitfalls.</p>
<h2 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h2><h3 id="The-Self-Improving-Coding-Agent-SICA"><a href="#The-Self-Improving-Coding-Agent-SICA" class="headerlink" title="The Self-Improving Coding Agent (SICA)"></a>The Self-Improving Coding Agent (SICA)</h3><p>It represents an advancement in agent-based learning, demonstrating the capacity for an agent to modify its own source code, without requiring traditional training paradigms. It acts as both the modifier and the modified entity, iteratively refining its code base to improve performance across various coding challenges.</p>
<ul>
<li>SICA reviews an archive of its past versions and their performance on benchmark tests.</li>
<li>SICA selects the version with the highest performance score, calculated based on a weighted formula considering success, time and computational cost.</li>
<li>This selected version then undertakes the next round of self-modification. </li>
<li>It analyzes the archive to identify potential improvements and then directly alters its codebase. </li>
<li>The modified agent is subsequently tested against benchmarks, with the results recorded in the archive.</li>
<li>This process repeats, facilitating learning directly from past performance. </li>
</ul>
<p>SICA uses Abstract Syntax Trees (AST) parsing for efficiency. Additionally, a “SmartEditor Input Normalizer” was added. In terms of navigation, SICA independently created an “AST Symbol Locator”, using the code’s structural map to identify definitions within the codebase. Later, a “Hybrid Symbol Locator” was developed, combining a quick search with AST checking. This was further optimized via “Optimized AST Parsing in Hybrid Symbol Locator” to focus on relevant code sections, improving search speed.</p>
<p>A critical component is the asynchronous overseer, an LLM that runs concurrently with the main agent. This overseer periodically assesses the agent’s behavior for pathological deviations or stagnation and can intervene by sending notifications or even cancelling the agent’s execution if necessary. It receives a detailed textual representation of the system’s state, including a callgraph and an event stream of LLM messages, tool calls, and responses, which allows it to detect inefficient patterns or repeated work.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MaximeRobeyns/self_improving_coding_agent/">SICA Github Repository</a></p>
<h3 id="AlphaEvolve"><a href="#AlphaEvolve" class="headerlink" title="AlphaEvolve"></a>AlphaEvolve</h3><p>AlphaEvolve is an AI agent developed by Google designed to discover and optimize algorithms. </p>
<p>It utilizes a combination of LLMs, specifically Gemini models (Flash and Pro), automated evaluation systems, and an evolutionary algorithm framework. </p>
<ul>
<li>Flash is used for generating a wide range of initial algorithm proposals.</li>
<li>Pro provides more in-depth analysis and refinement.</li>
</ul>
<p>This system aims to advance both theoretical mathematics and practical computing applications.</p>
<h3 id="OpenEvolve"><a href="#OpenEvolve" class="headerlink" title="OpenEvolve"></a>OpenEvolve</h3><p>OpenEvolve is an evolutionary coding agent that leverages LLMs to iteratively optimize code. </p>
<p>It orchestrates a pipeline of LLM-driven code generation, evaluation, and selection to continuously enhance programs for a wide range of tasks.</p>
<p>A key aspect of OpenEvolve is its capability to evolve entire code files rather than being limited to single functions. </p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Agent/">Agent</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/14/Agent/2025-09-14-8_Memory_Management/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">8. Memory Management</div></div></a></div><div class="next-post pull-right"><a href="/2025/09/14/Agent/2025-09-14-10_Model_Context_Protocol/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">10_Model_Context_Protocol</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/09/07/Agent/2025-09-07-2_Routing/" title="2. Routing"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-07</div><div class="title">2. Routing</div></div></a></div><div><a href="/2025/09/07/Agent/2025-09-07-1_Prompt_Chaining/" title="1. Prompt Chaining"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-07</div><div class="title">1. Prompt Chaining</div></div></a></div><div><a href="/2025/09/08/Agent/2025-09-08-3_Parallelization/" title="3. Parallelization"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-08</div><div class="title">3. Parallelization</div></div></a></div><div><a href="/2025/09/08/Agent/2025-09-08-4_Reflection/" title="4. Reflection"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-08</div><div class="title">4. Reflection</div></div></a></div><div><a href="/2025/09/08/Agent/2025-09-08-5_Tool_Use/" title="5. Tool_Use"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-08</div><div class="title">5. Tool_Use</div></div></a></div><div><a href="/2025/09/08/Agent/2025-09-08-6_Planning/" title="6. Planning"><img class="cover" src="https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-08</div><div class="title">6. Planning</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ZJN</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">78</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">26</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zjn-astonishe"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS链接"><i class="iconfont icon-rss card_icon"></i></a><a class="social-icon" href="https://github.com/zjn-astonishe" target="_blank" title="Github"><i class="iconfont icon-github crad_icon"></i></a><a class="social-icon" href="https://gitee.com/zhang-jianning/" target="_blank" title="Gitee"><i class="iconfont icon-gitee2 card_icon"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=627561610&amp;website=www.oicqzone.com" target="_blank" title=""><i class="iconfont icon-QQ-circle-fill card_icon"></i></a><a class="social-icon" href="mailto:627561610@qq.com" target="_blank" title="Email"><i class="iconfont icon-email-fill card_icon"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Learning-and-Adaptation"><span class="toc-text">Learning and Adaptation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivation"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition"><span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Template"><span class="toc-text">Template</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reinforcement-Learning"><span class="toc-text">Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-Learning"><span class="toc-text">Supervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsupervised-Learning"><span class="toc-text">Unsupervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Few-Shot-Zero-Shot-Learning-with-LLM-Based-Agents"><span class="toc-text">Few-Shot&#x2F;Zero-Shot Learning with LLM-Based Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Online-Learning"><span class="toc-text">Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Memory-Based-Learning"><span class="toc-text">Memory-Based Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proximal-Policy-Optimization-PPO"><span class="toc-text">Proximal Policy Optimization (PPO)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Direct-Preference-Optimization-DPO"><span class="toc-text">Direct Preference Optimization (DPO)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applicable-Scenarios"><span class="toc-text">Applicable Scenarios</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Personalized-assistant-agents"><span class="toc-text">Personalized assistant agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trading-bot-agents"><span class="toc-text">Trading bot agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Application-agents"><span class="toc-text">Application agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Robotic-and-autonomous-vehicles-agents"><span class="toc-text">Robotic and autonomous vehicles agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fraud-detection-agents"><span class="toc-text">Fraud detection agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Recommendation-agents"><span class="toc-text">Recommendation agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Game-AI-Agents"><span class="toc-text">Game AI Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Knowledge-Base-Learning-Agents"><span class="toc-text">Knowledge Base Learning Agents</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Coding"><span class="toc-text">Coding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Self-Improving-Coding-Agent-SICA"><span class="toc-text">The Self-Improving Coding Agent (SICA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AlphaEvolve"><span class="toc-text">AlphaEvolve</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenEvolve"><span class="toc-text">OpenEvolve</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/Agent/2025-09-14-21_Exploration_and_Discovery/" title="21_Exploration_and_Discovery">21_Exploration_and_Discovery</a><time datetime="2025-09-14T05:24:13.000Z" title="发表于 2025-09-14 13:24:13">2025-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/Agent/2025-09-14-20_Prioritization/" title="20_Prioritization">20_Prioritization</a><time datetime="2025-09-14T05:23:46.000Z" title="发表于 2025-09-14 13:23:46">2025-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/Agent/2025-09-14-19_Evaluation_and_Monitoring/" title="19_Evaluation_and_Monitoring">19_Evaluation_and_Monitoring</a><time datetime="2025-09-14T05:23:25.000Z" title="发表于 2025-09-14 13:23:25">2025-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/Agent/2025-09-14-18_Guardrails_Safety_Patterns/" title="18_Guardrails_Safety_Patterns">18_Guardrails_Safety_Patterns</a><time datetime="2025-09-14T05:23:02.000Z" title="发表于 2025-09-14 13:23:02">2025-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/14/Agent/2025-09-14-17_Reasoning_Techniques/" title="17_Reasoning_Techniques">17_Reasoning_Techniques</a><time datetime="2025-09-14T05:22:31.000Z" title="发表于 2025-09-14 13:22:31">2025-09-14</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.3.0/picture/7.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2025 By ZJN</div><div class="footer_custom_text">Hi, welcome to my <a href="https://zjn-astonishe.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><div class="aplayer no-destroy" data-id="7307479551" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="list" data-preload="auto" data-autoplay="true" data-volume=0.2></div><div class="Canvas" style="position:fixed; right:0px; bottom:0px;" id="L2dCanvas"></div><script src="https://cdn.jsdelivr.net/npm/promise-polyfill@8/dist/polyfill.min.js"> </script><script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pixi.js@4.6.1/dist/pixi.min.js"></script><script src="https://cdn.jsdelivr.net/gh/zjn-astonishe/CDN@1.2.9/live2dv3.min.js"></script><script>window.onload=()=>{new l2dViewer({width:window.screen.width / 18,height:window.screen.height / 7.5,el:document.getElementById('L2dCanvas'),basePath:'https://cdn.jsdelivr.net/gh/zjn-astonishe/cdn@1.2',modelName:'lafei_4',mobileLimit:true,sizeLimit:true })}</script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script></div></body></html>